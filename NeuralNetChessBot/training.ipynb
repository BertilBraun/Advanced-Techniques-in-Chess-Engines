{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip intall chess tqdm shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, TextIO, Tuple\n",
    "\n",
    "import chess\n",
    "import chess.pgn\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow.keras as keras\n",
    "import tqdm\n",
    "import shutil\n",
    "from pandas.core.frame import DataFrame\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import Callback, History\n",
    "\n",
    "\n",
    "def board_to_bitfields(board: chess.Board, turn: chess.Color) -> np.ndarray:\n",
    "\n",
    "    pieces_array = []\n",
    "    colors = [chess.WHITE, chess.BLACK]\n",
    "    for c in colors if turn == chess.WHITE else colors[::-1]:\n",
    "        for p in (chess.PAWN, chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN, chess.KING):\n",
    "            pieces_array.append(board.pieces_mask(p, c))\n",
    "\n",
    "    return np.array(pieces_array).astype(np.int64)\n",
    "\n",
    "\n",
    "def bitfield_to_nums(bitfield: np.int64, white: bool) -> np.ndarray:\n",
    "\n",
    "    board_array = np.zeros(64).astype(np.float32)\n",
    "\n",
    "    for i in np.arange(64).astype(np.int64):\n",
    "        if bitfield & (1 << i):\n",
    "            board_array[i] = 1. if white else -1.\n",
    "\n",
    "    return board_array\n",
    "\n",
    "\n",
    "def bitfields_to_nums(bitfields: np.ndarray) -> np.ndarray:\n",
    "    bitfields = bitfields.astype(np.int64)\n",
    "\n",
    "    boards = []\n",
    "\n",
    "    for i, bitfield in enumerate(bitfields):\n",
    "        boards.append(bitfield_to_nums(bitfield, i < 6))\n",
    "\n",
    "    return np.array(boards).astype(np.float32)\n",
    "\n",
    "\n",
    "def board_to_nums(board: chess.Board, turn: chess.Color) -> np.ndarray:\n",
    "\n",
    "    return bitfields_to_nums(board_to_bitfields(board, turn))\n",
    "\n",
    "\n",
    "def pager(in_file: TextIO, lines_per_page=20):\n",
    "    assert lines_per_page > 1 and lines_per_page == int(lines_per_page)\n",
    "\n",
    "    lin_ctr = 0\n",
    "    current = ''\n",
    "    for lin in in_file:\n",
    "        lin_ctr += 1\n",
    "        current += lin.decode('utf-8') + '\\n'\n",
    "        if lin_ctr % lines_per_page == 0:\n",
    "            yield current\n",
    "            current = ''\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def create_training_data(dataset: DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    def drop(indices, fract):\n",
    "        drop_index = np.random.choice(\n",
    "            indices,\n",
    "            size=int(len(indices) * fract),\n",
    "            replace=False)\n",
    "        dataset.drop(drop_index, inplace=True)\n",
    "\n",
    "    drop(dataset[abs(dataset[12] / 10.) > 30].index, fract=0.80)\n",
    "    drop(dataset[abs(dataset[12] / 10.) < 0.1].index, fract=0.90)\n",
    "    drop(dataset[abs(dataset[12] / 10.) < 0.15].index, fract=0.10)\n",
    "\n",
    "    y = dataset[12].values\n",
    "    X = dataset.drop(12, axis=1)\n",
    "\n",
    "    def transform(row):\n",
    "        return list(np.concatenate(bitfields_to_nums(row)))\n",
    "    X = X.apply(transform, axis=1, result_type='expand')\n",
    "\n",
    "    # move into range of -1 to 1\n",
    "    y = y.astype(np.float32)\n",
    "    y = np.tanh(y / 10.)\n",
    "    # y = sigmoid(y / 10.)\n",
    "    print(min(y), max(y))\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_history(history: History, index, folder: str):\n",
    "    plot(history.history['loss'],\n",
    "         history.history['val_loss'], 'loss', index, folder)\n",
    "    # plot(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', index)\n",
    "\n",
    "\n",
    "def plot(data: List, val_data: List, type: str, index, folder: str):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(data)\n",
    "    plt.plot(val_data)\n",
    "    plt.title(f'model {type}')\n",
    "    plt.ylabel(type)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(f'{folder}{type}{index}.png')\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def load_last_training_weights_file(model: Sequential, folder: str) -> Sequential:\n",
    "\n",
    "    # get the last filename in the sorted directory 'training'\n",
    "    last_files = sorted([\n",
    "        f for f in os.listdir(folder) if f.endswith('.h5')\n",
    "    ])\n",
    "\n",
    "    if len(last_files) > 0:\n",
    "        if 'weights' in last_files[-1]:\n",
    "            model.load_weights(f'{folder}{last_files[-1]}')\n",
    "        elif 'model' in last_files[-1]:\n",
    "            return keras.models.load_model(f'{folder}{last_files[-1]}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class Plotter(Callback):\n",
    "    batch_loss = []  # loss at given batch\n",
    "\n",
    "    def __init__(self, batches, folder):\n",
    "        super(Plotter, self).__init__()\n",
    "        self.batches = batches\n",
    "        self.folder = folder\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.current_batch += 1\n",
    "\n",
    "        Plotter.batch_loss.append(logs.get('loss'))\n",
    "\n",
    "        if self.current_batch % self.batches == 0:\n",
    "            plot(Plotter.batch_loss, Plotter.batch_loss, 'loss', '', self.folder)\n",
    "\n",
    "\n",
    "def getFile(url: str, path: str, limit: int = 1024) -> None:\n",
    "    # Limit in MB\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    print(f'Downloading {url}')\n",
    "\n",
    "    with open(path, 'wb') as file:\n",
    "        for i, block in tqdm.tqdm(enumerate(r.iter_content(chunk_size=1024 * 1024)), total=limit, unit='MB'):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "            if i > limit:\n",
    "                break\n",
    "\n",
    "\n",
    "def genFolder(folder: str) -> None:\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def delFolder(folder: str) -> None:\n",
    "    try:\n",
    "        shutil.rmtree(folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def delFile(file: str) -> None:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from bz2 import BZ2File\n",
    "from io import StringIO\n",
    "from typing import TextIO\n",
    "\n",
    "import chess\n",
    "import chess.pgn\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Conv3D, Dense,\n",
    "                                     Flatten, Reshape)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import History, TensorBoard\n",
    "\n",
    "out_files = {}\n",
    "\n",
    "\n",
    "def create_dataset(game: chess.pgn.Game, out: TextIO) -> None:\n",
    "    state = game.end()\n",
    "    while state:\n",
    "        evaluation = state.eval()\n",
    "\n",
    "        if evaluation is not None:\n",
    "            nums = board_to_bitfields(state.board(), state.turn())\n",
    "            evaluation = evaluation.relative.score(mate_score=10) / 100\n",
    "            evaluation = evaluation if state.turn() == chess.WHITE else -evaluation\n",
    "\n",
    "            out.write(','.join(map(str, nums)) + ',' + str(evaluation) + '\\n')\n",
    "\n",
    "        state = state.parent\n",
    "\n",
    "\n",
    "def process_game(lines: str, out_path: str) -> None:\n",
    "    game = chess.pgn.read_game(StringIO(lines))\n",
    "    if game is None:\n",
    "        return\n",
    "\n",
    "    cp = multiprocessing.current_process()\n",
    "    if cp.pid not in out_files:\n",
    "        out_files[cp.pid] = open(\n",
    "            f'{out_path[:-4]}.{cp.pid}{out_path[-4:]}', 'w', buffering=1024)\n",
    "\n",
    "    # if 'WhiteElo' in game.headers and 'BlackElo' in game.headers and \\\n",
    "    #         int(game.headers['WhiteElo']) > 2200 and int(game.headers['BlackElo']) > 2200:\n",
    "    #     create_dataset(game, out_files[cp.pid])\n",
    "    create_dataset(game, out_files[cp.pid])\n",
    "\n",
    "\n",
    "def preprocess(in_path: str, out_path: str, max: int) -> None:\n",
    "\n",
    "    print(f'Preprocessing {in_path}')\n",
    "\n",
    "    try:\n",
    "        with BZ2File(in_path, 'rb') as in_file:\n",
    "            with multiprocessing.Pool(os.cpu_count()-1) as pool:\n",
    "                for i, lines in tqdm.tqdm(enumerate(pager(in_file)), total=max, unit='games'):\n",
    "                    pool.apply_async(process_game, args=(str(lines), out_path))\n",
    "\n",
    "                    if i > max:\n",
    "                        break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for out_file in out_files.values():\n",
    "        out_file.close()\n",
    "    out_file = {}\n",
    "\n",
    "\n",
    "def unite(dir: str, out: str, ext: str) -> None:\n",
    "    \"\"\"\n",
    "    Unites all files in a directory into a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Uniting {dir}')\n",
    "\n",
    "    with open(out, 'w') as outfile:\n",
    "        for filename in os.listdir(dir):\n",
    "            if filename.endswith(ext):\n",
    "                with open(os.path.join(dir, filename)) as inFile:\n",
    "                    outfile.write(inFile.read())\n",
    "\n",
    "\n",
    "def gen_model() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(12 * 8 * 8,), activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    \"\"\" model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(2048, activation='relu')) \"\"\"\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        # metrics=['accuracy', 'mse']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((12, 8, 8, 1), input_shape=(12 * 64,)))\n",
    "    model.add(Conv3D(64, (12, 3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    for _ in range(15):\n",
    "        model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    \"\"\"model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\"\"\"\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    # model.add(Rescaling(scale=1 / 10., offset=0))\n",
    "    model.add(Dense(units=1, activation='tanh'))\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=Adam(learning_rate=0.01),\n",
    "        # metrics=['accuracy', 'mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(model: Sequential, X, y, index: int):\n",
    "    model.optimizer.learning_rate = 0.0005 / ((index + 1) * 2)\n",
    "\n",
    "    history: History = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(f'{TRAINING}{index:03d}weights.h5', save_weights_only=True, save_freq='epoch'),\n",
    "            # Plotter(batches=100, folder=TRAINING),\n",
    "            # access via tensorboard --logdir training/logs\n",
    "            # TensorBoard(log_dir=TRAINING + f'logs/{time()}.log')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plot_history(history, index, TRAINING)\n",
    "\n",
    "\n",
    "def test_model(model: str = None) -> None:\n",
    "    if not os.path.exists(TEST_GAMES):\n",
    "        testFile = DATASET + 'test.pgn.bz2'\n",
    "        getFile('https://database.lichess.org/standard/lichess_db_standard_rated_2021-12.pgn.bz2', testFile, 256)\n",
    "\n",
    "        delFolder(PROCESSED_TEST_GAMES)\n",
    "        genFolder(PROCESSED_TEST_GAMES)\n",
    "\n",
    "        preprocess(testFile, PROCESSED_TEST_GAMES + 'nm_games.csv', 200_000)\n",
    "        unite(PROCESSED_TEST_GAMES, TEST_GAMES, '.csv')\n",
    "        delFile(testFile)\n",
    "\n",
    "    if model is None:\n",
    "        model = gen_model()\n",
    "        model = load_last_training_weights_file(model, TRAINING)\n",
    "    else:\n",
    "        model = keras.models.load_model(model)\n",
    "\n",
    "    # test the model\n",
    "\n",
    "    for chunk in pd.read_csv(TEST_GAMES, header=None, chunksize=200):\n",
    "        X, y = create_training_data(chunk)\n",
    "\n",
    "        predictions = model.predict(X)\n",
    "\n",
    "        # print the results and evaluate the error\n",
    "        for v, p in zip(y, predictions):\n",
    "            print(f'actual: {v} prediction: {p} - loss: {abs(v - p)}')\n",
    "\n",
    "        total_loss = sum(abs(v - p) for v, p in zip(y, predictions))\n",
    "        print(f'total loss: {total_loss} average loss: {total_loss / len(y)}')\n",
    "        break\n",
    "\n",
    "\n",
    "def learn(dataset: str, iter: int = 0) -> None:\n",
    "    model = gen_model()\n",
    "    model.summary()\n",
    "    model = load_last_training_weights_file(model, TRAINING)\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(dataset, header=None, chunksize=50000)):\n",
    "        X, y = create_training_data(chunk)\n",
    "        train(model, X, y, i)\n",
    "\n",
    "        model.save(TRAINING + f'{iter:03d}model{i:03d}.h5')\n",
    "        model.save_weights(TRAINING + f'{iter:03d}weights{i:03d}.h5')\n",
    "\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(TRAINING + f'{iter:03d}model{i:03d}.h5')\n",
    "            files.download(TRAINING + f'{iter:03d}weights{i:03d}.h5')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'dataset/'\n",
    "PROCESSED_GAMES = DATASET + 'processed_games/'\n",
    "PROCESSED_TEST_GAMES = DATASET + 'processed_games_test/'\n",
    "TRAINING = 'training/'\n",
    "TEST_GAMES = DATASET + 'nm_games_test.csv'\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    onColab = True\n",
    "    maxGames = 2_200_000\n",
    "except:\n",
    "    onColab = False\n",
    "    maxGames = 20_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onColab:\n",
    "    delFolder(TRAINING)\n",
    "    delFolder(DATASET)\n",
    "    delFolder(\"sample_data\")\n",
    "    genFolder(TRAINING)\n",
    "    genFolder(DATASET)\n",
    "    genFolder(PROCESSED_GAMES)\n",
    "    genFolder(PROCESSED_TEST_GAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 12):\n",
    "    file = f'lichess_db_standard_rated_2021-{i:02d}.pgn.bz2'\n",
    "    games = f'{DATASET}nm_games{i:02d}.csv'\n",
    "\n",
    "    if not os.path.exists(games):\n",
    "        if not os.path.exists(DATASET + file):\n",
    "            getFile('https://database.lichess.org/standard/' + file, DATASET + file, 6 * 1024)\n",
    "\n",
    "        delFolder(PROCESSED_GAMES)\n",
    "        genFolder(PROCESSED_GAMES)\n",
    "\n",
    "        preprocess(DATASET + file, PROCESSED_GAMES + 'nm_games.csv', maxGames)\n",
    "        unite(PROCESSED_GAMES, games, '.csv')\n",
    "\n",
    "        if onColab:\n",
    "            delFile(DATASET + file)\n",
    "\n",
    "    learn(games, i)\n",
    "    test_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
