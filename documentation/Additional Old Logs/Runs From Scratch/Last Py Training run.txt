Last Py Training run


(Chess) [fu5896@hkn1991 py]$ tail train_zero_3098546.txt
[17.32.06] [INFO] Model and optimizer loaded from iteration 20
[17.32.33] [INFO] Evaluation results at iteration 19:
[17.32.33] [INFO]     Policy accuracy @1: 18.40%
[17.32.33] [INFO]     Policy accuracy @5: 45.21%
[17.32.33] [INFO]     Policy accuracy @10: 62.22%
[17.32.33] [INFO]     Avg value loss: 1.019020434220632
[17.35.45] [INFO] Results after playing two most recent models at iteration 19: Wins: 7, Losses: 10, Draws: 23
[17.36.44] [INFO] Results after playing the current vs the reference at iteration 19: Wins: 0, Losses: 40, Draws: 0
[17.37.52] [INFO] Results after playing vs random at iteration 19: Wins: 31, Losses: 0, Draws: 9
[17.42.31] [INFO] Loading memories for iteration 20 with window size 3 (17-20)
[17.42.45] [INFO] Loaded 3577355 samples from 20180 games
Training batches: 100%|█████████▉| 1745/1746 [01:58<00:00, 14.70it/s]
[17.44.44] [INFO] Last gradient norm: 0.494140625
[17.44.44] [INFO] Training stats: Policy Loss: 1.7625, Value Loss: 0.0594, Total Loss: 1.8219, Value Mean: 0.0010, Value Std: 0.7608
Validation batches: 100%|██████████| 81/81 [00:18<00:00,  4.38it/s]
[17.45.03] [INFO] Validation stats: Policy Loss: 1.6964, Value Loss: 0.0765, Total Loss: 1.7731, Value Mean: -0.0100, Value Std: 0.7276
[17.45.05] [INFO] Trainer finished at iteration 20.
[17.45.05] [INFO] All processes started at iteration 21.
[17.45.05] [INFO] Model and optimizer loaded from iteration 21
[17.45.30] [INFO] Evaluation results at iteration 20:
[17.45.30] [INFO]     Policy accuracy @1: 18.35%
[17.45.30] [INFO]     Policy accuracy @5: 44.95%
[17.45.30] [INFO]     Policy accuracy @10: 62.64%
[17.45.30] [INFO]     Avg value loss: 1.0710562626520792
[17.48.49] [INFO] Results after playing two most recent models at iteration 20: Wins: 6, Losses: 10, Draws: 24
[17.50.01] [INFO] Results after playing the current vs the reference at iteration 20: Wins: 0, Losses: 40, Draws: 0
[17.51.12] [INFO] Results after playing vs random at iteration 20: Wins: 30, Losses: 0, Draws: 10
[17.56.00] [INFO] Loading memories for iteration 21 with window size 3 (18-21)
[17.56.15] [INFO] Loaded 3243175 samples from 19500 games
Training batches: 100%|█████████▉| 1581/1583 [01:42<00:00, 15.38it/s]
[17.57.58] [INFO] Last gradient norm: 0.5859375
[17.57.58] [INFO] Training stats: Policy Loss: 1.7366, Value Loss: 0.0624, Total Loss: 1.7990, Value Mean: 0.0015, Value Std: 0.7587
Validation batches: 100%|██████████| 77/77 [00:16<00:00,  4.73it/s]
[17.58.14] [INFO] Validation stats: Policy Loss: 1.7237, Value Loss: 0.0747, Total Loss: 1.7981, Value Mean: 0.0258, Value Std: 0.7584
[17.58.16] [INFO] Trainer finished at iteration 21.
[17.58.16] [INFO] All processes started at iteration 22.
[17.58.16] [INFO] Model and optimizer loaded from iteration 22
[17.58.41] [INFO] Evaluation results at iteration 21:
[17.58.41] [INFO]     Policy accuracy @1: 18.06%
[17.58.41] [INFO]     Policy accuracy @5: 46.06%
[17.58.41] [INFO]     Policy accuracy @10: 63.96%
[17.58.41] [INFO]     Avg value loss: 0.9813426991303762
[18.01.59] [INFO] Results after playing two most recent models at iteration 21: Wins: 12, Losses: 12, Draws: 16
[18.03.17] [INFO] Results after playing the current vs the reference at iteration 21: Wins: 0, Losses: 40, Draws: 0
[18.04.19] [INFO] Results after playing vs random at iteration 21: Wins: 34, Losses: 0, Draws: 6
[18.09.00] [INFO] Loading memories for iteration 22 with window size 3 (19-22)
[18.09.17] [INFO] Loaded 3409730 samples from 20505 games
Training batches: 100%|█████████▉| 1662/1664 [01:53<00:00, 14.65it/s]
[18.11.11] [INFO] Last gradient norm: 0.5859375
[18.11.11] [INFO] Training stats: Policy Loss: 1.7134, Value Loss: 0.0601, Total Loss: 1.7735, Value Mean: 0.0013, Value Std: 0.7568
Validation batches: 100%|██████████| 82/82 [00:20<00:00,  3.92it/s]
[18.11.32] [INFO] Validation stats: Policy Loss: 1.6826, Value Loss: 0.0706, Total Loss: 1.7530, Value Mean: 0.0401, Value Std: 0.7640
[18.11.33] [INFO] Trainer finished at iteration 22.
[18.11.33] [INFO] All processes started at iteration 23.
[18.11.33] [INFO] Model and optimizer loaded from iteration 23
[18.11.58] [INFO] Evaluation results at iteration 22:
[18.11.58] [INFO]     Policy accuracy @1: 18.38%
[18.11.58] [INFO]     Policy accuracy @5: 46.96%
[18.11.58] [INFO]     Policy accuracy @10: 63.38%
[18.11.58] [INFO]     Avg value loss: 1.0304714779059092
[18.15.38] [INFO] Results after playing two most recent models at iteration 22: Wins: 12, Losses: 7, Draws: 21
[18.17.07] [INFO] Results after playing the current vs the reference at iteration 22: Wins: 1, Losses: 36, Draws: 3
[18.18.08] [INFO] Results after playing vs random at iteration 22: Wins: 34, Losses: 0, Draws: 6
[18.21.58] [INFO] Loading memories for iteration 23 with window size 3 (20-23)
[18.22.12] [INFO] Loaded 3308670 samples from 19905 games
Training batches: 100%|█████████▉| 1613/1615 [01:45<00:00, 15.27it/s]
[18.23.58] [INFO] Last gradient norm: 0.67578125
[18.23.58] [INFO] Training stats: Policy Loss: 1.6958, Value Loss: 0.0623, Total Loss: 1.7581, Value Mean: 0.0011, Value Std: 0.7551
Validation batches: 100%|██████████| 81/81 [00:18<00:00,  4.37it/s]
[18.24.16] [INFO] Validation stats: Policy Loss: 1.6279, Value Loss: 0.0556, Total Loss: 1.6834, Value Mean: -0.0144, Value Std: 0.7515
[18.24.18] [INFO] Trainer finished at iteration 23.
[18.24.18] [INFO] All processes started at iteration 24.
[18.24.18] [INFO] Model and optimizer loaded from iteration 24
[18.24.45] [INFO] Evaluation results at iteration 23:
[18.24.45] [INFO]     Policy accuracy @1: 18.67%
[18.24.45] [INFO]     Policy accuracy @5: 46.77%
[18.24.45] [INFO]     Policy accuracy @10: 64.15%
[18.24.45] [INFO]     Avg value loss: 1.0866901417573294
[18.27.47] [INFO] Results after playing two most recent models at iteration 23: Wins: 12, Losses: 8, Draws: 20
[18.29.08] [INFO] Results after playing the current vs the reference at iteration 23: Wins: 0, Losses: 40, Draws: 0
[18.30.28] [INFO] Results after playing vs random at iteration 23: Wins: 32, Losses: 0, Draws: 8
[18.34.52] [INFO] Loading memories for iteration 24 with window size 3 (21-24)
[18.35.07] [INFO] Loaded 3308725 samples from 19920 games
Training batches: 100%|█████████▉| 1613/1615 [01:48<00:00, 14.92it/s]
[18.36.55] [INFO] Last gradient norm: 0.6640625
[18.36.55] [INFO] Training stats: Policy Loss: 1.6757, Value Loss: 0.0636, Total Loss: 1.7393, Value Mean: 0.0002, Value Std: 0.7520
Validation batches: 100%|██████████| 81/81 [00:19<00:00,  4.17it/s]
[18.37.15] [INFO] Validation stats: Policy Loss: 1.6268, Value Loss: 0.0573, Total Loss: 1.6838, Value Mean: -0.0035, Value Std: 0.7623
[18.37.17] [INFO] Trainer finished at iteration 24.
[18.37.17] [INFO] All processes started at iteration 25.
[18.37.17] [INFO] Model and optimizer loaded from iteration 25
[18.37.40] [INFO] Evaluation results at iteration 24:
[18.37.40] [INFO]     Policy accuracy @1: 19.88%
[18.37.40] [INFO]     Policy accuracy @5: 47.96%
[18.37.40] [INFO]     Policy accuracy @10: 65.49%
[18.37.40] [INFO]     Avg value loss: 1.124549502134323
[18.40.55] [INFO] Results after playing two most recent models at iteration 24: Wins: 10, Losses: 8, Draws: 22
[18.42.18] [INFO] Results after playing the current vs the reference at iteration 24: Wins: 0, Losses: 38, Draws: 2
[18.43.27] [INFO] Results after playing vs random at iteration 24: Wins: 34, Losses: 0, Draws: 6
[18.47.54] [INFO] Loading memories for iteration 25 with window size 3 (22-25)
[18.48.09] [INFO] Loaded 3366105 samples from 20265 games
Training batches: 100%|█████████▉| 1641/1643 [01:46<00:00, 15.37it/s]
[18.49.56] [INFO] Last gradient norm: 0.7109375
[18.49.56] [INFO] Training stats: Policy Loss: 1.6583, Value Loss: 0.0621, Total Loss: 1.7204, Value Mean: 0.0003, Value Std: 0.7539
Validation batches: 100%|██████████| 83/83 [00:19<00:00,  4.20it/s]
[18.50.16] [INFO] Validation stats: Policy Loss: 1.6193, Value Loss: 0.0520, Total Loss: 1.6711, Value Mean: 0.0119, Value Std: 0.7705
[18.50.18] [INFO] Trainer finished at iteration 25.
[18.50.18] [INFO] All processes started at iteration 26.
[18.50.18] [INFO] Model and optimizer loaded from iteration 26
[18.50.45] [INFO] Evaluation results at iteration 25:
[18.50.45] [INFO]     Policy accuracy @1: 18.19%
[18.50.45] [INFO]     Policy accuracy @5: 46.22%
[18.50.45] [INFO]     Policy accuracy @10: 64.25%
[18.50.45] [INFO]     Avg value loss: 1.0343479931354522
[18.54.12] [INFO] Results after playing two most recent models at iteration 25: Wins: 10, Losses: 6, Draws: 24
[18.55.48] [INFO] Results after playing the current vs the reference at iteration 25: Wins: 1, Losses: 38, Draws: 1
[18.57.05] [INFO] Results after playing vs random at iteration 25: Wins: 29, Losses: 0, Draws: 11
[19.00.12] [INFO] Loading memories for iteration 26 with window size 3 (23-26)
[19.00.27] [INFO] Loaded 3331280 samples from 20055 games
Training batches: 100%|█████████▉| 1624/1626 [01:45<00:00, 15.44it/s]
[19.02.13] [INFO] Last gradient norm: 0.68359375
[19.02.13] [INFO] Training stats: Policy Loss: 1.6460, Value Loss: 0.0622, Total Loss: 1.7082, Value Mean: 0.0008, Value Std: 0.7530
Validation batches: 100%|██████████| 78/78 [00:18<00:00,  4.17it/s]
[19.02.31] [INFO] Validation stats: Policy Loss: 1.6215, Value Loss: 0.0589, Total Loss: 1.6804, Value Mean: -0.0325, Value Std: 0.7534
[19.02.33] [INFO] Trainer finished at iteration 26.
[19.02.33] [INFO] All processes started at iteration 27.
[19.02.33] [INFO] Model and optimizer loaded from iteration 27
[19.02.59] [INFO] Evaluation results at iteration 26:
[19.02.59] [INFO]     Policy accuracy @1: 20.04%
[19.02.59] [INFO]     Policy accuracy @5: 48.63%
[19.02.59] [INFO]     Policy accuracy @10: 66.37%
[19.02.59] [INFO]     Avg value loss: 1.0283686161041259
[19.06.14] [INFO] Results after playing two most recent models at iteration 26: Wins: 14, Losses: 7, Draws: 19
[19.07.51] [INFO] Results after playing the current vs the reference at iteration 26: Wins: 0, Losses: 37, Draws: 3
[19.08.55] [INFO] Results after playing vs random at iteration 26: Wins: 30, Losses: 0, Draws: 10
[19.12.39] [INFO] Loading memories for iteration 27 with window size 3 (24-27)
[19.12.53] [INFO] Loaded 3271165 samples from 19695 games
Training batches: 100%|█████████▉| 1596/1597 [01:45<00:00, 15.17it/s]
[19.14.39] [INFO] Last gradient norm: 0.60546875
[19.14.39] [INFO] Training stats: Policy Loss: 1.6412, Value Loss: 0.0639, Total Loss: 1.7051, Value Mean: 0.0012, Value Std: 0.7513
Validation batches: 100%|██████████| 76/76 [00:17<00:00,  4.27it/s]
[19.14.57] [INFO] Validation stats: Policy Loss: 1.5884, Value Loss: 0.0623, Total Loss: 1.6508, Value Mean: -0.0012, Value Std: 0.7420
[19.14.59] [INFO] Trainer finished at iteration 27.
[19.14.59] [INFO] All processes started at iteration 28.
[19.14.59] [INFO] Model and optimizer loaded from iteration 28
[19.15.22] [INFO] Evaluation results at iteration 27:
[19.15.22] [INFO]     Policy accuracy @1: 19.78%
[19.15.22] [INFO]     Policy accuracy @5: 46.72%
[19.15.22] [INFO]     Policy accuracy @10: 65.42%
[19.15.22] [INFO]     Avg value loss: 1.015562931696574
[19.19.04] [INFO] Results after playing two most recent models at iteration 27: Wins: 8, Losses: 12, Draws: 20
[19.20.30] [INFO] Results after playing the current vs the reference at iteration 27: Wins: 0, Losses: 39, Draws: 1
[19.21.50] [INFO] Results after playing vs random at iteration 27: Wins: 30, Losses: 0, Draws: 10
[19.25.23] [INFO] Loading memories for iteration 28 with window size 3 (25-28)
[19.25.37] [INFO] Loaded 3255040 samples from 19605 games
Training batches: 100%|█████████▉| 1587/1589 [01:43<00:00, 15.38it/s]
[19.27.21] [INFO] Last gradient norm: 0.60546875
[19.27.21] [INFO] Training stats: Policy Loss: 1.6211, Value Loss: 0.0627, Total Loss: 1.6839, Value Mean: 0.0009, Value Std: 0.7539
Validation batches: 100%|██████████| 79/79 [00:20<00:00,  3.80it/s]
[19.27.41] [INFO] Validation stats: Policy Loss: 1.5568, Value Loss: 0.0642, Total Loss: 1.6209, Value Mean: -0.0081, Value Std: 0.7386
[19.27.43] [INFO] Trainer finished at iteration 28.
[19.27.43] [INFO] All processes started at iteration 29.
[19.27.43] [INFO] Model and optimizer loaded from iteration 29
[19.28.08] [INFO] Evaluation results at iteration 28:
[19.28.08] [INFO]     Policy accuracy @1: 19.83%
[19.28.08] [INFO]     Policy accuracy @5: 47.88%
[19.28.08] [INFO]     Policy accuracy @10: 66.37%
[19.28.08] [INFO]     Avg value loss: 1.0086265206336975
[19.31.21] [INFO] Results after playing two most recent models at iteration 28: Wins: 12, Losses: 10, Draws: 18
[19.32.41] [INFO] Results after playing the current vs the reference at iteration 28: Wins: 0, Losses: 40, Draws: 0
[19.33.42] [INFO] Results after playing vs random at iteration 28: Wins: 33, Losses: 0, Draws: 7
[19.37.59] [INFO] Loading memories for iteration 29 with window size 3 (26-29)
[19.38.13] [INFO] Loaded 3169910 samples from 19095 games
Training batches: 100%|█████████▉| 1545/1547 [01:37<00:00, 15.79it/s]
[19.39.51] [INFO] Last gradient norm: 0.69140625
[19.39.51] [INFO] Training stats: Policy Loss: 1.6091, Value Loss: 0.0641, Total Loss: 1.6732, Value Mean: 0.0002, Value Std: 0.7565
Validation batches: 100%|██████████| 74/74 [00:17<00:00,  4.13it/s]
[19.40.09] [INFO] Validation stats: Policy Loss: 1.5752, Value Loss: 0.0530, Total Loss: 1.6283, Value Mean: 0.0021, Value Std: 0.7512
[19.40.11] [INFO] Trainer finished at iteration 29.
[19.40.11] [INFO] All processes started at iteration 30.
[19.40.11] [INFO] Model and optimizer loaded from iteration 30
[19.40.37] [INFO] Evaluation results at iteration 29:
[19.40.37] [INFO]     Policy accuracy @1: 20.70%
[19.40.37] [INFO]     Policy accuracy @5: 48.92%
[19.40.37] [INFO]     Policy accuracy @10: 67.19%
[19.40.37] [INFO]     Avg value loss: 1.032900987068812
[19.43.20] [INFO] Results after playing two most recent models at iteration 29: Wins: 18, Losses: 9, Draws: 13
[19.44.43] [INFO] Results after playing the current vs the reference at iteration 29: Wins: 0, Losses: 37, Draws: 3
[19.45.56] [INFO] Results after playing vs random at iteration 29: Wins: 30, Losses: 0, Draws: 10
[19.50.25] [INFO] Loading memories for iteration 30 with window size 3 (27-30)
[19.50.40] [INFO] Loaded 3186590 samples from 19185 games
Training batches: 100%|█████████▉| 1553/1555 [01:36<00:00, 16.05it/s]
[19.52.17] [INFO] Last gradient norm: 0.66015625
[19.52.17] [INFO] Training stats: Policy Loss: 1.5949, Value Loss: 0.0646, Total Loss: 1.6596, Value Mean: 0.0001, Value Std: 0.7560
Validation batches: 100%|██████████| 80/80 [00:17<00:00,  4.67it/s]
[19.52.34] [INFO] Validation stats: Policy Loss: 1.5378, Value Loss: 0.0444, Total Loss: 1.5822, Value Mean: -0.0083, Value Std: 0.7431
[19.52.36] [INFO] Trainer finished at iteration 30.
[19.52.36] [INFO] All processes started at iteration 31.
[19.52.36] [INFO] Model and optimizer loaded from iteration 31
[19.53.01] [INFO] Evaluation results at iteration 30:
[19.53.01] [INFO]     Policy accuracy @1: 20.86%
[19.53.01] [INFO]     Policy accuracy @5: 49.00%
[19.53.01] [INFO]     Policy accuracy @10: 66.79%
[19.53.01] [INFO]     Avg value loss: 1.0522753099600475
[19.56.26] [INFO] Results after playing two most recent models at iteration 30: Wins: 11, Losses: 11, Draws: 18
[19.57.39] [INFO] Results after playing the current vs the reference at iteration 30: Wins: 1, Losses: 38, Draws: 1
[19.58.48] [INFO] Results after playing vs random at iteration 30: Wins: 29, Losses: 0, Draws: 11
[20.03.01] [INFO] Loading memories for iteration 31 with window size 3 (28-31)
[20.03.14] [INFO] Loaded 3231950 samples from 19455 games
Training batches: 100%|█████████▉| 1575/1578 [01:37<00:00, 16.21it/s]
[20.04.51] [INFO] Last gradient norm: 0.671875
[20.04.51] [INFO] Training stats: Policy Loss: 1.5791, Value Loss: 0.0621, Total Loss: 1.6412, Value Mean: 0.0001, Value Std: 0.7574
Validation batches: 100%|██████████| 80/80 [00:17<00:00,  4.49it/s]
[20.05.09] [INFO] Validation stats: Policy Loss: 1.5248, Value Loss: 0.0473, Total Loss: 1.5721, Value Mean: 0.0268, Value Std: 0.7625
[20.05.11] [INFO] Trainer finished at iteration 31.
[20.05.11] [INFO] All processes started at iteration 32.
[20.05.11] [INFO] Model and optimizer loaded from iteration 32
[20.05.37] [INFO] Evaluation results at iteration 31:
[20.05.37] [INFO]     Policy accuracy @1: 21.05%
[20.05.37] [INFO]     Policy accuracy @5: 50.50%
[20.05.37] [INFO]     Policy accuracy @10: 67.77%
[20.05.37] [INFO]     Avg value loss: 0.9422361254692078
[20.08.52] [INFO] Results after playing two most recent models at iteration 31: Wins: 10, Losses: 12, Draws: 18
[20.10.20] [INFO] Results after playing the current vs the reference at iteration 31: Wins: 1, Losses: 38, Draws: 1
[20.11.20] [INFO] Results after playing vs random at iteration 31: Wins: 31, Losses: 0, Draws: 9
[20.15.45] [INFO] Loading memories for iteration 32 with window size 3 (29-32)
[20.16.01] [INFO] Loaded 3227705 samples from 19425 games
Training batches: 100%|█████████▉| 1573/1576 [01:41<00:00, 15.48it/s]
[20.17.42] [INFO] Last gradient norm: 0.6875
[20.17.42] [INFO] Training stats: Policy Loss: 1.5669, Value Loss: 0.0644, Total Loss: 1.6313, Value Mean: 0.0009, Value Std: 0.7573
Validation batches: 100%|██████████| 79/79 [00:17<00:00,  4.40it/s]
[20.18.00] [INFO] Validation stats: Policy Loss: 1.5171, Value Loss: 0.0678, Total Loss: 1.5851, Value Mean: 0.0520, Value Std: 0.7765
[20.18.02] [INFO] Trainer finished at iteration 32.
[20.18.02] [INFO] All processes started at iteration 33.
[20.18.02] [INFO] Model and optimizer loaded from iteration 33
[20.18.26] [INFO] Evaluation results at iteration 32:
[20.18.26] [INFO]     Policy accuracy @1: 21.29%
[20.18.26] [INFO]     Policy accuracy @5: 51.45%
[20.18.26] [INFO]     Policy accuracy @10: 67.77%
[20.18.26] [INFO]     Avg value loss: 0.9614282310009002
[20.21.45] [INFO] Results after playing two most recent models at iteration 32: Wins: 8, Losses: 12, Draws: 20
[20.23.19] [INFO] Results after playing the current vs the reference at iteration 32: Wins: 0, Losses: 35, Draws: 5
[20.24.16] [INFO] Results after playing vs random at iteration 32: Wins: 32, Losses: 0, Draws: 8
[20.28.08] [INFO] Loading memories for iteration 33 with window size 3 (30-33)
[20.28.22] [INFO] Loaded 3258785 samples from 19620 games
Training batches: 100%|█████████▉| 1588/1591 [01:38<00:00, 16.12it/s]
[20.30.01] [INFO] Last gradient norm: 0.6953125
[20.30.01] [INFO] Training stats: Policy Loss: 1.5489, Value Loss: 0.0631, Total Loss: 1.6119, Value Mean: 0.0009, Value Std: 0.7578
Validation batches: 100%|██████████| 77/77 [00:18<00:00,  4.13it/s]
[20.30.19] [INFO] Validation stats: Policy Loss: 1.4889, Value Loss: 0.0673, Total Loss: 1.5561, Value Mean: 0.0374, Value Std: 0.7575
[20.30.21] [INFO] Trainer finished at iteration 33.
[20.30.21] [INFO] All processes started at iteration 34.
[20.30.21] [INFO] Model and optimizer loaded from iteration 34
[20.30.47] [INFO] Evaluation results at iteration 33:
[20.30.47] [INFO]     Policy accuracy @1: 21.76%
[20.30.47] [INFO]     Policy accuracy @5: 51.11%
[20.30.47] [INFO]     Policy accuracy @10: 67.37%
[20.30.47] [INFO]     Avg value loss: 0.9879840910434723
[20.33.38] [INFO] Results after playing two most recent models at iteration 33: Wins: 13, Losses: 10, Draws: 17
[20.35.01] [INFO] Results after playing the current vs the reference at iteration 33: Wins: 3, Losses: 36, Draws: 1
[20.35.54] [INFO] Results after playing vs random at iteration 33: Wins: 34, Losses: 0, Draws: 6
[20.40.35] [INFO] Loading memories for iteration 34 with window size 3 (31-34)
[20.40.50] [INFO] Loaded 3255600 samples from 19605 games
Training batches: 100%|█████████▉| 1587/1589 [01:44<00:00, 15.26it/s]
[20.42.34] [INFO] Last gradient norm: 0.65234375
[20.42.34] [INFO] Training stats: Policy Loss: 1.5307, Value Loss: 0.0629, Total Loss: 1.5937, Value Mean: 0.0009, Value Std: 0.7585
Validation batches: 100%|██████████| 80/80 [00:19<00:00,  4.04it/s]
[20.42.54] [INFO] Validation stats: Policy Loss: 1.4748, Value Loss: 0.0748, Total Loss: 1.5498, Value Mean: 0.0680, Value Std: 0.7535
[20.42.56] [INFO] Trainer finished at iteration 34.
[20.42.56] [INFO] All processes started at iteration 35.
[20.42.56] [INFO] Model and optimizer loaded from iteration 35
[20.43.19] [INFO] Evaluation results at iteration 34:
[20.43.19] [INFO]     Policy accuracy @1: 21.81%
[20.43.19] [INFO]     Policy accuracy @5: 51.03%
[20.43.19] [INFO]     Policy accuracy @10: 67.64%
[20.43.19] [INFO]     Avg value loss: 1.0349499086538951
[20.46.18] [INFO] Results after playing two most recent models at iteration 34: Wins: 10, Losses: 4, Draws: 26
[20.47.52] [INFO] Results after playing the current vs the reference at iteration 34: Wins: 0, Losses: 37, Draws: 3
[20.48.57] [INFO] Results after playing vs random at iteration 34: Wins: 32, Losses: 0, Draws: 8
[20.52.51] [INFO] Loading memories for iteration 35 with window size 3 (32-35)
[20.53.04] [INFO] Loaded 3201220 samples from 19275 games
Training batches: 100%|█████████▉| 1561/1563 [01:41<00:00, 15.35it/s]
[20.54.47] [INFO] Last gradient norm: 0.671875
[20.54.47] [INFO] Training stats: Policy Loss: 1.5141, Value Loss: 0.0648, Total Loss: 1.5788, Value Mean: 0.0001, Value Std: 0.7580
Validation batches: 100%|██████████| 75/75 [00:18<00:00,  4.04it/s]
[20.55.05] [INFO] Validation stats: Policy Loss: 1.4708, Value Loss: 0.0628, Total Loss: 1.5337, Value Mean: 0.0056, Value Std: 0.7640
[20.55.07] [INFO] Trainer finished at iteration 35.
[20.55.07] [INFO] All processes started at iteration 36.
[20.55.07] [INFO] Model and optimizer loaded from iteration 36
[20.55.30] [INFO] Evaluation results at iteration 35:
[20.55.30] [INFO]     Policy accuracy @1: 21.60%
[20.55.30] [INFO]     Policy accuracy @5: 51.85%
[20.55.30] [INFO]     Policy accuracy @10: 69.01%
[20.55.30] [INFO]     Avg value loss: 0.9914114395777385
[20.58.20] [INFO] Results after playing two most recent models at iteration 35: Wins: 16, Losses: 5, Draws: 19
[20.59.54] [INFO] Results after playing the current vs the reference at iteration 35: Wins: 0, Losses: 37, Draws: 3
[21.00.45] [INFO] Results after playing vs random at iteration 35: Wins: 35, Losses: 0, Draws: 5
[21.05.43] [INFO] Loading memories for iteration 36 with window size 3 (33-36)
[21.05.56] [INFO] Loaded 3198630 samples from 19260 games
Training batches: 100%|█████████▉| 1560/1561 [01:37<00:00, 15.93it/s]
[21.07.35] [INFO] Last gradient norm: 0.7109375
[21.07.35] [INFO] Training stats: Policy Loss: 1.4958, Value Loss: 0.0642, Total Loss: 1.5600, Value Mean: -0.0005, Value Std: 0.7566
Validation batches: 100%|██████████| 79/79 [00:19<00:00,  3.98it/s]
[21.07.55] [INFO] Validation stats: Policy Loss: 1.4376, Value Loss: 0.0521, Total Loss: 1.4891, Value Mean: -0.0387, Value Std: 0.7717
[21.07.56] [INFO] Trainer finished at iteration 36.
[21.07.57] [INFO] All processes started at iteration 37.
[21.07.57] [INFO] Model and optimizer loaded from iteration 37
[21.08.20] [INFO] Evaluation results at iteration 36:
[21.08.20] [INFO]     Policy accuracy @1: 22.74%
[21.08.20] [INFO]     Policy accuracy @5: 51.30%
[21.08.20] [INFO]     Policy accuracy @10: 67.98%
[21.08.20] [INFO]     Avg value loss: 1.0803412199020386
[21.11.16] [INFO] Results after playing two most recent models at iteration 36: Wins: 10, Losses: 12, Draws: 18
[21.13.06] [INFO] Results after playing the current vs the reference at iteration 36: Wins: 0, Losses: 39, Draws: 1
[21.14.09] [INFO] Results after playing vs random at iteration 36: Wins: 30, Losses: 0, Draws: 10
[21.17.51] [INFO] Loading memories for iteration 37 with window size 3 (34-37)
[21.18.05] [INFO] Loaded 3207825 samples from 19290 games
Training batches: 100%|█████████▉| 1563/1566 [01:38<00:00, 15.91it/s]
[21.19.44] [INFO] Last gradient norm: 0.6953125
[21.19.44] [INFO] Training stats: Policy Loss: 1.4845, Value Loss: 0.0641, Total Loss: 1.5486, Value Mean: -0.0005, Value Std: 0.7547
Validation batches: 100%|██████████| 78/78 [00:18<00:00,  4.16it/s]
[21.20.02] [INFO] Validation stats: Policy Loss: 1.4421, Value Loss: 0.0577, Total Loss: 1.4995, Value Mean: 0.0136, Value Std: 0.7569
[21.20.05] [INFO] Trainer finished at iteration 37.
[21.20.05] [INFO] All processes started at iteration 38.
[21.20.05] [INFO] Model and optimizer loaded from iteration 38
[21.20.27] [INFO] Evaluation results at iteration 37:
[21.20.27] [INFO]     Policy accuracy @1: 21.81%
[21.20.27] [INFO]     Policy accuracy @5: 52.25%
[21.20.27] [INFO]     Policy accuracy @10: 67.72%
[21.20.27] [INFO]     Avg value loss: 1.083825906117757
[21.23.06] [INFO] Results after playing two most recent models at iteration 37: Wins: 8, Losses: 14, Draws: 18
[21.24.35] [INFO] Results after playing the current vs the reference at iteration 37: Wins: 0, Losses: 37, Draws: 3
[21.25.46] [INFO] Results after playing vs random at iteration 37: Wins: 29, Losses: 0, Draws: 11
[21.30.20] [INFO] Loading memories for iteration 38 with window size 3 (35-38)
[21.30.34] [INFO] Loaded 3134575 samples from 18855 games
Training batches: 100%|█████████▉| 1528/1530 [01:31<00:00, 16.65it/s]
[21.32.06] [INFO] Last gradient norm: 0.66015625
[21.32.06] [INFO] Training stats: Policy Loss: 1.4778, Value Loss: 0.0671, Total Loss: 1.5449, Value Mean: 0.0002, Value Std: 0.7546
Validation batches: 100%|██████████| 73/73 [00:16<00:00,  4.50it/s]
[21.32.22] [INFO] Validation stats: Policy Loss: 1.4327, Value Loss: 0.0592, Total Loss: 1.4920, Value Mean: 0.0047, Value Std: 0.7504
[21.32.24] [INFO] Trainer finished at iteration 38.
[21.32.24] [INFO] All processes started at iteration 39.
[21.32.24] [INFO] Model and optimizer loaded from iteration 39
[21.32.50] [INFO] Evaluation results at iteration 38:
[21.32.50] [INFO]     Policy accuracy @1: 22.42%
[21.32.50] [INFO]     Policy accuracy @5: 52.25%
[21.32.50] [INFO]     Policy accuracy @10: 68.46%
[21.32.50] [INFO]     Avg value loss: 1.0335412720839183
[21.35.34] [INFO] Results after playing two most recent models at iteration 38: Wins: 10, Losses: 15, Draws: 15
[21.36.58] [INFO] Results after playing the current vs the reference at iteration 38: Wins: 1, Losses: 36, Draws: 3
[21.37.47] [INFO] Results after playing vs random at iteration 38: Wins: 36, Losses: 0, Draws: 4
[21.42.29] [INFO] Loading memories for iteration 39 with window size 3 (36-39)
[21.42.43] [INFO] Loaded 3179455 samples from 19140 games
Training batches: 100%|█████████▉| 1549/1552 [01:40<00:00, 15.45it/s]
[21.44.23] [INFO] Last gradient norm: 0.6875
[21.44.23] [INFO] Training stats: Policy Loss: 1.4608, Value Loss: 0.0640, Total Loss: 1.5247, Value Mean: 0.0001, Value Std: 0.7545
Validation batches: 100%|██████████| 79/79 [00:15<00:00,  5.04it/s]
[21.44.39] [INFO] Validation stats: Policy Loss: 1.4006, Value Loss: 0.0487, Total Loss: 1.4496, Value Mean: 0.0361, Value Std: 0.7526
[21.44.40] [INFO] Trainer finished at iteration 39.
[21.44.40] [INFO] All processes started at iteration 40.
[21.44.41] [INFO] Model and optimizer loaded from iteration 40
[21.45.07] [INFO] Evaluation results at iteration 39:
[21.45.07] [INFO]     Policy accuracy @1: 22.66%
[21.45.07] [INFO]     Policy accuracy @5: 52.88%
[21.45.07] [INFO]     Policy accuracy @10: 68.85%
[21.45.07] [INFO]     Avg value loss: 0.9715289115905762
[21.48.15] [INFO] Results after playing two most recent models at iteration 39: Wins: 9, Losses: 8, Draws: 23
[21.49.46] [INFO] Results after playing the current vs the reference at iteration 39: Wins: 1, Losses: 38, Draws: 1
[21.50.39] [INFO] Results after playing vs random at iteration 39: Wins: 29, Losses: 0, Draws: 11
[21.54.35] [INFO] Loading memories for iteration 40 with window size 3 (37-40)
[21.54.48] [INFO] Loaded 3128235 samples from 18840 games
Training batches: 100%|█████████▉| 1525/1527 [01:37<00:00, 15.66it/s]
[21.56.26] [INFO] Last gradient norm: 0.66015625
[21.56.26] [INFO] Training stats: Policy Loss: 1.4534, Value Loss: 0.0642, Total Loss: 1.5177, Value Mean: -0.0001, Value Std: 0.7531
Validation batches: 100%|██████████| 74/74 [00:17<00:00,  4.25it/s]
[21.56.43] [INFO] Validation stats: Policy Loss: 1.4112, Value Loss: 0.0701, Total Loss: 1.4811, Value Mean: 0.0364, Value Std: 0.7298
[21.56.45] [INFO] Trainer finished at iteration 40.
[21.56.45] [INFO] All processes started at iteration 41.
[21.56.45] [INFO] Model and optimizer loaded from iteration 41
[21.57.10] [INFO] Evaluation results at iteration 40:
[21.57.10] [INFO]     Policy accuracy @1: 22.61%
[21.57.10] [INFO]     Policy accuracy @5: 52.41%
[21.57.10] [INFO]     Policy accuracy @10: 68.88%
[21.57.10] [INFO]     Avg value loss: 0.8969698826471965
[22.00.14] [INFO] Results after playing two most recent models at iteration 40: Wins: 16, Losses: 5, Draws: 19
[22.01.43] [INFO] Results after playing the current vs the reference at iteration 40: Wins: 0, Losses: 37, Draws: 3
[22.02.49] [INFO] Results after playing vs random at iteration 40: Wins: 32, Losses: 0, Draws: 8
[22.07.00] [INFO] Loading memories for iteration 41 with window size 3 (38-41)
[22.07.14] [INFO] Loaded 3106365 samples from 18705 games
Training batches: 100%|█████████▉| 1515/1516 [01:35<00:00, 15.88it/s]
[22.08.49] [INFO] Last gradient norm: 0.66015625
[22.08.49] [INFO] Training stats: Policy Loss: 1.4441, Value Loss: 0.0647, Total Loss: 1.5088, Value Mean: -0.0006, Value Std: 0.7532
Validation batches: 100%|██████████| 76/76 [00:18<00:00,  4.19it/s]
[22.09.07] [INFO] Validation stats: Policy Loss: 1.3886, Value Loss: 0.0469, Total Loss: 1.4354, Value Mean: -0.0152, Value Std: 0.7557
[22.09.09] [INFO] Trainer finished at iteration 41.
[22.09.09] [INFO] All processes started at iteration 42.
[22.09.09] [INFO] Model and optimizer loaded from iteration 42
[22.09.33] [INFO] Evaluation results at iteration 41:
[22.09.33] [INFO]     Policy accuracy @1: 22.50%
[22.09.33] [INFO]     Policy accuracy @5: 53.36%
[22.09.33] [INFO]     Policy accuracy @10: 69.06%
[22.09.33] [INFO]     Avg value loss: 0.9243123372395833
[22.12.21] [INFO] Results after playing two most recent models at iteration 41: Wins: 15, Losses: 6, Draws: 19
[22.13.55] [INFO] Results after playing the current vs the reference at iteration 41: Wins: 0, Losses: 38, Draws: 2
[22.14.40] [INFO] Results after playing vs random at iteration 41: Wins: 36, Losses: 0, Draws: 4
[22.19.34] [INFO] Loading memories for iteration 42 with window size 3 (39-42)
[22.19.48] [INFO] Loaded 3147740 samples from 18945 games
Training batches: 100%|██████████| 1536/1536 [01:37<00:00, 15.74it/s]
[22.21.26] [INFO] Last gradient norm: 0.6484375
[22.21.26] [INFO] Training stats: Policy Loss: 1.4387, Value Loss: 0.0636, Total Loss: 1.5023, Value Mean: -0.0002, Value Std: 0.7533
Validation batches: 100%|██████████| 77/77 [00:19<00:00,  3.99it/s]
[22.21.45] [INFO] Validation stats: Policy Loss: 1.3905, Value Loss: 0.0481, Total Loss: 1.4380, Value Mean: -0.0032, Value Std: 0.7513
[22.21.47] [INFO] Trainer finished at iteration 42.
[22.21.47] [INFO] All processes started at iteration 43.
[22.21.47] [INFO] Model and optimizer loaded from iteration 43
[22.22.11] [INFO] Evaluation results at iteration 42:
[22.22.11] [INFO]     Policy accuracy @1: 22.50%
[22.22.11] [INFO]     Policy accuracy @5: 52.86%
[22.22.11] [INFO]     Policy accuracy @10: 69.70%
[22.22.11] [INFO]     Avg value loss: 1.0257855852444966
[22.24.40] [INFO] Results after playing two most recent models at iteration 42: Wins: 10, Losses: 14, Draws: 16
[22.26.09] [INFO] Results after playing the current vs the reference at iteration 42: Wins: 2, Losses: 36, Draws: 2
[22.26.57] [INFO] Results after playing vs random at iteration 42: Wins: 34, Losses: 0, Draws: 6
[22.31.50] [INFO] Loading memories for iteration 43 with window size 3 (40-43)
[22.32.04] [INFO] Loaded 3122450 samples from 18795 games
Training batches: 100%|█████████▉| 1523/1524 [01:36<00:00, 15.84it/s]
[22.33.40] [INFO] Last gradient norm: 0.8125
[22.33.40] [INFO] Training stats: Policy Loss: 1.4389, Value Loss: 0.0643, Total Loss: 1.5031, Value Mean: 0.0004, Value Std: 0.7529
Validation batches: 100%|██████████| 77/77 [00:19<00:00,  3.85it/s]
[22.34.00] [INFO] Validation stats: Policy Loss: 1.4159, Value Loss: 0.0564, Total Loss: 1.4723, Value Mean: -0.0478, Value Std: 0.7653
[22.34.02] [INFO] Trainer finished at iteration 43.
[22.34.02] [INFO] All processes started at iteration 44.
[22.34.02] [INFO] Model and optimizer loaded from iteration 44
[22.34.27] [INFO] Evaluation results at iteration 43:
[22.34.27] [INFO]     Policy accuracy @1: 22.95%
[22.34.27] [INFO]     Policy accuracy @5: 53.83%
[22.34.27] [INFO]     Policy accuracy @10: 69.22%
[22.34.27] [INFO]     Avg value loss: 1.0181627174218495
[22.37.16] [INFO] Results after playing two most recent models at iteration 43: Wins: 11, Losses: 11, Draws: 18
[22.38.45] [INFO] Results after playing the current vs the reference at iteration 43: Wins: 2, Losses: 37, Draws: 1
[22.39.47] [INFO] Results after playing vs random at iteration 43: Wins: 29, Losses: 0, Draws: 11
[22.44.16] [INFO] Loading memories for iteration 44 with window size 3 (41-44)
[22.44.31] [INFO] Loaded 3154210 samples from 18975 games
Training batches: 100%|█████████▉| 1538/1540 [01:35<00:00, 16.02it/s]
[22.46.07] [INFO] Last gradient norm: 0.671875
[22.46.07] [INFO] Training stats: Policy Loss: 1.4335, Value Loss: 0.0632, Total Loss: 1.4966, Value Mean: 0.0003, Value Std: 0.7555
Validation batches: 100%|██████████| 77/77 [00:18<00:00,  4.12it/s]
[22.46.25] [INFO] Validation stats: Policy Loss: 1.4064, Value Loss: 0.0544, Total Loss: 1.4610, Value Mean: -0.0253, Value Std: 0.7450
[22.46.27] [INFO] Trainer finished at iteration 44.
[22.46.28] [INFO] All processes started at iteration 45.
[22.46.28] [INFO] Model and optimizer loaded from iteration 45
[22.46.51] [INFO] Evaluation results at iteration 44:
[22.46.51] [INFO]     Policy accuracy @1: 24.03%
[22.46.51] [INFO]     Policy accuracy @5: 54.60%
[22.46.51] [INFO]     Policy accuracy @10: 69.88%
[22.46.51] [INFO]     Avg value loss: 1.0258814136187235
[22.49.46] [INFO] Results after playing two most recent models at iteration 44: Wins: 9, Losses: 7, Draws: 24
[22.51.17] [INFO] Results after playing the current vs the reference at iteration 44: Wins: 1, Losses: 36, Draws: 3
[22.52.08] [INFO] Results after playing vs random at iteration 44: Wins: 34, Losses: 0, Draws: 6
[22.56.11] [INFO] Loading memories for iteration 45 with window size 3 (42-45)
[22.56.25] [INFO] Loaded 3147310 samples from 18930 games
Training batches: 100%|█████████▉| 1534/1536 [01:36<00:00, 15.93it/s]
[22.58.02] [INFO] Last gradient norm: 0.671875
[22.58.02] [INFO] Training stats: Policy Loss: 1.4376, Value Loss: 0.0628, Total Loss: 1.5004, Value Mean: -0.0002, Value Std: 0.7574
Validation batches: 100%|██████████| 75/75 [00:17<00:00,  4.22it/s]
[22.58.19] [INFO] Validation stats: Policy Loss: 1.4100, Value Loss: 0.0586, Total Loss: 1.4684, Value Mean: -0.0421, Value Std: 0.7631
[22.58.21] [INFO] Trainer finished at iteration 45.
[22.58.21] [INFO] All processes started at iteration 46.
[22.58.21] [INFO] Model and optimizer loaded from iteration 46
[22.58.45] [INFO] Evaluation results at iteration 45:
[22.58.45] [INFO]     Policy accuracy @1: 24.43%
[22.58.45] [INFO]     Policy accuracy @5: 55.02%
[22.58.45] [INFO]     Policy accuracy @10: 71.10%
[22.58.45] [INFO]     Avg value loss: 0.9438336789608002
[23.01.51] [INFO] Results after playing two most recent models at iteration 45: Wins: 6, Losses: 10, Draws: 24
[23.03.23] [INFO] Results after playing the current vs the reference at iteration 45: Wins: 0, Losses: 34, Draws: 6
[23.04.13] [INFO] Results after playing vs random at iteration 45: Wins: 30, Losses: 0, Draws: 10
[23.08.26] [INFO] Loading memories for iteration 46 with window size 3 (43-46)
[23.08.40] [INFO] Loaded 3142020 samples from 18900 games
Training batches: 100%|█████████▉| 1532/1534 [01:35<00:00, 15.96it/s]
[23.10.17] [INFO] Last gradient norm: 0.7578125
[23.10.17] [INFO] Training stats: Policy Loss: 1.4400, Value Loss: 0.0644, Total Loss: 1.5044, Value Mean: -0.0005, Value Std: 0.7570
Validation batches: 100%|██████████| 76/76 [00:15<00:00,  4.78it/s]
[23.10.33] [INFO] Validation stats: Policy Loss: 1.4115, Value Loss: 0.0668, Total Loss: 1.4784, Value Mean: -0.0759, Value Std: 0.7460
[23.10.35] [INFO] Trainer finished at iteration 46.
[23.10.35] [INFO] All processes started at iteration 47.
[23.10.35] [INFO] Model and optimizer loaded from iteration 47
[23.11.00] [INFO] Evaluation results at iteration 46:
[23.11.00] [INFO]     Policy accuracy @1: 24.11%
[23.11.00] [INFO]     Policy accuracy @5: 55.68%
[23.11.00] [INFO]     Policy accuracy @10: 71.10%
[23.11.00] [INFO]     Avg value loss: 1.0875905911127726
[23.13.38] [INFO] Results after playing two most recent models at iteration 46: Wins: 15, Losses: 11, Draws: 14
[23.15.10] [INFO] Results after playing the current vs the reference at iteration 46: Wins: 1, Losses: 37, Draws: 2
[23.16.08] [INFO] Results after playing vs random at iteration 46: Wins: 36, Losses: 0, Draws: 4
[23.20.20] [INFO] Loading memories for iteration 47 with window size 3 (44-47)
[23.20.33] [INFO] Loaded 3112815 samples from 18720 games
Training batches: 100%|█████████▉| 1517/1519 [01:33<00:00, 16.22it/s]
[23.22.07] [INFO] Last gradient norm: 0.71875
[23.22.07] [INFO] Training stats: Policy Loss: 1.4396, Value Loss: 0.0648, Total Loss: 1.5045, Value Mean: -0.0000, Value Std: 0.7603
Validation batches: 100%|██████████| 74/74 [00:17<00:00,  4.32it/s]
[23.22.24] [INFO] Validation stats: Policy Loss: 1.4200, Value Loss: 0.0566, Total Loss: 1.4766, Value Mean: 0.0114, Value Std: 0.7615
[23.22.26] [INFO] Trainer finished at iteration 47.
[23.22.26] [INFO] All processes started at iteration 48.
[23.22.26] [INFO] Model and optimizer loaded from iteration 48
[23.22.51] [INFO] Evaluation results at iteration 47:
[23.22.51] [INFO]     Policy accuracy @1: 23.72%
[23.22.51] [INFO]     Policy accuracy @5: 55.37%
[23.22.51] [INFO]     Policy accuracy @10: 71.66%
[23.22.51] [INFO]     Avg value loss: 1.1357050398985544
[23.25.25] [INFO] Results after playing two most recent models at iteration 47: Wins: 12, Losses: 14, Draws: 14
[23.26.43] [INFO] Results after playing the current vs the reference at iteration 47: Wins: 1, Losses: 37, Draws: 2
[23.27.27] [INFO] Results after playing vs random at iteration 47: Wins: 31, Losses: 0, Draws: 9
[23.32.10] [INFO] Loading memories for iteration 48 with window size 3 (45-48)
[23.32.24] [INFO] Loaded 3099485 samples from 18645 games
Training batches: 100%|█████████▉| 1510/1513 [01:41<00:00, 14.88it/s]
[23.34.05] [INFO] Last gradient norm: 0.6953125
[23.34.05] [INFO] Training stats: Policy Loss: 1.4369, Value Loss: 0.0626, Total Loss: 1.4994, Value Mean: 0.0009, Value Std: 0.7649
Validation batches: 100%|██████████| 75/75 [00:18<00:00,  4.04it/s]
[23.34.24] [INFO] Validation stats: Policy Loss: 1.4087, Value Loss: 0.0505, Total Loss: 1.4596, Value Mean: -0.0055, Value Std: 0.7755
[23.34.26] [INFO] Trainer finished at iteration 48.
[23.34.26] [INFO] All processes started at iteration 49.
[23.34.26] [INFO] Model and optimizer loaded from iteration 49
[23.34.49] [INFO] Evaluation results at iteration 48:
[23.34.49] [INFO]     Policy accuracy @1: 23.82%
[23.34.49] [INFO]     Policy accuracy @5: 55.82%
[23.34.49] [INFO]     Policy accuracy @10: 72.53%
[23.34.49] [INFO]     Avg value loss: 1.134480647246043
[23.37.32] [INFO] Results after playing two most recent models at iteration 48: Wins: 14, Losses: 12, Draws: 14
[23.39.19] [INFO] Results after playing the current vs the reference at iteration 48: Wins: 1, Losses: 35, Draws: 4
[23.40.14] [INFO] Results after playing vs random at iteration 48: Wins: 37, Losses: 0, Draws: 3
[23.44.10] [INFO] Loading memories for iteration 49 with window size 3 (46-49)
[23.44.23] [INFO] Loaded 3077000 samples from 18510 games
Training batches: 100%|█████████▉| 1500/1502 [01:34<00:00, 15.84it/s]
[23.45.58] [INFO] Last gradient norm: 0.65625
[23.45.58] [INFO] Training stats: Policy Loss: 1.4399, Value Loss: 0.0650, Total Loss: 1.5050, Value Mean: 0.0008, Value Std: 0.7663
Validation batches: 100%|██████████| 73/73 [00:18<00:00,  3.89it/s]
[23.46.17] [INFO] Validation stats: Policy Loss: 1.4101, Value Loss: 0.0877, Total Loss: 1.4983, Value Mean: -0.0805, Value Std: 0.7458
[23.46.19] [INFO] Trainer finished at iteration 49.
[23.46.19] [INFO] All processes started at iteration 50.
[23.46.19] [INFO] Model and optimizer loaded from iteration 50
[23.46.42] [INFO] Evaluation results at iteration 49:
[23.46.42] [INFO]     Policy accuracy @1: 24.46%
[23.46.42] [INFO]     Policy accuracy @5: 55.29%
[23.46.42] [INFO]     Policy accuracy @10: 71.66%
[23.46.42] [INFO]     Avg value loss: 1.273992113272349
[23.49.32] [INFO] Results after playing two most recent models at iteration 49: Wins: 11, Losses: 10, Draws: 19
[23.51.02] [INFO] Results after playing the current vs the reference at iteration 49: Wins: 0, Losses: 37, Draws: 3
[23.51.48] [INFO] Results after playing vs random at iteration 49: Wins: 33, Losses: 0, Draws: 7
[23.56.13] [INFO] Loading memories for iteration 50 with window size 3 (47-50)
[23.56.27] [INFO] Loaded 3045635 samples from 18315 games
Training batches: 100%|█████████▉| 1483/1487 [01:35<00:00, 15.59it/s]
[23.58.02] [INFO] Last gradient norm: 0.69921875
[23.58.02] [INFO] Training stats: Policy Loss: 1.4350, Value Loss: 0.0627, Total Loss: 1.4977, Value Mean: 0.0005, Value Std: 0.7683
Validation batches: 100%|██████████| 73/73 [00:18<00:00,  3.86it/s]
[23.58.21] [INFO] Validation stats: Policy Loss: 1.4257, Value Loss: 0.0544, Total Loss: 1.4807, Value Mean: 0.0324, Value Std: 0.7678
[23.58.23] [INFO] Trainer finished at iteration 50.
[23.58.23] [INFO] All processes started at iteration 51.
[23.58.23] [INFO] Model and optimizer loaded from iteration 51
[23.58.48] [INFO] Evaluation results at iteration 50:
[23.58.48] [INFO]     Policy accuracy @1: 23.53%
[23.58.48] [INFO]     Policy accuracy @5: 56.00%
[23.58.48] [INFO]     Policy accuracy @10: 72.18%
[23.58.48] [INFO]     Avg value loss: 1.1827282706896465
[00.01.31] [INFO] Results after playing two most recent models at iteration 50: Wins: 15, Losses: 9, Draws: 16
[00.03.04] [INFO] Results after playing the current vs the reference at iteration 50: Wins: 2, Losses: 37, Draws: 1
[00.03.47] [INFO] Results after playing vs random at iteration 50: Wins: 34, Losses: 0, Draws: 6
[00.08.28] [INFO] Loading memories for iteration 51 with window size 3 (48-51)
[00.08.44] [INFO] Loaded 3042685 samples from 18300 games
Training batches: 100%|█████████▉| 1483/1485 [01:38<00:00, 15.03it/s]
[00.10.23] [INFO] Last gradient norm: 0.6953125
[00.10.23] [INFO] Training stats: Policy Loss: 1.4316, Value Loss: 0.0639, Total Loss: 1.4954, Value Mean: -0.0003, Value Std: 0.7667
Validation batches: 100%|██████████| 74/74 [00:18<00:00,  4.03it/s]
[00.10.41] [INFO] Validation stats: Policy Loss: 1.3869, Value Loss: 0.0466, Total Loss: 1.4339, Value Mean: -0.0066, Value Std: 0.7724
[00.10.43] [INFO] Trainer finished at iteration 51.
[00.10.43] [INFO] All processes started at iteration 52.
[00.10.43] [INFO] Model and optimizer loaded from iteration 52
[00.11.08] [INFO] Evaluation results at iteration 51:
[00.11.08] [INFO]     Policy accuracy @1: 23.56%
[00.11.08] [INFO]     Policy accuracy @5: 55.58%
[00.11.08] [INFO]     Policy accuracy @10: 73.11%
[00.11.08] [INFO]     Avg value loss: 1.0676686445871988
[00.13.56] [INFO] Results after playing two most recent models at iteration 51: Wins: 12, Losses: 5, Draws: 23
[00.15.14] [INFO] Results after playing the current vs the reference at iteration 51: Wins: 1, Losses: 37, Draws: 2
[00.16.03] [INFO] Results after playing vs random at iteration 51: Wins: 30, Losses: 0, Draws: 10
[00.20.18] [INFO] Loading memories for iteration 52 with window size 3 (49-52)
[00.20.31] [INFO] Loaded 3024560 samples from 18195 games
Training batches: 100%|█████████▉| 1474/1476 [01:36<00:00, 15.27it/s]
[00.22.08] [INFO] Last gradient norm: 0.73046875
[00.22.08] [INFO] Training stats: Policy Loss: 1.4239, Value Loss: 0.0643, Total Loss: 1.4881, Value Mean: 0.0005, Value Std: 0.7672
Validation batches: 100%|██████████| 74/74 [00:17<00:00,  4.19it/s]
[00.22.26] [INFO] Validation stats: Policy Loss: 1.3694, Value Loss: 0.0460, Total Loss: 1.4154, Value Mean: -0.0111, Value Std: 0.7771
[00.22.27] [INFO] Trainer finished at iteration 52.
[00.22.27] [INFO] All processes started at iteration 53.
[00.22.28] [INFO] Model and optimizer loaded from iteration 53
[00.22.50] [INFO] Evaluation results at iteration 52:
[00.22.50] [INFO]     Policy accuracy @1: 24.70%
[00.22.50] [INFO]     Policy accuracy @5: 56.16%
[00.22.50] [INFO]     Policy accuracy @10: 73.29%
[00.22.50] [INFO]     Avg value loss: 1.117036404212316
[00.25.37] [INFO] Results after playing two most recent models at iteration 52: Wins: 12, Losses: 12, Draws: 16
[00.27.17] [INFO] Results after playing the current vs the reference at iteration 52: Wins: 0, Losses: 35, Draws: 5
[00.28.09] [INFO] Results after playing vs random at iteration 52: Wins: 32, Losses: 0, Draws: 8
[00.32.14] [INFO] Loading memories for iteration 53 with window size 3 (50-53)
[00.32.27] [INFO] Loaded 3014430 samples from 18135 games
Training batches: 100%|█████████▉| 1469/1471 [01:37<00:00, 15.06it/s]
[00.34.05] [INFO] Last gradient norm: 0.63671875
[00.34.05] [INFO] Training stats: Policy Loss: 1.4161, Value Loss: 0.0646, Total Loss: 1.4808, Value Mean: 0.0004, Value Std: 0.7663
Validation batches: 100%|██████████| 72/72 [00:19<00:00,  3.71it/s]
[00.34.24] [INFO] Validation stats: Policy Loss: 1.3804, Value Loss: 0.0574, Total Loss: 1.4376, Value Mean: 0.0170, Value Std: 0.7633
[00.34.27] [INFO] Trainer finished at iteration 53.
[00.34.27] [INFO] All processes started at iteration 54.
[00.34.27] [INFO] Model and optimizer loaded from iteration 54
[00.34.50] [INFO] Evaluation results at iteration 53:
[00.34.50] [INFO]     Policy accuracy @1: 24.64%
[00.34.50] [INFO]     Policy accuracy @5: 56.21%
[00.34.50] [INFO]     Policy accuracy @10: 72.53%
[00.34.50] [INFO]     Avg value loss: 1.0921858390172323
[00.37.43] [INFO] Results after playing two most recent models at iteration 53: Wins: 10, Losses: 13, Draws: 17
[00.39.10] [INFO] Results after playing the current vs the reference at iteration 53: Wins: 1, Losses: 37, Draws: 2
[00.40.00] [INFO] Results after playing vs random at iteration 53: Wins: 32, Losses: 0, Draws: 8
[00.44.23] [INFO] Loading memories for iteration 54 with window size 3 (51-54)
[00.44.36] [INFO] Loaded 3044385 samples from 18315 games
Training batches: 100%|█████████▉| 1485/1486 [01:35<00:00, 15.49it/s]
[00.46.12] [INFO] Last gradient norm: 0.7421875
[00.46.12] [INFO] Training stats: Policy Loss: 1.4038, Value Loss: 0.0649, Total Loss: 1.4687, Value Mean: 0.0007, Value Std: 0.7673
Validation batches: 100%|██████████| 76/76 [00:19<00:00,  3.96it/s]
[00.46.31] [INFO] Validation stats: Policy Loss: 1.3461, Value Loss: 0.0471, Total Loss: 1.3932, Value Mean: -0.0449, Value Std: 0.7563
[00.46.33] [INFO] Trainer finished at iteration 54.
[00.46.33] [INFO] All processes started at iteration 55.
[00.46.33] [INFO] Model and optimizer loaded from iteration 55
[00.46.57] [INFO] Evaluation results at iteration 54:
[00.46.57] [INFO]     Policy accuracy @1: 26.10%
[00.46.57] [INFO]     Policy accuracy @5: 58.46%
[00.46.57] [INFO]     Policy accuracy @10: 74.03%
[00.46.57] [INFO]     Avg value loss: 1.047385859489441
[00.50.08] [INFO] Results after playing two most recent models at iteration 54: Wins: 12, Losses: 12, Draws: 16
[00.51.53] [INFO] Results after playing the current vs the reference at iteration 54: Wins: 2, Losses: 34, Draws: 4
[00.52.32] [INFO] Results after playing vs random at iteration 54: Wins: 32, Losses: 0, Draws: 8
[00.56.07] [INFO] Loading memories for iteration 55 with window size 3 (52-55)
[00.56.20] [INFO] Loaded 3029670 samples from 18225 games
Training batches: 100%|█████████▉| 1477/1479 [01:39<00:00, 14.90it/s]
[00.57.59] [INFO] Last gradient norm: 0.66796875
[00.57.59] [INFO] Training stats: Policy Loss: 1.3960, Value Loss: 0.0648, Total Loss: 1.4609, Value Mean: 0.0009, Value Std: 0.7690
Validation batches: 100%|██████████| 72/72 [00:19<00:00,  3.79it/s]
[00.58.18] [INFO] Validation stats: Policy Loss: 1.3596, Value Loss: 0.0620, Total Loss: 1.4220, Value Mean: -0.0528, Value Std: 0.7669
[00.58.21] [INFO] Trainer finished at iteration 55.
[00.58.21] [INFO] All processes started at iteration 56.
[00.58.21] [INFO] Model and optimizer loaded from iteration 56
[00.58.44] [INFO] Evaluation results at iteration 55:
[00.58.44] [INFO]     Policy accuracy @1: 25.36%
[00.58.44] [INFO]     Policy accuracy @5: 57.30%
[00.58.44] [INFO]     Policy accuracy @10: 73.72%
[00.58.44] [INFO]     Avg value loss: 1.1383992652098338
[01.01.41] [INFO] Results after playing two most recent models at iteration 55: Wins: 13, Losses: 12, Draws: 15
[01.03.02] [INFO] Results after playing the current vs the reference at iteration 55: Wins: 2, Losses: 37, Draws: 1
[01.03.45] [INFO] Results after playing vs random at iteration 55: Wins: 33, Losses: 0, Draws: 7
[01.08.06] [INFO] Loading memories for iteration 56 with window size 3 (53-56)
[01.08.20] [INFO] Loaded 3041135 samples from 18285 games
Training batches: 100%|█████████▉| 1483/1484 [01:39<00:00, 14.97it/s]
[01.10.00] [INFO] Last gradient norm: 0.65625
[01.10.00] [INFO] Training stats: Policy Loss: 1.3937, Value Loss: 0.0642, Total Loss: 1.4580, Value Mean: 0.0010, Value Std: 0.7669
Validation batches: 100%|██████████| 75/75 [00:18<00:00,  4.01it/s]
[01.10.19] [INFO] Validation stats: Policy Loss: 1.3410, Value Loss: 0.0557, Total Loss: 1.3961, Value Mean: -0.0432, Value Std: 0.7606
[01.10.21] [INFO] Trainer finished at iteration 56.
[01.10.21] [INFO] All processes started at iteration 57.
[01.10.21] [INFO] Model and optimizer loaded from iteration 57
[01.10.44] [INFO] Evaluation results at iteration 56:
[01.10.44] [INFO]     Policy accuracy @1: 24.64%
[01.10.44] [INFO]     Policy accuracy @5: 57.30%
[01.10.44] [INFO]     Policy accuracy @10: 73.35%
[01.10.44] [INFO]     Avg value loss: 1.0844626267751059
[01.13.27] [INFO] Results after playing two most recent models at iteration 56: Wins: 12, Losses: 4, Draws: 24
[01.15.18] [INFO] Results after playing the current vs the reference at iteration 56: Wins: 1, Losses: 37, Draws: 2
[01.16.03] [INFO] Results after playing vs random at iteration 56: Wins: 31, Losses: 0, Draws: 9
[01.20.15] [INFO] Loading memories for iteration 57 with window size 3 (54-57)
[01.20.28] [INFO] Loaded 3060650 samples from 18405 games
Training batches: 100%|█████████▉| 1491/1494 [01:42<00:00, 14.48it/s]
[01.22.11] [INFO] Last gradient norm: 0.66015625
[01.22.11] [INFO] Training stats: Policy Loss: 1.3859, Value Loss: 0.0655, Total Loss: 1.4513, Value Mean: 0.0012, Value Std: 0.7683
Validation batches: 100%|██████████| 74/74 [00:19<00:00,  3.70it/s]
[01.22.31] [INFO] Validation stats: Policy Loss: 1.3413, Value Loss: 0.0522, Total Loss: 1.3932, Value Mean: -0.0189, Value Std: 0.7764
[01.22.33] [INFO] Trainer finished at iteration 57.
[01.22.33] [INFO] All processes started at iteration 58.
[01.22.33] [INFO] Model and optimizer loaded from iteration 58
[01.22.58] [INFO] Evaluation results at iteration 57:
[01.22.58] [INFO]     Policy accuracy @1: 25.59%
[01.22.58] [INFO]     Policy accuracy @5: 58.73%
[01.22.58] [INFO]     Policy accuracy @10: 74.72%
[01.22.58] [INFO]     Avg value loss: 1.0919898609320322
[01.25.36] [INFO] Results after playing two most recent models at iteration 57: Wins: 13, Losses: 10, Draws: 17
[01.26.53] [INFO] Results after playing the current vs the reference at iteration 57: Wins: 4, Losses: 35, Draws: 1
[01.27.36] [INFO] Results after playing vs random at iteration 57: Wins: 32, Losses: 0, Draws: 8
[01.31.59] [INFO] Loading memories for iteration 58 with window size 3 (55-58)
[01.32.12] [INFO] Loaded 3002230 samples from 18060 games
Training batches: 100%|█████████▉| 1463/1465 [01:40<00:00, 14.53it/s]
[01.33.53] [INFO] Last gradient norm: 0.68359375
[01.33.53] [INFO] Training stats: Policy Loss: 1.3843, Value Loss: 0.0656, Total Loss: 1.4499, Value Mean: 0.0008, Value Std: 0.7695
Validation batches: 100%|██████████| 70/70 [00:19<00:00,  3.68it/s]
[01.34.12] [INFO] Validation stats: Policy Loss: 1.3568, Value Loss: 0.0638, Total Loss: 1.4204, Value Mean: -0.0058, Value Std: 0.7454
[01.34.14] [INFO] Trainer finished at iteration 58.
[01.34.14] [INFO] All processes started at iteration 59.
[01.34.14] [INFO] Model and optimizer loaded from iteration 59
[01.34.37] [INFO] Evaluation results at iteration 58:
[01.34.37] [INFO]     Policy accuracy @1: 24.88%
[01.34.37] [INFO]     Policy accuracy @5: 57.67%
[01.34.37] [INFO]     Policy accuracy @10: 74.06%
[01.34.37] [INFO]     Avg value loss: 1.0497757236162821
[01.37.13] [INFO] Results after playing two most recent models at iteration 58: Wins: 13, Losses: 9, Draws: 18
[01.38.41] [INFO] Results after playing the current vs the reference at iteration 58: Wins: 0, Losses: 39, Draws: 1
[01.39.38] [INFO] Results after playing vs random at iteration 58: Wins: 29, Losses: 0, Draws: 11
[01.43.49] [INFO] Loading memories for iteration 59 with window size 3 (56-59)
[01.44.03] [INFO] Loaded 3014420 samples from 18135 games
Training batches: 100%|█████████▉| 1470/1471 [01:38<00:00, 14.92it/s]
[01.45.42] [INFO] Last gradient norm: 0.75
[01.45.42] [INFO] Training stats: Policy Loss: 1.3740, Value Loss: 0.0644, Total Loss: 1.4384, Value Mean: 0.0004, Value Std: 0.7710
Validation batches: 100%|██████████| 74/74 [00:19<00:00,  3.80it/s]
[01.46.02] [INFO] Validation stats: Policy Loss: 1.3347, Value Loss: 0.0479, Total Loss: 1.3827, Value Mean: -0.0008, Value Std: 0.7664
[01.46.03] [INFO] Trainer finished at iteration 59.
[01.46.03] [INFO] All processes started at iteration 60.
[01.46.04] [INFO] Model and optimizer loaded from iteration 60
[01.46.28] [INFO] Evaluation results at iteration 59:
[01.46.28] [INFO]     Policy accuracy @1: 25.07%
[01.46.28] [INFO]     Policy accuracy @5: 58.65%
[01.46.28] [INFO]     Policy accuracy @10: 74.14%
[01.46.28] [INFO]     Avg value loss: 0.9939340015252431
[01.49.37] [INFO] Results after playing two most recent models at iteration 59: Wins: 8, Losses: 11, Draws: 21
[01.51.18] [INFO] Results after playing the current vs the reference at iteration 59: Wins: 0, Losses: 36, Draws: 4
[01.52.05] [INFO] Results after playing vs random at iteration 59: Wins: 29, Losses: 0, Draws: 11
[01.55.28] [INFO] Loading memories for iteration 60 with window size 3 (57-60)
[01.55.42] [INFO] Loaded 3002285 samples from 18060 games
Training batches: 100%|█████████▉| 1464/1465 [01:36<00:00, 15.13it/s]
[01.57.19] [INFO] Last gradient norm: 0.64453125
[01.57.19] [INFO] Training stats: Policy Loss: 1.3716, Value Loss: 0.0652, Total Loss: 1.4367, Value Mean: 0.0001, Value Std: 0.7739
Validation batches: 100%|██████████| 74/74 [00:19<00:00,  3.84it/s]
[01.57.38] [INFO] Validation stats: Policy Loss: 1.3252, Value Loss: 0.0469, Total Loss: 1.3721, Value Mean: -0.0110, Value Std: 0.7628
[01.57.40] [INFO] Trainer finished at iteration 60.
[01.57.40] [INFO] All processes started at iteration 61.
[01.57.41] [INFO] Model and optimizer loaded from iteration 61
[01.58.03] [INFO] Evaluation results at iteration 60:
[01.58.03] [INFO]     Policy accuracy @1: 24.99%
[01.58.03] [INFO]     Policy accuracy @5: 58.25%
[01.58.03] [INFO]     Policy accuracy @10: 73.61%
[01.58.03] [INFO]     Avg value loss: 1.08714164296786
[02.00.34] [INFO] Results after playing two most recent models at iteration 60: Wins: 15, Losses: 12, Draws: 13
[02.02.06] [INFO] Results after playing the current vs the reference at iteration 60: Wins: 3, Losses: 35, Draws: 2
[02.02.46] [INFO] Results after playing vs random at iteration 60: Wins: 32, Losses: 0, Draws: 8
[02.07.47] [INFO] Loading memories for iteration 61 with window size 3 (58-61)
[02.08.00] [INFO] Loaded 3002155 samples from 18060 games
Training batches: 100%|█████████▉| 1464/1465 [01:36<00:00, 15.12it/s]
[02.09.37] [INFO] Last gradient norm: 0.69921875
[02.09.37] [INFO] Training stats: Policy Loss: 1.3628, Value Loss: 0.0635, Total Loss: 1.4263, Value Mean: 0.0002, Value Std: 0.7751
Validation batches: 100%|██████████| 74/74 [00:19<00:00,  3.86it/s]
[02.09.56] [INFO] Validation stats: Policy Loss: 1.3292, Value Loss: 0.0654, Total Loss: 1.3950, Value Mean: 0.0409, Value Std: 0.7715
[02.09.58] [INFO] Trainer finished at iteration 61.
[02.09.58] [INFO] All processes started at iteration 62.
[02.09.58] [INFO] Model and optimizer loaded from iteration 62
[02.10.21] [INFO] Evaluation results at iteration 61:
[02.10.21] [INFO]     Policy accuracy @1: 25.20%
[02.10.21] [INFO]     Policy accuracy @5: 58.25%
[02.10.21] [INFO]     Policy accuracy @10: 73.96%
[02.10.21] [INFO]     Avg value loss: 1.0595692574977875
[02.12.55] [INFO] Results after playing two most recent models at iteration 61: Wins: 11, Losses: 9, Draws: 20
[02.14.29] [INFO] Results after playing the current vs the reference at iteration 61: Wins: 1, Losses: 34, Draws: 5
[02.15.17] [INFO] Results after playing vs random at iteration 61: Wins: 30, Losses: 0, Draws: 10
[02.19.13] [INFO] Loading memories for iteration 62 with window size 3 (59-62)
[02.19.27] [INFO] Loaded 3001735 samples from 18060 games
Training batches: 100%|█████████▉| 1463/1465 [01:37<00:00, 14.97it/s]
[02.21.05] [INFO] Last gradient norm: 0.6953125
[02.21.05] [INFO] Training stats: Policy Loss: 1.3548, Value Loss: 0.0662, Total Loss: 1.4209, Value Mean: -0.0002, Value Std: 0.7704
Validation batches: 100%|██████████| 70/70 [00:18<00:00,  3.72it/s]
[02.21.24] [INFO] Validation stats: Policy Loss: 1.3233, Value Loss: 0.0682, Total Loss: 1.3915, Value Mean: 0.0071, Value Std: 0.7677
[02.21.26] [INFO] Trainer finished at iteration 62.
[02.21.26] [INFO] All processes started at iteration 63.
[02.21.26] [INFO] Model and optimizer loaded from iteration 63
[02.21.50] [INFO] Evaluation results at iteration 62:
[02.21.50] [INFO]     Policy accuracy @1: 25.38%
[02.21.50] [INFO]     Policy accuracy @5: 58.38%
[02.21.50] [INFO]     Policy accuracy @10: 73.48%
[02.21.50] [INFO]     Avg value loss: 0.9922176281611125
[02.24.44] [INFO] Results after playing two most recent models at iteration 62: Wins: 8, Losses: 15, Draws: 17
[02.26.21] [INFO] Results after playing the current vs the reference at iteration 62: Wins: 1, Losses: 36, Draws: 3
[02.27.11] [INFO] Results after playing vs random at iteration 62: Wins: 32, Losses: 0, Draws: 8
[02.30.30] [INFO] Loading memories for iteration 63 with window size 3 (60-63)
[02.30.43] [INFO] Loaded 2956845 samples from 17790 games
Training batches: 100%|█████████▉| 1441/1443 [01:39<00:00, 14.55it/s]
[02.32.23] [INFO] Last gradient norm: 0.7265625
[02.32.23] [INFO] Training stats: Policy Loss: 1.3485, Value Loss: 0.0666, Total Loss: 1.4150, Value Mean: 0.0002, Value Std: 0.7701
Validation batches: 100%|██████████| 69/69 [00:18<00:00,  3.67it/s]
[02.32.42] [INFO] Validation stats: Policy Loss: 1.3120, Value Loss: 0.0809, Total Loss: 1.3928, Value Mean: 0.0449, Value Std: 0.7701
[02.32.43] [INFO] Trainer finished at iteration 63.
[02.32.43] [INFO] All processes started at iteration 64.
[02.32.44] [INFO] Model and optimizer loaded from iteration 64
[02.33.07] [INFO] Evaluation results at iteration 63:
[02.33.07] [INFO]     Policy accuracy @1: 26.52%
[02.33.07] [INFO]     Policy accuracy @5: 59.57%
[02.33.07] [INFO]     Policy accuracy @10: 75.67%
[02.33.07] [INFO]     Avg value loss: 1.0321066816647848
[02.35.42] [INFO] Results after playing two most recent models at iteration 63: Wins: 15, Losses: 7, Draws: 18
[02.37.26] [INFO] Results after playing the current vs the reference at iteration 63: Wins: 3, Losses: 33, Draws: 4
[02.38.11] [INFO] Results after playing vs random at iteration 63: Wins: 32, Losses: 0, Draws: 8
[02.42.29] [INFO] Loading memories for iteration 64 with window size 3 (61-64)
[02.42.41] [INFO] Loaded 2927380 samples from 17625 games
Training batches: 100%|█████████▉| 1427/1429 [01:36<00:00, 14.72it/s]
[02.44.18] [INFO] Last gradient norm: 0.68359375
[02.44.18] [INFO] Training stats: Policy Loss: 1.3426, Value Loss: 0.0676, Total Loss: 1.4102, Value Mean: 0.0004, Value Std: 0.7669
Validation batches: 100%|██████████| 71/71 [00:18<00:00,  3.81it/s]
[02.44.37] [INFO] Validation stats: Policy Loss: 1.2854, Value Loss: 0.0501, Total Loss: 1.3356, Value Mean: -0.0010, Value Std: 0.7742
[02.44.39] [INFO] Trainer finished at iteration 64.
[02.44.39] [INFO] All processes started at iteration 65.
[02.44.39] [INFO] Model and optimizer loaded from iteration 65
[02.45.02] [INFO] Evaluation results at iteration 64:
[02.45.02] [INFO]     Policy accuracy @1: 25.75%
[02.45.02] [INFO]     Policy accuracy @5: 58.78%
[02.45.02] [INFO]     Policy accuracy @10: 74.83%
[02.45.02] [INFO]     Avg value loss: 1.0512037694454193
[02.47.59] [INFO] Results after playing two most recent models at iteration 64: Wins: 8, Losses: 10, Draws: 22
[02.49.35] [INFO] Results after playing the current vs the reference at iteration 64: Wins: 1, Losses: 34, Draws: 5
[02.50.14] [INFO] Results after playing vs random at iteration 64: Wins: 30, Losses: 0, Draws: 10
[02.54.16] [INFO] Loading memories for iteration 65 with window size 3 (62-65)
[02.54.30] [INFO] Loaded 2948415 samples from 17745 games
Training batches: 100%|█████████▉| 1437/1439 [01:41<00:00, 14.22it/s]
[02.56.11] [INFO] Last gradient norm: 0.7421875
[02.56.11] [INFO] Training stats: Policy Loss: 1.3297, Value Loss: 0.0670, Total Loss: 1.3968, Value Mean: 0.0004, Value Std: 0.7658
Validation batches: 100%|██████████| 76/76 [00:19<00:00,  3.91it/s]
[02.56.31] [INFO] Validation stats: Policy Loss: 1.2588, Value Loss: 0.0465, Total Loss: 1.3057, Value Mean: 0.0253, Value Std: 0.7695
[02.56.33] [INFO] Trainer finished at iteration 65.
[02.56.33] [INFO] All processes started at iteration 66.
[02.56.33] [INFO] Model and optimizer loaded from iteration 66
[02.56.57] [INFO] Evaluation results at iteration 65:
[02.56.57] [INFO]     Policy accuracy @1: 26.20%
[02.56.57] [INFO]     Policy accuracy @5: 59.76%
[02.56.57] [INFO]     Policy accuracy @10: 75.01%
[02.56.57] [INFO]     Avg value loss: 1.1040740887324014
[02.59.36] [INFO] Results after playing two most recent models at iteration 65: Wins: 14, Losses: 10, Draws: 16
[03.01.29] [INFO] Results after playing the current vs the reference at iteration 65: Wins: 1, Losses: 34, Draws: 5
[03.02.03] [INFO] Results after playing vs random at iteration 65: Wins: 37, Losses: 0, Draws: 3
[03.05.36] [INFO] Loading memories for iteration 66 with window size 3 (63-66)
[03.05.49] [INFO] Loaded 2929730 samples from 17625 games
Training batches: 100%|█████████▉| 1428/1430 [01:37<00:00, 14.67it/s]
[03.07.27] [INFO] Last gradient norm: 0.69140625
[03.07.27] [INFO] Training stats: Policy Loss: 1.3168, Value Loss: 0.0670, Total Loss: 1.3838, Value Mean: 0.0003, Value Std: 0.7678
Validation batches: 100%|██████████| 68/68 [00:18<00:00,  3.64it/s]
[03.07.46] [INFO] Validation stats: Policy Loss: 1.2632, Value Loss: 0.0631, Total Loss: 1.3265, Value Mean: -0.0419, Value Std: 0.7630
[03.07.47] [INFO] Trainer finished at iteration 66.
[03.07.48] [INFO] All processes started at iteration 67.
[03.07.48] [INFO] Model and optimizer loaded from iteration 67
[03.08.11] [INFO] Evaluation results at iteration 66:
[03.08.11] [INFO]     Policy accuracy @1: 26.04%
[03.08.11] [INFO]     Policy accuracy @5: 59.60%
[03.08.11] [INFO]     Policy accuracy @10: 75.44%
[03.08.11] [INFO]     Avg value loss: 0.9994528591632843
[03.10.50] [INFO] Results after playing two most recent models at iteration 66: Wins: 10, Losses: 10, Draws: 20
[03.12.40] [INFO] Results after playing the current vs the reference at iteration 66: Wins: 0, Losses: 33, Draws: 7
[03.13.22] [INFO] Results after playing vs random at iteration 66: Wins: 30, Losses: 0, Draws: 10
[03.17.12] [INFO] Loading memories for iteration 67 with window size 3 (64-67)
[03.17.26] [INFO] Loaded 2967445 samples from 17865 games
Training batches: 100%|█████████▉| 1447/1448 [01:36<00:00, 15.00it/s]
[03.19.02] [INFO] Last gradient norm: 0.7109375
[03.19.02] [INFO] Training stats: Policy Loss: 1.3059, Value Loss: 0.0667, Total Loss: 1.3726, Value Mean: -0.0001, Value Std: 0.7667
Validation batches: 100%|██████████| 73/73 [00:18<00:00,  3.96it/s]
[03.19.21] [INFO] Validation stats: Policy Loss: 1.2373, Value Loss: 0.0467, Total Loss: 1.2839, Value Mean: -0.0235, Value Std: 0.7729
[03.19.23] [INFO] Trainer finished at iteration 67.
[03.19.23] [INFO] All processes started at iteration 68.
[03.19.23] [INFO] Model and optimizer loaded from iteration 68
[03.19.48] [INFO] Evaluation results at iteration 67:
[03.19.48] [INFO]     Policy accuracy @1: 26.81%
[03.19.48] [INFO]     Policy accuracy @5: 60.74%
[03.19.48] [INFO]     Policy accuracy @10: 75.99%
[03.19.48] [INFO]     Avg value loss: 1.1336491147677104
[03.22.30] [INFO] Results after playing two most recent models at iteration 67: Wins: 12, Losses: 12, Draws: 16
[03.24.07] [INFO] Results after playing the current vs the reference at iteration 67: Wins: 1, Losses: 36, Draws: 3
[03.24.44] [INFO] Results after playing vs random at iteration 67: Wins: 34, Losses: 0, Draws: 6
[03.28.37] [INFO] Loading memories for iteration 68 with window size 3 (65-68)
[03.28.50] [INFO] Loaded 2954250 samples from 17790 games
Training batches: 100%|█████████▉| 1440/1442 [01:31<00:00, 15.81it/s]
[03.30.21] [INFO] Last gradient norm: 0.73046875
[03.30.21] [INFO] Training stats: Policy Loss: 1.2968, Value Loss: 0.0648, Total Loss: 1.3615, Value Mean: -0.0001, Value Std: 0.7688
Validation batches: 100%|██████████| 70/70 [00:17<00:00,  3.91it/s]
[03.30.39] [INFO] Validation stats: Policy Loss: 1.2614, Value Loss: 0.0560, Total Loss: 1.3171, Value Mean: -0.0084, Value Std: 0.7493
[03.30.42] [INFO] Trainer finished at iteration 68.
[03.30.42] [INFO] All processes started at iteration 69.
[03.30.42] [INFO] Model and optimizer loaded from iteration 69
[03.31.06] [INFO] Evaluation results at iteration 68:
[03.31.06] [INFO]     Policy accuracy @1: 27.37%
[03.31.06] [INFO]     Policy accuracy @5: 60.68%
[03.31.06] [INFO]     Policy accuracy @10: 75.83%
[03.31.06] [INFO]     Avg value loss: 1.1155943274497986
[03.33.41] [INFO] Results after playing two most recent models at iteration 68: Wins: 11, Losses: 15, Draws: 14
[03.35.21] [INFO] Results after playing the current vs the reference at iteration 68: Wins: 2, Losses: 36, Draws: 2
[03.36.01] [INFO] Results after playing vs random at iteration 68: Wins: 31, Losses: 0, Draws: 9
[03.40.35] [INFO] Loading memories for iteration 69 with window size 3 (66-69)
[03.40.48] [INFO] Loaded 2955130 samples from 17805 games
Training batches: 100%|█████████▉| 1440/1442 [01:31<00:00, 15.68it/s]
[03.42.20] [INFO] Last gradient norm: 0.7265625
[03.42.20] [INFO] Training stats: Policy Loss: 1.2991, Value Loss: 0.0677, Total Loss: 1.3668, Value Mean: -0.0000, Value Std: 0.7669
Validation batches: 100%|██████████| 76/76 [00:18<00:00,  4.12it/s]
[03.42.38] [INFO] Validation stats: Policy Loss: 1.2544, Value Loss: 0.0412, Total Loss: 1.2952, Value Mean: -0.0207, Value Std: 0.7887
[03.42.40] [INFO] Trainer finished at iteration 69.
[03.42.40] [INFO] All processes started at iteration 70.
[03.42.41] [INFO] Model and optimizer loaded from iteration 70
[03.43.04] [INFO] Evaluation results at iteration 69:
[03.43.04] [INFO]     Policy accuracy @1: 26.18%
[03.43.04] [INFO]     Policy accuracy @5: 60.66%
[03.43.04] [INFO]     Policy accuracy @10: 76.31%
[03.43.04] [INFO]     Avg value loss: 1.031358551979065
[03.45.13] [INFO] Results after playing two most recent models at iteration 69: Wins: 18, Losses: 12, Draws: 10
[03.46.46] [INFO] Results after playing the current vs the reference at iteration 69: Wins: 2, Losses: 30, Draws: 8
[03.47.32] [INFO] Results after playing vs random at iteration 69: Wins: 31, Losses: 0, Draws: 9
[03.52.25] [INFO] Loading memories for iteration 70 with window size 3 (67-70)
[03.52.39] [INFO] Loaded 2970250 samples from 17895 games
Training batches: 100%|█████████▉| 1448/1450 [01:35<00:00, 15.13it/s]
[03.54.15] [INFO] Last gradient norm: 0.69140625
[03.54.15] [INFO] Training stats: Policy Loss: 1.2906, Value Loss: 0.0655, Total Loss: 1.3561, Value Mean: -0.0004, Value Std: 0.7668
Validation batches: 100%|██████████| 70/70 [00:19<00:00,  3.62it/s]
[03.54.34] [INFO] Validation stats: Policy Loss: 1.2473, Value Loss: 0.0654, Total Loss: 1.3128, Value Mean: 0.0552, Value Std: 0.7650
[03.54.36] [INFO] Trainer finished at iteration 70.
[03.54.36] [INFO] All processes started at iteration 71.
[03.54.37] [INFO] Model and optimizer loaded from iteration 71
[03.55.00] [INFO] Evaluation results at iteration 70:
[03.55.00] [INFO]     Policy accuracy @1: 26.71%
[03.55.00] [INFO]     Policy accuracy @5: 60.52%
[03.55.00] [INFO]     Policy accuracy @10: 75.89%
[03.55.00] [INFO]     Avg value loss: 1.1216172913710276
[03.57.40] [INFO] Results after playing two most recent models at iteration 70: Wins: 15, Losses: 11, Draws: 14
[03.59.10] [INFO] Results after playing the current vs the reference at iteration 70: Wins: 1, Losses: 33, Draws: 6
[03.59.46] [INFO] Results after playing vs random at iteration 70: Wins: 32, Losses: 0, Draws: 8
[04.04.00] [INFO] Loading memories for iteration 71 with window size 3 (68-71)
[04.04.12] [INFO] Loaded 2974290 samples from 17910 games
Training batches: 100%|█████████▉| 1450/1452 [01:35<00:00, 15.26it/s]
[04.05.48] [INFO] Last gradient norm: 0.64453125
[04.05.48] [INFO] Training stats: Policy Loss: 1.2880, Value Loss: 0.0678, Total Loss: 1.3558, Value Mean: 0.0001, Value Std: 0.7657
Validation batches: 100%|██████████| 73/73 [00:19<00:00,  3.77it/s]
[04.06.07] [INFO] Validation stats: Policy Loss: 1.2225, Value Loss: 0.0549, Total Loss: 1.2768, Value Mean: 0.0395, Value Std: 0.7672
[04.06.09] [INFO] Trainer finished at iteration 71.
[04.06.09] [INFO] All processes started at iteration 72.
[04.06.09] [INFO] Model and optimizer loaded from iteration 72
[04.06.33] [INFO] Evaluation results at iteration 71:
[04.06.33] [INFO]     Policy accuracy @1: 26.97%
[04.06.33] [INFO]     Policy accuracy @5: 61.00%
[04.06.33] [INFO]     Policy accuracy @10: 75.89%
[04.06.33] [INFO]     Avg value loss: 0.9903220196564992
[04.09.19] [INFO] Results after playing two most recent models at iteration 71: Wins: 10, Losses: 6, Draws: 24
[04.10.44] [INFO] Results after playing the current vs the reference at iteration 71: Wins: 1, Losses: 36, Draws: 3
[04.11.18] [INFO] Results after playing vs random at iteration 71: Wins: 37, Losses: 0, Draws: 3
[04.15.53] [INFO] Loading memories for iteration 72 with window size 3 (69-72)
[04.16.06] [INFO] Loaded 2962735 samples from 17835 games
Training batches: 100%|█████████▉| 1444/1446 [01:35<00:00, 15.19it/s]
[04.17.41] [INFO] Last gradient norm: 0.62109375
[04.17.41] [INFO] Training stats: Policy Loss: 1.2686, Value Loss: 0.0662, Total Loss: 1.3348, Value Mean: -0.0002, Value Std: 0.7665
Validation batches: 100%|██████████| 68/68 [00:17<00:00,  3.94it/s]
[04.17.59] [INFO] Validation stats: Policy Loss: 1.2174, Value Loss: 0.0967, Total Loss: 1.3141, Value Mean: -0.0539, Value Std: 0.7664
[04.18.01] [INFO] Trainer finished at iteration 72.
[04.18.01] [INFO] All processes started at iteration 73.
[04.18.01] [INFO] Model and optimizer loaded from iteration 73
[04.18.25] [INFO] Evaluation results at iteration 72:
[04.18.25] [INFO]     Policy accuracy @1: 27.47%
[04.18.25] [INFO]     Policy accuracy @5: 60.97%
[04.18.25] [INFO]     Policy accuracy @10: 76.02%
[04.18.25] [INFO]     Avg value loss: 1.0054831782976785
[04.20.55] [INFO] Results after playing two most recent models at iteration 72: Wins: 12, Losses: 7, Draws: 21
[04.22.30] [INFO] Results after playing the current vs the reference at iteration 72: Wins: 3, Losses: 35, Draws: 2
[04.23.06] [INFO] Results after playing vs random at iteration 72: Wins: 37, Losses: 0, Draws: 3