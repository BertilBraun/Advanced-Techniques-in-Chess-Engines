(Chess) [fu5896@hkn1993 py]$ cat train_zero_3101507.txt
Conda environment already exists.
/home/hk-project-starter-p0023563/fu5896/.bashrc: line 46: cd: Advanced-Techniques-in-Chess-Engines/cpp_py: No such file or directory
[11.23.58] [INFO] Starting training
[11.23.58] [INFO] Training on: GPU
[11.23.58] [INFO] Training args:
TrainingArgs(save_path='training_data/chess',
             num_iterations=120,
             num_games_per_iteration=1888,
             network=NetworkParams(num_layers=15, hidden_size=128),
             self_play=SelfPlayParams(mcts=MCTSParams(num_searches_per_turn=640, num_parallel_searches=8, dirichlet_epsilon=0.25, dirichlet_alpha=0.3, c_param=1.7, min_visit_count=0),
                                      num_parallel_games=16,
                                      num_moves_after_which_to_play_greedy=25,
                                      temperature=1.25,
                                      result_score_weight=0.15,
                                      num_games_after_which_to_write=5,
                                      resignation_threshold=-1.0),
             training=TrainingParams(num_epochs=1,
                                     batch_size=2048,
                                     sampling_window=<function sampling_window at 0x14ee85b107c0>,
                                     learning_rate=<function learning_rate at 0x14ee85bbe020>,
                                     learning_rate_scheduler=<function learning_rate_scheduler at 0x14ee858bc5e0>,
                                     num_workers=4),
             cluster=ClusterParams(num_self_play_nodes_on_cluster=118),
             evaluation=EvaluationParams(num_searches_per_turn=60, num_games=40, every_n_iterations=1, dataset_path='reference/memory_0_chess_database.hdf5'))
[11.23.58] [INFO] Run ID: 0
[11.23.58] [INFO] Setting up connections...
[11.23.59] [INFO] Connections set up.
[11.23.59] [INFO] Starting training at iteration 1.
[11.23.59] [INFO] All processes started at iteration 1.
[11.27.00] [INFO] No optimizer found for: training_data/chess/optimizer_1.pt
[11.27.00] [INFO] Model and optimizer loaded from iteration 1
[11.45.16] [INFO] Loading memories for iteration 1 with window size 2 (0-1)
[11.45.25] [INFO] Loaded 826530 samples from 8275 games
Training batches: 100%|██████████| 403/403 [01:33<00:00,  4.30it/s]
[11.46.59] [INFO] Last gradient norm: 0.466796875
[11.46.59] [INFO] Training stats: Policy Loss: 1.3031, Value Loss: 0.1578, Total Loss: 1.4609, Value Mean: 0.0069, Value Std: 0.7971
Validation batches: 100%|██████████| 80/80 [00:24<00:00,  3.24it/s]
[11.47.23] [INFO] Validation stats: Policy Loss: 1.2359, Value Loss: 0.1316, Total Loss: 1.3678, Value Mean: 0.0889, Value Std: 0.8469
[11.47.25] [INFO] Trainer finished at iteration 1.
[11.47.25] [INFO] All processes started at iteration 2.
[11.47.25] [INFO] Model and optimizer loaded from iteration 2
[11.47.57] [INFO] Evaluation results at iteration 1:
[11.47.57] [INFO]     Policy accuracy @1: 43.79%
[11.47.57] [INFO]     Policy accuracy @5: 84.11%
[11.47.57] [INFO]     Policy accuracy @10: 94.05%
[11.47.57] [INFO]     Avg value loss: 0.8128059844175974
[11.47.58] [INFO] No model found for: training_data/chess/model_0.pt
[11.48.54] [INFO] Results after playing two most recent models at iteration 1: Wins: 40, Losses: 0, Draws: 0
[11.50.22] [INFO] Results after playing the current vs the reference at iteration 1: Wins: 18, Losses: 18, Draws: 4
[11.50.44] [INFO] Results after playing vs random at iteration 1: Wins: 40, Losses: 0, Draws: 0
[11.52.22] [INFO] Loading memories for iteration 2 with window size 2 (0-2)
[11.52.29] [INFO] Loaded 1170175 samples from 10500 games
Training batches: 100%|█████████▉| 569/571 [01:12<00:00,  7.85it/s]
[11.53.42] [INFO] Last gradient norm: 0.431640625
[11.53.42] [INFO] Training stats: Policy Loss: 1.2668, Value Loss: 0.1092, Total Loss: 1.3759, Value Mean: 0.0055, Value Std: 0.8046
Validation batches: 100%|██████████| 33/33 [00:11<00:00,  2.95it/s]
[11.53.53] [INFO] Validation stats: Policy Loss: 1.2434, Value Loss: 0.1837, Total Loss: 1.4273, Value Mean: 0.0711, Value Std: 0.8022
[11.53.55] [INFO] Trainer finished at iteration 2.
[11.53.55] [INFO] All processes started at iteration 3.
[11.53.55] [INFO] Model and optimizer loaded from iteration 3
[11.54.19] [INFO] Evaluation results at iteration 2:
[11.54.19] [INFO]     Policy accuracy @1: 43.36%
[11.54.19] [INFO]     Policy accuracy @5: 83.71%
[11.54.19] [INFO]     Policy accuracy @10: 93.79%
[11.54.19] [INFO]     Avg value loss: 1.1992499987284342
[11.56.27] [INFO] Results after playing two most recent models at iteration 2: Wins: 16, Losses: 21, Draws: 3
[11.58.23] [INFO] Results after playing the current vs the reference at iteration 2: Wins: 16, Losses: 21, Draws: 3
[11.58.48] [INFO] Results after playing vs random at iteration 2: Wins: 39, Losses: 0, Draws: 1
[12.02.15] [INFO] Loading memories for iteration 3 with window size 2 (1-3)
[12.02.25] [INFO] Loaded 1830860 samples from 15820 games
Training batches: 100%|█████████▉| 891/893 [01:22<00:00, 10.78it/s]
[12.03.48] [INFO] Last gradient norm: 0.498046875
[12.03.48] [INFO] Training stats: Policy Loss: 1.2603, Value Loss: 0.1158, Total Loss: 1.3760, Value Mean: 0.0055, Value Std: 0.7955
Validation batches: 100%|██████████| 64/64 [00:14<00:00,  4.38it/s]
[12.04.02] [INFO] Validation stats: Policy Loss: 1.1327, Value Loss: 0.0859, Total Loss: 1.2185, Value Mean: 0.0367, Value Std: 0.8143
[12.04.04] [INFO] Trainer finished at iteration 3.
[12.04.04] [INFO] All processes started at iteration 4.
[12.04.04] [INFO] Model and optimizer loaded from iteration 4
[12.04.27] [INFO] Evaluation results at iteration 3:
[12.04.27] [INFO]     Policy accuracy @1: 41.88%
[12.04.27] [INFO]     Policy accuracy @5: 83.98%
[12.04.27] [INFO]     Policy accuracy @10: 94.13%
[12.04.27] [INFO]     Avg value loss: 1.0401897271474203
[12.06.16] [INFO] Results after playing two most recent models at iteration 3: Wins: 20, Losses: 17, Draws: 3
[12.08.08] [INFO] Results after playing the current vs the reference at iteration 3: Wins: 14, Losses: 20, Draws: 6
[12.08.32] [INFO] Results after playing vs random at iteration 3: Wins: 40, Losses: 0, Draws: 0
[12.10.44] [INFO] Loading memories for iteration 4 with window size 2 (2-4)
[12.10.52] [INFO] Loaded 1520355 samples from 12240 games
Training batches: 100%|█████████▉| 740/742 [01:10<00:00, 10.49it/s]
[12.12.03] [INFO] Last gradient norm: 0.58203125
[12.12.03] [INFO] Training stats: Policy Loss: 1.2231, Value Loss: 0.1110, Total Loss: 1.3341, Value Mean: 0.0076, Value Std: 0.7909
Validation batches: 100%|██████████| 50/50 [00:13<00:00,  3.77it/s]
[12.12.16] [INFO] Validation stats: Policy Loss: 1.1309, Value Loss: 0.0654, Total Loss: 1.1961, Value Mean: 0.0479, Value Std: 0.8193
[12.12.18] [INFO] Trainer finished at iteration 4.
[12.12.18] [INFO] All processes started at iteration 5.
[12.12.18] [INFO] Model and optimizer loaded from iteration 5
[12.12.42] [INFO] Evaluation results at iteration 4:
[12.12.42] [INFO]     Policy accuracy @1: 42.54%
[12.12.42] [INFO]     Policy accuracy @5: 83.10%
[12.12.42] [INFO]     Policy accuracy @10: 93.76%
[12.12.42] [INFO]     Avg value loss: 1.1118813097476958
[12.14.33] [INFO] Results after playing two most recent models at iteration 4: Wins: 21, Losses: 11, Draws: 8
[12.16.19] [INFO] Results after playing the current vs the reference at iteration 4: Wins: 16, Losses: 18, Draws: 6
[12.16.42] [INFO] Results after playing vs random at iteration 4: Wins: 40, Losses: 0, Draws: 0
[12.19.37] [INFO] Loading memories for iteration 5 with window size 3 (2-5)
[12.19.47] [INFO] Loaded 2127630 samples from 17120 games
Training batches: 100%|█████████▉| 1035/1038 [01:18<00:00, 13.18it/s]
[12.21.05] [INFO] Last gradient norm: 0.58984375
[12.21.05] [INFO] Training stats: Policy Loss: 1.2270, Value Loss: 0.1048, Total Loss: 1.3318, Value Mean: 0.0078, Value Std: 0.7975
Validation batches: 100%|██████████| 59/59 [00:15<00:00,  3.81it/s]
[12.21.21] [INFO] Validation stats: Policy Loss: 1.1348, Value Loss: 0.0801, Total Loss: 1.2146, Value Mean: -0.0027, Value Std: 0.8242
[12.21.23] [INFO] Trainer finished at iteration 5.
[12.21.23] [INFO] All processes started at iteration 6.
[12.21.23] [INFO] Model and optimizer loaded from iteration 6
[12.21.47] [INFO] Evaluation results at iteration 5:
[12.21.47] [INFO]     Policy accuracy @1: 41.96%
[12.21.47] [INFO]     Policy accuracy @5: 82.39%
[12.21.47] [INFO]     Policy accuracy @10: 92.86%
[12.21.47] [INFO]     Avg value loss: 1.1677225967248281
[12.23.41] [INFO] Results after playing two most recent models at iteration 5: Wins: 14, Losses: 20, Draws: 6
[12.25.25] [INFO] Results after playing the current vs the reference at iteration 5: Wins: 8, Losses: 26, Draws: 6
[12.25.48] [INFO] Results after playing vs random at iteration 5: Wins: 40, Losses: 0, Draws: 0
[12.28.12] [INFO] Loading memories for iteration 6 with window size 3 (3-6)
[12.28.21] [INFO] Loaded 2330840 samples from 18740 games
Training batches: 100%|█████████▉| 1136/1138 [01:21<00:00, 13.89it/s]
[12.29.43] [INFO] Last gradient norm: 0.58203125
[12.29.43] [INFO] Training stats: Policy Loss: 1.2217, Value Loss: 0.1040, Total Loss: 1.3256, Value Mean: 0.0074, Value Std: 0.8027
Validation batches: 100%|██████████| 53/53 [00:14<00:00,  3.68it/s]
[12.29.57] [INFO] Validation stats: Policy Loss: 1.1436, Value Loss: 0.0968, Total Loss: 1.2400, Value Mean: -0.0372, Value Std: 0.8175
[12.30.00] [INFO] Trainer finished at iteration 6.
[12.30.00] [INFO] All processes started at iteration 7.
[12.30.00] [INFO] Model and optimizer loaded from iteration 7
[12.30.22] [INFO] Evaluation results at iteration 6:
[12.30.22] [INFO]     Policy accuracy @1: 41.27%
[12.30.22] [INFO]     Policy accuracy @5: 82.13%
[12.30.22] [INFO]     Policy accuracy @10: 92.68%
[12.30.22] [INFO]     Avg value loss: 1.3022959192593893
[12.32.08] [INFO] Results after playing two most recent models at iteration 6: Wins: 22, Losses: 13, Draws: 5
[12.34.06] [INFO] Results after playing the current vs the reference at iteration 6: Wins: 16, Losses: 16, Draws: 8
[12.34.29] [INFO] Results after playing vs random at iteration 6: Wins: 40, Losses: 0, Draws: 0
[12.37.19] [INFO] Loading memories for iteration 7 with window size 3 (4-7)
[12.37.28] [INFO] Loaded 2249830 samples from 18080 games
Training batches: 100%|█████████▉| 1095/1098 [01:17<00:00, 14.11it/s]
[12.38.46] [INFO] Last gradient norm: 0.73828125
[12.38.46] [INFO] Training stats: Policy Loss: 1.2271, Value Loss: 0.1019, Total Loss: 1.3291, Value Mean: 0.0059, Value Std: 0.8014
Validation batches: 100%|██████████| 56/56 [00:15<00:00,  3.56it/s]
[12.39.02] [INFO] Validation stats: Policy Loss: 1.1823, Value Loss: 0.1662, Total Loss: 1.3489, Value Mean: 0.0270, Value Std: 0.8211
[12.39.04] [INFO] Trainer finished at iteration 7.
[12.39.04] [INFO] All processes started at iteration 8.
[12.39.04] [INFO] Model and optimizer loaded from iteration 8
[12.39.27] [INFO] Evaluation results at iteration 7:
[12.39.27] [INFO]     Policy accuracy @1: 41.51%
[12.39.27] [INFO]     Policy accuracy @5: 82.31%
[12.39.27] [INFO]     Policy accuracy @10: 92.62%
[12.39.27] [INFO]     Avg value loss: 1.0992071370283762
[12.41.20] [INFO] Results after playing two most recent models at iteration 7: Wins: 16, Losses: 18, Draws: 6
[12.43.05] [INFO] Results after playing the current vs the reference at iteration 7: Wins: 20, Losses: 18, Draws: 2
[12.43.31] [INFO] Results after playing vs random at iteration 7: Wins: 39, Losses: 0, Draws: 1
[12.46.13] [INFO] Loading memories for iteration 8 with window size 3 (5-8)
[12.46.23] [INFO] Loaded 2322365 samples from 18640 games
Training batches: 100%|█████████▉| 1131/1133 [01:15<00:00, 15.02it/s]
[12.47.39] [INFO] Last gradient norm: 0.74609375
[12.47.39] [INFO] Training stats: Policy Loss: 1.2273, Value Loss: 0.0982, Total Loss: 1.3255, Value Mean: 0.0045, Value Std: 0.8048
Validation batches: 100%|██████████| 57/57 [00:14<00:00,  3.85it/s]
[12.47.54] [INFO] Validation stats: Policy Loss: 1.1911, Value Loss: 0.0795, Total Loss: 1.2706, Value Mean: 0.0438, Value Std: 0.8123
[12.47.56] [INFO] Trainer finished at iteration 8.
[12.47.56] [INFO] All processes started at iteration 9.
[12.47.56] [INFO] Model and optimizer loaded from iteration 9
[12.48.20] [INFO] Evaluation results at iteration 8:
[12.48.20] [INFO]     Policy accuracy @1: 40.24%
[12.48.20] [INFO]     Policy accuracy @5: 81.49%
[12.48.20] [INFO]     Policy accuracy @10: 92.01%
[12.48.20] [INFO]     Avg value loss: 1.38240038951238
[12.50.26] [INFO] Results after playing two most recent models at iteration 8: Wins: 18, Losses: 13, Draws: 9
[12.52.15] [INFO] Results after playing the current vs the reference at iteration 8: Wins: 13, Losses: 20, Draws: 7
[12.52.43] [INFO] Results after playing vs random at iteration 8: Wins: 40, Losses: 0, Draws: 0
[12.55.06] [INFO] Loading memories for iteration 9 with window size 3 (6-9)
[12.55.16] [INFO] Loaded 2271475 samples from 18220 games
Training batches: 100%|█████████▉| 1106/1109 [01:17<00:00, 14.32it/s]
[12.56.33] [INFO] Last gradient norm: 0.76953125
[12.56.33] [INFO] Training stats: Policy Loss: 1.2303, Value Loss: 0.1019, Total Loss: 1.3321, Value Mean: 0.0036, Value Std: 0.8028
Validation batches: 100%|██████████| 54/54 [00:13<00:00,  4.00it/s]
[12.56.47] [INFO] Validation stats: Policy Loss: 1.2005, Value Loss: 0.1196, Total Loss: 1.3194, Value Mean: 0.0920, Value Std: 0.8129
[12.56.49] [INFO] Trainer finished at iteration 9.
[12.56.49] [INFO] All processes started at iteration 10.
[12.56.49] [INFO] Model and optimizer loaded from iteration 10
[12.57.13] [INFO] Evaluation results at iteration 9:
[12.57.13] [INFO]     Policy accuracy @1: 39.03%
[12.57.13] [INFO]     Policy accuracy @5: 80.91%
[12.57.13] [INFO]     Policy accuracy @10: 91.80%
[12.57.13] [INFO]     Avg value loss: 1.1592109680175782
[12.59.14] [INFO] Results after playing two most recent models at iteration 9: Wins: 17, Losses: 18, Draws: 5
[13.01.00] [INFO] Results after playing the current vs the reference at iteration 9: Wins: 18, Losses: 16, Draws: 6
[13.01.26] [INFO] Results after playing vs random at iteration 9: Wins: 40, Losses: 0, Draws: 0
[13.03.59] [INFO] Loading memories for iteration 10 with window size 3 (7-10)
[13.04.08] [INFO] Loaded 2313245 samples from 18540 games
Training batches: 100%|█████████▉| 1125/1129 [01:18<00:00, 14.37it/s]
[13.05.27] [INFO] Last gradient norm: 0.8046875
[13.05.27] [INFO] Training stats: Policy Loss: 1.2301, Value Loss: 0.0965, Total Loss: 1.3266, Value Mean: 0.0037, Value Std: 0.8070
Validation batches: 100%|██████████| 57/57 [00:14<00:00,  3.89it/s]
[13.05.41] [INFO] Validation stats: Policy Loss: 1.2002, Value Loss: 0.1112, Total Loss: 1.3113, Value Mean: -0.0108, Value Std: 0.8335
[13.05.43] [INFO] Trainer finished at iteration 10.
[13.05.43] [INFO] All processes started at iteration 11.
[13.05.44] [INFO] Model and optimizer loaded from iteration 11
[13.06.08] [INFO] Evaluation results at iteration 10:
[13.06.08] [INFO]     Policy accuracy @1: 39.45%
[13.06.08] [INFO]     Policy accuracy @5: 80.35%
[13.06.08] [INFO]     Policy accuracy @10: 92.09%
[13.06.08] [INFO]     Avg value loss: 1.3238402962684632
[13.08.02] [INFO] Results after playing two most recent models at iteration 10: Wins: 21, Losses: 13, Draws: 6
[13.09.46] [INFO] Results after playing the current vs the reference at iteration 10: Wins: 16, Losses: 17, Draws: 7
[13.10.12] [INFO] Results after playing vs random at iteration 10: Wins: 40, Losses: 0, Draws: 0
[13.13.13] [INFO] Loading memories for iteration 11 with window size 3 (8-11)
[13.13.23] [INFO] Loaded 2300605 samples from 18440 games
Training batches: 100%|█████████▉| 1120/1123 [01:17<00:00, 14.47it/s]
[13.14.40] [INFO] Last gradient norm: 0.8125
[13.14.40] [INFO] Training stats: Policy Loss: 1.2384, Value Loss: 0.0954, Total Loss: 1.3339, Value Mean: 0.0042, Value Std: 0.8078
Validation batches: 100%|██████████| 55/55 [00:13<00:00,  4.00it/s]
[13.14.54] [INFO] Validation stats: Policy Loss: 1.1950, Value Loss: 0.0635, Total Loss: 1.2589, Value Mean: 0.0193, Value Std: 0.8318
[13.14.57] [INFO] Trainer finished at iteration 11.
[13.14.57] [INFO] All processes started at iteration 12.
[13.14.57] [INFO] Model and optimizer loaded from iteration 12
[13.15.20] [INFO] Evaluation results at iteration 11:
[13.15.20] [INFO]     Policy accuracy @1: 39.16%
[13.15.20] [INFO]     Policy accuracy @5: 79.30%
[13.15.20] [INFO]     Policy accuracy @10: 91.75%
[13.15.20] [INFO]     Avg value loss: 1.4114861925443014
[13.17.25] [INFO] Results after playing two most recent models at iteration 11: Wins: 13, Losses: 21, Draws: 6
[13.19.09] [INFO] Results after playing the current vs the reference at iteration 11: Wins: 10, Losses: 25, Draws: 5
[13.19.34] [INFO] Results after playing vs random at iteration 11: Wins: 40, Losses: 0, Draws: 0
[13.22.07] [INFO] Loading memories for iteration 12 with window size 3 (9-12)
[13.22.17] [INFO] Loaded 2296515 samples from 18420 games
Training batches: 100%|█████████▉| 1118/1121 [01:20<00:00, 13.86it/s]
[13.23.38] [INFO] Last gradient norm: 0.6875
[13.23.38] [INFO] Training stats: Policy Loss: 1.2442, Value Loss: 0.0959, Total Loss: 1.3399, Value Mean: 0.0051, Value Std: 0.8105
Validation batches: 100%|██████████| 56/56 [00:14<00:00,  3.74it/s]
[13.23.53] [INFO] Validation stats: Policy Loss: 1.2097, Value Loss: 0.1671, Total Loss: 1.3764, Value Mean: -0.1107, Value Std: 0.8288
[13.23.55] [INFO] Trainer finished at iteration 12.
[13.23.55] [INFO] All processes started at iteration 13.
[13.23.55] [INFO] Model and optimizer loaded from iteration 13
[13.24.20] [INFO] Evaluation results at iteration 12:
[13.24.20] [INFO]     Policy accuracy @1: 39.79%
[13.24.20] [INFO]     Policy accuracy @5: 79.19%
[13.24.20] [INFO]     Policy accuracy @10: 91.30%
[13.24.20] [INFO]     Avg value loss: 1.1955758651097617
[13.26.16] [INFO] Results after playing two most recent models at iteration 12: Wins: 22, Losses: 15, Draws: 3
[13.28.08] [INFO] Results after playing the current vs the reference at iteration 12: Wins: 16, Losses: 18, Draws: 6
[13.28.36] [INFO] Results after playing vs random at iteration 12: Wins: 40, Losses: 0, Draws: 0
[13.31.36] [INFO] Loading memories for iteration 13 with window size 3 (10-13)
[13.31.45] [INFO] Loaded 2290470 samples from 18400 games
Training batches: 100%|█████████▉| 1117/1118 [01:15<00:00, 14.83it/s]
[13.33.02] [INFO] Last gradient norm: 0.703125
[13.33.02] [INFO] Training stats: Policy Loss: 1.2446, Value Loss: 0.0898, Total Loss: 1.3343, Value Mean: 0.0054, Value Std: 0.8169
Validation batches: 100%|██████████| 53/53 [00:14<00:00,  3.68it/s]
[13.33.16] [INFO] Validation stats: Policy Loss: 1.1968, Value Loss: 0.1363, Total Loss: 1.3330, Value Mean: 0.0823, Value Std: 0.8407
[13.33.18] [INFO] Trainer finished at iteration 13.
[13.33.18] [INFO] All processes started at iteration 14.
[13.33.18] [INFO] Model and optimizer loaded from iteration 14
[13.33.44] [INFO] Evaluation results at iteration 13:
[13.33.44] [INFO]     Policy accuracy @1: 38.82%
[13.33.44] [INFO]     Policy accuracy @5: 79.27%
[13.33.44] [INFO]     Policy accuracy @10: 91.14%
[13.33.44] [INFO]     Avg value loss: 1.497272249062856
[13.35.55] [INFO] Results after playing two most recent models at iteration 13: Wins: 13, Losses: 19, Draws: 8
[13.37.36] [INFO] Results after playing the current vs the reference at iteration 13: Wins: 10, Losses: 26, Draws: 4
[13.38.04] [INFO] Results after playing vs random at iteration 13: Wins: 40, Losses: 0, Draws: 0
[13.40.19] [INFO] Loading memories for iteration 14 with window size 3 (11-14)
[13.40.30] [INFO] Loaded 2282965 samples from 18340 games
Training batches: 100%|█████████▉| 1113/1114 [01:15<00:00, 14.66it/s]
[13.41.46] [INFO] Last gradient norm: 0.765625
[13.41.46] [INFO] Training stats: Policy Loss: 1.2437, Value Loss: 0.0897, Total Loss: 1.3333, Value Mean: 0.0049, Value Std: 0.8182
Validation batches: 100%|██████████| 56/56 [00:15<00:00,  3.51it/s]
[13.42.02] [INFO] Validation stats: Policy Loss: 1.1981, Value Loss: 0.0992, Total Loss: 1.2972, Value Mean: 0.1012, Value Std: 0.8237
[13.42.04] [INFO] Trainer finished at iteration 14.
[13.42.04] [INFO] All processes started at iteration 15.
[13.42.04] [INFO] Model and optimizer loaded from iteration 15
[13.42.27] [INFO] Evaluation results at iteration 14:
[13.42.27] [INFO]     Policy accuracy @1: 38.23%
[13.42.27] [INFO]     Policy accuracy @5: 79.03%
[13.42.27] [INFO]     Policy accuracy @10: 90.82%
[13.42.27] [INFO]     Avg value loss: 1.448618268966675
[13.44.25] [INFO] Results after playing two most recent models at iteration 14: Wins: 13, Losses: 23, Draws: 4
[13.46.03] [INFO] Results after playing the current vs the reference at iteration 14: Wins: 14, Losses: 23, Draws: 3
[13.46.29] [INFO] Results after playing vs random at iteration 14: Wins: 40, Losses: 0, Draws: 0
[13.49.24] [INFO] Loading memories for iteration 15 with window size 3 (12-15)
[13.49.34] [INFO] Loaded 2284390 samples from 18360 games
Training batches: 100%|█████████▉| 1114/1115 [01:19<00:00, 14.05it/s]
[13.50.54] [INFO] Last gradient norm: 0.6875
[13.50.54] [INFO] Training stats: Policy Loss: 1.2291, Value Loss: 0.0867, Total Loss: 1.3158, Value Mean: 0.0040, Value Std: 0.8176
Validation batches: 100%|██████████| 55/55 [00:13<00:00,  3.99it/s]
[13.51.07] [INFO] Validation stats: Policy Loss: 1.1652, Value Loss: 0.0618, Total Loss: 1.2271, Value Mean: -0.0194, Value Std: 0.8398
[13.51.09] [INFO] Trainer finished at iteration 15.
[13.51.09] [INFO] All processes started at iteration 16.
[13.51.09] [INFO] Model and optimizer loaded from iteration 16
[13.51.36] [INFO] Evaluation results at iteration 15:
[13.51.36] [INFO]     Policy accuracy @1: 37.92%
[13.51.36] [INFO]     Policy accuracy @5: 78.61%
[13.51.36] [INFO]     Policy accuracy @10: 90.03%
[13.51.36] [INFO]     Avg value loss: 1.1401116212209066
[13.53.33] [INFO] Results after playing two most recent models at iteration 15: Wins: 20, Losses: 17, Draws: 3
[13.55.34] [INFO] Results after playing the current vs the reference at iteration 15: Wins: 11, Losses: 21, Draws: 8
[13.55.57] [INFO] Results after playing vs random at iteration 15: Wins: 39, Losses: 0, Draws: 1
[13.58.40] [INFO] Loading memories for iteration 16 with window size 3 (13-16)
[13.58.49] [INFO] Loaded 2318250 samples from 18620 games
Training batches: 100%|█████████▉| 1129/1131 [01:19<00:00, 14.16it/s]
[14.00.09] [INFO] Last gradient norm: 0.7890625
[14.00.09] [INFO] Training stats: Policy Loss: 1.2208, Value Loss: 0.0882, Total Loss: 1.3091, Value Mean: 0.0038, Value Std: 0.8135
Validation batches: 100%|██████████| 59/59 [00:15<00:00,  3.81it/s]
[14.00.25] [INFO] Validation stats: Policy Loss: 1.1307, Value Loss: 0.0442, Total Loss: 1.1747, Value Mean: -0.0179, Value Std: 0.8267
[14.00.27] [INFO] Trainer finished at iteration 16.
[14.00.27] [INFO] All processes started at iteration 17.
[14.00.27] [INFO] Model and optimizer loaded from iteration 17
[14.00.51] [INFO] Evaluation results at iteration 16:
[14.00.51] [INFO]     Policy accuracy @1: 38.31%
[14.00.51] [INFO]     Policy accuracy @5: 78.56%
[14.00.51] [INFO]     Policy accuracy @10: 90.88%
[14.00.51] [INFO]     Avg value loss: 1.1595461308956145
[14.02.52] [INFO] Results after playing two most recent models at iteration 16: Wins: 17, Losses: 16, Draws: 7
[14.04.42] [INFO] Results after playing the current vs the reference at iteration 16: Wins: 8, Losses: 29, Draws: 3
[14.05.09] [INFO] Results after playing vs random at iteration 16: Wins: 39, Losses: 0, Draws: 1
[14.07.37] [INFO] Loading memories for iteration 17 with window size 3 (14-17)
[14.07.46] [INFO] Loaded 2338970 samples from 18760 games
Training batches: 100%|█████████▉| 1139/1142 [01:19<00:00, 14.26it/s]
[14.09.06] [INFO] Last gradient norm: 0.7890625
[14.09.06] [INFO] Training stats: Policy Loss: 1.2156, Value Loss: 0.0863, Total Loss: 1.3019, Value Mean: 0.0037, Value Std: 0.8162
Validation batches: 100%|██████████| 55/55 [00:14<00:00,  3.69it/s]
[14.09.21] [INFO] Validation stats: Policy Loss: 1.1683, Value Loss: 0.1037, Total Loss: 1.2723, Value Mean: 0.0448, Value Std: 0.8263
[14.09.23] [INFO] Trainer finished at iteration 17.
[14.09.23] [INFO] All processes started at iteration 18.
[14.09.23] [INFO] Model and optimizer loaded from iteration 18
[14.09.47] [INFO] Evaluation results at iteration 17:
[14.09.47] [INFO]     Policy accuracy @1: 38.39%
[14.09.47] [INFO]     Policy accuracy @5: 78.64%
[14.09.47] [INFO]     Policy accuracy @10: 90.48%
[14.09.47] [INFO]     Avg value loss: 1.1464169104894002
[14.11.40] [INFO] Results after playing two most recent models at iteration 17: Wins: 19, Losses: 17, Draws: 4
[14.13.15] [INFO] Results after playing the current vs the reference at iteration 17: Wins: 14, Losses: 19, Draws: 7
[14.13.39] [INFO] Results after playing vs random at iteration 17: Wins: 40, Losses: 0, Draws: 0
[14.16.32] [INFO] Loading memories for iteration 18 with window size 3 (15-18)
[14.16.43] [INFO] Loaded 2348740 samples from 18840 games
Training batches: 100%|█████████▉| 1144/1146 [01:15<00:00, 15.07it/s]
[14.17.59] [INFO] Last gradient norm: 0.69921875
[14.17.59] [INFO] Training stats: Policy Loss: 1.2168, Value Loss: 0.0861, Total Loss: 1.3029, Value Mean: 0.0046, Value Std: 0.8148
Validation batches: 100%|██████████| 57/57 [00:14<00:00,  4.01it/s]
[14.18.13] [INFO] Validation stats: Policy Loss: 1.1609, Value Loss: 0.0861, Total Loss: 1.2473, Value Mean: 0.0869, Value Std: 0.8351
[14.18.15] [INFO] Trainer finished at iteration 18.
[14.18.15] [INFO] All processes started at iteration 19.
[14.18.15] [INFO] Model and optimizer loaded from iteration 19
[14.18.38] [INFO] Evaluation results at iteration 18:
[14.18.38] [INFO]     Policy accuracy @1: 38.18%
[14.18.38] [INFO]     Policy accuracy @5: 78.19%
[14.18.38] [INFO]     Policy accuracy @10: 90.43%
[14.18.38] [INFO]     Avg value loss: 1.113001177708308
[14.20.30] [INFO] Results after playing two most recent models at iteration 18: Wins: 16, Losses: 18, Draws: 6
[14.22.20] [INFO] Results after playing the current vs the reference at iteration 18: Wins: 10, Losses: 23, Draws: 7
[14.22.43] [INFO] Results after playing vs random at iteration 18: Wins: 40, Losses: 0, Draws: 0
[14.25.14] [INFO] Loading memories for iteration 19 with window size 3 (16-19)
[14.25.24] [INFO] Loaded 2349925 samples from 18840 games
Training batches: 100%|█████████▉| 1144/1147 [01:20<00:00, 14.17it/s]
[14.26.45] [INFO] Last gradient norm: 0.671875
[14.26.45] [INFO] Training stats: Policy Loss: 1.2107, Value Loss: 0.0867, Total Loss: 1.2973, Value Mean: 0.0049, Value Std: 0.8110
Validation batches: 100%|██████████| 55/55 [00:14<00:00,  3.79it/s]
[14.26.59] [INFO] Validation stats: Policy Loss: 1.1459, Value Loss: 0.0664, Total Loss: 1.2121, Value Mean: 0.0125, Value Std: 0.8413
[14.27.01] [INFO] Trainer finished at iteration 19.
[14.27.01] [INFO] All processes started at iteration 20.
[14.27.01] [INFO] Model and optimizer loaded from iteration 20
[14.27.29] [INFO] Evaluation results at iteration 19:
[14.27.29] [INFO]     Policy accuracy @1: 37.02%
[14.27.29] [INFO]     Policy accuracy @5: 78.27%
[14.27.29] [INFO]     Policy accuracy @10: 90.69%
[14.27.29] [INFO]     Avg value loss: 1.1603581925233206
[14.29.27] [INFO] Results after playing two most recent models at iteration 19: Wins: 18, Losses: 15, Draws: 7
[14.31.15] [INFO] Results after playing the current vs the reference at iteration 19: Wins: 11, Losses: 22, Draws: 7
[14.31.38] [INFO] Results after playing vs random at iteration 19: Wins: 39, Losses: 0, Draws: 1
[14.34.02] [INFO] Loading memories for iteration 20 with window size 3 (17-20)
[14.34.10] [INFO] Loaded 2317595 samples from 18580 games
Training batches: 100%|█████████▉| 1128/1131 [01:15<00:00, 15.01it/s]
[14.35.25] [INFO] Last gradient norm: 0.73828125
[14.35.25] [INFO] Training stats: Policy Loss: 1.2043, Value Loss: 0.0866, Total Loss: 1.2909, Value Mean: 0.0057, Value Std: 0.8131
Validation batches: 100%|██████████| 56/56 [00:14<00:00,  3.94it/s]
[14.35.39] [INFO] Validation stats: Policy Loss: 1.1564, Value Loss: 0.0913, Total Loss: 1.2479, Value Mean: -0.0169, Value Std: 0.8215
[14.35.41] [INFO] Trainer finished at iteration 20.
[14.35.41] [INFO] All processes started at iteration 21.
[14.35.41] [INFO] Model and optimizer loaded from iteration 21
[14.36.05] [INFO] Evaluation results at iteration 20:
[14.36.05] [INFO]     Policy accuracy @1: 38.13%
[14.36.05] [INFO]     Policy accuracy @5: 78.79%
[14.36.05] [INFO]     Policy accuracy @10: 90.35%
[14.36.05] [INFO]     Avg value loss: 1.207214609781901
[14.38.03] [INFO] Results after playing two most recent models at iteration 20: Wins: 15, Losses: 18, Draws: 7
[14.39.45] [INFO] Results after playing the current vs the reference at iteration 20: Wins: 16, Losses: 19, Draws: 5
[14.40.14] [INFO] Results after playing vs random at iteration 20: Wins: 40, Losses: 0, Draws: 0
[14.43.02] [INFO] Loading memories for iteration 21 with window size 3 (18-21)
[14.43.12] [INFO] Loaded 2330170 samples from 18700 games
Training batches: 100%|█████████▉| 1134/1137 [01:14<00:00, 15.27it/s]
[14.44.26] [INFO] Last gradient norm: 0.76953125
[14.44.26] [INFO] Training stats: Policy Loss: 1.1987, Value Loss: 0.0848, Total Loss: 1.2835, Value Mean: 0.0052, Value Std: 0.8155
Validation batches: 100%|██████████| 56/56 [00:12<00:00,  4.32it/s]
[14.44.39] [INFO] Validation stats: Policy Loss: 1.1726, Value Loss: 0.0718, Total Loss: 1.2439, Value Mean: 0.0139, Value Std: 0.8267
[14.44.41] [INFO] Trainer finished at iteration 21.
[14.44.41] [INFO] All processes started at iteration 22.
[14.44.41] [INFO] Model and optimizer loaded from iteration 22
[14.45.03] [INFO] Evaluation results at iteration 21:
[14.45.03] [INFO]     Policy accuracy @1: 37.31%
[14.45.03] [INFO]     Policy accuracy @5: 78.27%
[14.45.03] [INFO]     Policy accuracy @10: 89.85%
[14.45.03] [INFO]     Avg value loss: 1.008933142820994
[14.46.45] [INFO] Results after playing two most recent models at iteration 21: Wins: 17, Losses: 22, Draws: 1
[14.48.40] [INFO] Results after playing the current vs the reference at iteration 21: Wins: 10, Losses: 26, Draws: 4
[14.49.05] [INFO] Results after playing vs random at iteration 21: Wins: 38, Losses: 0, Draws: 2
[14.52.01] [INFO] Loading memories for iteration 22 with window size 3 (19-22)
[14.52.10] [INFO] Loaded 2334730 samples from 18740 games
Training batches: 100%|█████████▉| 1137/1140 [01:17<00:00, 14.73it/s]
[14.53.27] [INFO] Last gradient norm: 0.90234375
[14.53.27] [INFO] Training stats: Policy Loss: 1.2029, Value Loss: 0.0865, Total Loss: 1.2895, Value Mean: 0.0049, Value Std: 0.8134
Validation batches: 100%|██████████| 58/58 [00:14<00:00,  4.01it/s]
[14.53.42] [INFO] Validation stats: Policy Loss: 1.1789, Value Loss: 0.0848, Total Loss: 1.2640, Value Mean: -0.0772, Value Std: 0.7893
[14.53.44] [INFO] Trainer finished at iteration 22.
[14.53.44] [INFO] All processes started at iteration 23.
[14.53.44] [INFO] Model and optimizer loaded from iteration 23
[14.54.09] [INFO] Evaluation results at iteration 22:
[14.54.09] [INFO]     Policy accuracy @1: 37.07%
[14.54.09] [INFO]     Policy accuracy @5: 77.76%
[14.54.09] [INFO]     Policy accuracy @10: 89.95%
[14.54.09] [INFO]     Avg value loss: 1.034398724635442
[14.55.57] [INFO] Results after playing two most recent models at iteration 22: Wins: 16, Losses: 20, Draws: 4
[14.57.40] [INFO] Results after playing the current vs the reference at iteration 22: Wins: 20, Losses: 16, Draws: 4
[14.58.06] [INFO] Results after playing vs random at iteration 22: Wins: 39, Losses: 0, Draws: 1
[15.00.44] [INFO] Loading memories for iteration 23 with window size 3 (20-23)
[15.00.53] [INFO] Loaded 2354745 samples from 18900 games
Training batches: 100%|█████████▉| 1148/1149 [01:19<00:00, 14.41it/s]
[15.02.13] [INFO] Last gradient norm: 0.828125
[15.02.13] [INFO] Training stats: Policy Loss: 1.2016, Value Loss: 0.0841, Total Loss: 1.2857, Value Mean: 0.0051, Value Std: 0.8159
Validation batches: 100%|██████████| 57/57 [00:15<00:00,  3.78it/s]
[15.02.28] [INFO] Validation stats: Policy Loss: 1.1695, Value Loss: 0.0841, Total Loss: 1.2534, Value Mean: -0.0142, Value Std: 0.8224
[15.02.31] [INFO] Trainer finished at iteration 23.
[15.02.31] [INFO] All processes started at iteration 24.
[15.02.31] [INFO] Model and optimizer loaded from iteration 24
[15.02.53] [INFO] Evaluation results at iteration 23:
[15.02.53] [INFO]     Policy accuracy @1: 37.78%
[15.02.53] [INFO]     Policy accuracy @5: 77.21%
[15.02.53] [INFO]     Policy accuracy @10: 88.87%
[15.02.53] [INFO]     Avg value loss: 1.07283762494723
[15.04.45] [INFO] Results after playing two most recent models at iteration 23: Wins: 19, Losses: 17, Draws: 4
[15.06.43] [INFO] Results after playing the current vs the reference at iteration 23: Wins: 11, Losses: 24, Draws: 5
[15.07.07] [INFO] Results after playing vs random at iteration 23: Wins: 40, Losses: 0, Draws: 0
[15.10.11] [INFO] Loading memories for iteration 24 with window size 3 (21-24)
[15.10.21] [INFO] Loaded 2369645 samples from 19020 games
Training batches: 100%|█████████▉| 1155/1157 [01:18<00:00, 14.71it/s]
[15.11.39] [INFO] Last gradient norm: 0.78125
[15.11.39] [INFO] Training stats: Policy Loss: 1.2062, Value Loss: 0.0826, Total Loss: 1.2887, Value Mean: 0.0049, Value Std: 0.8159
Validation batches: 100%|██████████| 58/58 [00:15<00:00,  3.63it/s]
[15.11.55] [INFO] Validation stats: Policy Loss: 1.1771, Value Loss: 0.0720, Total Loss: 1.2488, Value Mean: 0.0057, Value Std: 0.8436
[15.11.58] [INFO] Trainer finished at iteration 24.
[15.11.58] [INFO] All processes started at iteration 25.
[15.11.58] [INFO] Model and optimizer loaded from iteration 25
[15.12.21] [INFO] Evaluation results at iteration 24:
[15.12.21] [INFO]     Policy accuracy @1: 36.12%
[15.12.21] [INFO]     Policy accuracy @5: 76.84%
[15.12.21] [INFO]     Policy accuracy @10: 89.03%
[15.12.21] [INFO]     Avg value loss: 1.3312485734621684
[15.14.11] [INFO] Results after playing two most recent models at iteration 24: Wins: 10, Losses: 23, Draws: 7
[15.16.05] [INFO] Results after playing the current vs the reference at iteration 24: Wins: 9, Losses: 26, Draws: 5
[15.16.31] [INFO] Results after playing vs random at iteration 24: Wins: 40, Losses: 0, Draws: 0
[15.18.57] [INFO] Loading memories for iteration 25 with window size 3 (22-25)
[15.19.07] [INFO] Loaded 2336615 samples from 18760 games
Training batches: 100%|█████████▉| 1138/1140 [01:17<00:00, 14.78it/s]
[15.20.24] [INFO] Last gradient norm: 0.734375
[15.20.24] [INFO] Training stats: Policy Loss: 1.2178, Value Loss: 0.0855, Total Loss: 1.3033, Value Mean: 0.0053, Value Std: 0.8132
Validation batches: 100%|██████████| 53/53 [00:15<00:00,  3.41it/s]
[15.20.40] [INFO] Validation stats: Policy Loss: 1.1986, Value Loss: 0.2384, Total Loss: 1.4369, Value Mean: -0.0211, Value Std: 0.8120
[15.20.42] [INFO] Trainer finished at iteration 25.
[15.20.42] [INFO] All processes started at iteration 26.
[15.20.42] [INFO] Model and optimizer loaded from iteration 26
[15.21.05] [INFO] Evaluation results at iteration 25:
[15.21.05] [INFO]     Policy accuracy @1: 37.36%
[15.21.05] [INFO]     Policy accuracy @5: 77.26%
[15.21.05] [INFO]     Policy accuracy @10: 89.00%
[15.21.05] [INFO]     Avg value loss: 1.222480583190918
[15.23.00] [INFO] Results after playing two most recent models at iteration 25: Wins: 13, Losses: 22, Draws: 5
[15.24.40] [INFO] Results after playing the current vs the reference at iteration 25: Wins: 13, Losses: 25, Draws: 2
[15.25.08] [INFO] Results after playing vs random at iteration 25: Wins: 40, Losses: 0, Draws: 0
[15.27.51] [INFO] Loading memories for iteration 26 with window size 3 (23-26)
[15.28.00] [INFO] Loaded 2344910 samples from 18820 games
Training batches: 100%|█████████▉| 1143/1144 [01:15<00:00, 15.20it/s]
[15.29.16] [INFO] Last gradient norm: 0.7734375
[15.29.16] [INFO] Training stats: Policy Loss: 1.2184, Value Loss: 0.0862, Total Loss: 1.3046, Value Mean: 0.0044, Value Std: 0.8156
Validation batches: 100%|██████████| 58/58 [00:15<00:00,  3.71it/s]
[15.29.31] [INFO] Validation stats: Policy Loss: 1.1686, Value Loss: 0.0965, Total Loss: 1.2648, Value Mean: 0.0405, Value Std: 0.8390
[15.29.33] [INFO] Trainer finished at iteration 26.
[15.29.33] [INFO] All processes started at iteration 27.
[15.29.33] [INFO] Model and optimizer loaded from iteration 27
[15.29.56] [INFO] Evaluation results at iteration 26:
[15.29.56] [INFO]     Policy accuracy @1: 35.43%
[15.29.56] [INFO]     Policy accuracy @5: 76.52%
[15.29.56] [INFO]     Policy accuracy @10: 88.74%
[15.29.56] [INFO]     Avg value loss: 1.4201122800509134
[15.31.43] [INFO] Results after playing two most recent models at iteration 26: Wins: 16, Losses: 17, Draws: 7
[15.33.22] [INFO] Results after playing the current vs the reference at iteration 26: Wins: 15, Losses: 22, Draws: 3
[15.33.51] [INFO] Results after playing vs random at iteration 26: Wins: 40, Losses: 0, Draws: 0
[15.36.42] [INFO] Loading memories for iteration 27 with window size 3 (24-27)
[15.36.51] [INFO] Loaded 2321620 samples from 18640 games
Training batches: 100%|█████████▉| 1131/1133 [01:19<00:00, 14.18it/s]
[15.38.11] [INFO] Last gradient norm: 0.75390625
[15.38.11] [INFO] Training stats: Policy Loss: 1.2136, Value Loss: 0.0865, Total Loss: 1.3001, Value Mean: 0.0047, Value Std: 0.8131
Validation batches: 100%|██████████| 55/55 [00:15<00:00,  3.49it/s]
[15.38.27] [INFO] Validation stats: Policy Loss: 1.1760, Value Loss: 0.0806, Total Loss: 1.2565, Value Mean: -0.0260, Value Std: 0.8254
[15.38.29] [INFO] Trainer finished at iteration 27.
[15.38.29] [INFO] All processes started at iteration 28.
[15.38.29] [INFO] Model and optimizer loaded from iteration 28
[15.38.52] [INFO] Evaluation results at iteration 27:
[15.38.52] [INFO]     Policy accuracy @1: 36.81%
[15.38.52] [INFO]     Policy accuracy @5: 76.73%
[15.38.52] [INFO]     Policy accuracy @10: 89.32%
[15.38.52] [INFO]     Avg value loss: 1.3537587682406107
[15.40.49] [INFO] Results after playing two most recent models at iteration 27: Wins: 14, Losses: 19, Draws: 7
[15.42.30] [INFO] Results after playing the current vs the reference at iteration 27: Wins: 16, Losses: 24, Draws: 0
[15.42.57] [INFO] Results after playing vs random at iteration 27: Wins: 39, Losses: 0, Draws: 1
[15.45.28] [INFO] Loading memories for iteration 28 with window size 3 (25-28)
[15.45.37] [INFO] Loaded 2312860 samples from 18560 games
Training batches: 100%|█████████▉| 1127/1129 [01:13<00:00, 15.26it/s]
[15.46.52] [INFO] Last gradient norm: 0.703125
[15.46.52] [INFO] Training stats: Policy Loss: 1.2139, Value Loss: 0.0834, Total Loss: 1.2974, Value Mean: 0.0048, Value Std: 0.8156
Validation batches: 100%|██████████| 57/57 [00:12<00:00,  4.67it/s]
[15.47.04] [INFO] Validation stats: Policy Loss: 1.1694, Value Loss: 0.0723, Total Loss: 1.2421, Value Mean: 0.0018, Value Std: 0.8343
[15.47.06] [INFO] Trainer finished at iteration 28.
[15.47.06] [INFO] All processes started at iteration 29.
[15.47.06] [INFO] Model and optimizer loaded from iteration 29
[15.47.32] [INFO] Evaluation results at iteration 28:
[15.47.32] [INFO]     Policy accuracy @1: 36.14%
[15.47.32] [INFO]     Policy accuracy @5: 77.31%
[15.47.32] [INFO]     Policy accuracy @10: 88.82%
[15.47.32] [INFO]     Avg value loss: 1.162091334660848
[15.49.29] [INFO] Results after playing two most recent models at iteration 28: Wins: 19, Losses: 12, Draws: 9
[15.51.10] [INFO] Results after playing the current vs the reference at iteration 28: Wins: 8, Losses: 28, Draws: 4
[15.51.37] [INFO] Results after playing vs random at iteration 28: Wins: 38, Losses: 0, Draws: 2
[15.54.35] [INFO] Loading memories for iteration 29 with window size 3 (26-29)
[15.54.45] [INFO] Loaded 2361905 samples from 18940 games
Training batches: 100%|█████████▉| 1150/1153 [01:13<00:00, 15.57it/s]
[15.55.59] [INFO] Last gradient norm: 0.6484375
[15.55.59] [INFO] Training stats: Policy Loss: 1.2181, Value Loss: 0.0826, Total Loss: 1.3008, Value Mean: 0.0042, Value Std: 0.8152
Validation batches: 100%|██████████| 58/58 [00:16<00:00,  3.55it/s]
[15.56.15] [INFO] Validation stats: Policy Loss: 1.1785, Value Loss: 0.0649, Total Loss: 1.2430, Value Mean: 0.0123, Value Std: 0.8359
[15.56.17] [INFO] Trainer finished at iteration 29.
[15.56.17] [INFO] All processes started at iteration 30.
[15.56.18] [INFO] Model and optimizer loaded from iteration 30
[15.56.41] [INFO] Evaluation results at iteration 29:
[15.56.41] [INFO]     Policy accuracy @1: 36.49%
[15.56.41] [INFO]     Policy accuracy @5: 77.10%
[15.56.41] [INFO]     Policy accuracy @10: 88.89%
[15.56.41] [INFO]     Avg value loss: 1.0636916299661001
[15.58.36] [INFO] Results after playing two most recent models at iteration 29: Wins: 12, Losses: 15, Draws: 13
[16.00.31] [INFO] Results after playing the current vs the reference at iteration 29: Wins: 18, Losses: 12, Draws: 10
[16.00.59] [INFO] Results after playing vs random at iteration 29: Wins: 39, Losses: 0, Draws: 1
[16.03.37] [INFO] Loading memories for iteration 30 with window size 3 (27-30)
[16.03.47] [INFO] Loaded 2318190 samples from 18580 games
Training batches: 100%|█████████▉| 1130/1131 [01:15<00:00, 14.98it/s]
[16.05.02] [INFO] Last gradient norm: 0.7890625
[16.05.02] [INFO] Training stats: Policy Loss: 1.2323, Value Loss: 0.0861, Total Loss: 1.3185, Value Mean: 0.0049, Value Std: 0.8134
Validation batches: 100%|██████████| 54/54 [00:14<00:00,  3.60it/s]
[16.05.17] [INFO] Validation stats: Policy Loss: 1.1839, Value Loss: 0.0919, Total Loss: 1.2762, Value Mean: 0.0200, Value Std: 0.8107
[16.05.19] [INFO] Trainer finished at iteration 30.
[16.05.19] [INFO] All processes started at iteration 31.
[16.05.20] [INFO] Model and optimizer loaded from iteration 31
[16.05.43] [INFO] Evaluation results at iteration 30:
[16.05.43] [INFO]     Policy accuracy @1: 35.46%
[16.05.43] [INFO]     Policy accuracy @5: 77.26%
[16.05.43] [INFO]     Policy accuracy @10: 88.76%
[16.05.43] [INFO]     Avg value loss: 1.0029115001360576
[16.07.40] [INFO] Results after playing two most recent models at iteration 30: Wins: 20, Losses: 12, Draws: 8
[16.09.32] [INFO] Results after playing the current vs the reference at iteration 30: Wins: 10, Losses: 24, Draws: 6
[16.09.55] [INFO] Results after playing vs random at iteration 30: Wins: 40, Losses: 0, Draws: 0
[16.12.19] [INFO] Loading memories for iteration 31 with window size 3 (28-31)
[16.12.29] [INFO] Loaded 2344015 samples from 18800 games
Training batches: 100%|█████████▉| 1141/1144 [01:15<00:00, 15.12it/s]
[16.13.44] [INFO] Last gradient norm: 0.71875
[16.13.44] [INFO] Training stats: Policy Loss: 1.2257, Value Loss: 0.0808, Total Loss: 1.3066, Value Mean: 0.0049, Value Std: 0.8140
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.38it/s]
[16.14.01] [INFO] Validation stats: Policy Loss: 1.1624, Value Loss: 0.0935, Total Loss: 1.2562, Value Mean: 0.0955, Value Std: 0.8248
[16.14.03] [INFO] Trainer finished at iteration 31.
[16.14.03] [INFO] All processes started at iteration 32.
[16.14.03] [INFO] Model and optimizer loaded from iteration 32
[16.14.27] [INFO] Evaluation results at iteration 31:
[16.14.27] [INFO]     Policy accuracy @1: 35.17%
[16.14.27] [INFO]     Policy accuracy @5: 76.02%
[16.14.27] [INFO]     Policy accuracy @10: 88.55%
[16.14.27] [INFO]     Avg value loss: 1.1334146857261658
[16.16.27] [INFO] Results after playing two most recent models at iteration 31: Wins: 16, Losses: 16, Draws: 8
[16.18.15] [INFO] Results after playing the current vs the reference at iteration 31: Wins: 9, Losses: 21, Draws: 10
[16.18.41] [INFO] Results after playing vs random at iteration 31: Wins: 40, Losses: 0, Draws: 0
[16.21.14] [INFO] Loading memories for iteration 32 with window size 3 (29-32)
[16.21.24] [INFO] Loaded 2307070 samples from 18520 games
Training batches: 100%|█████████▉| 1123/1126 [01:13<00:00, 15.25it/s]
[16.22.38] [INFO] Last gradient norm: 0.73046875
[16.22.38] [INFO] Training stats: Policy Loss: 1.2181, Value Loss: 0.0824, Total Loss: 1.3005, Value Mean: 0.0052, Value Std: 0.8159
Validation batches: 100%|██████████| 53/53 [00:13<00:00,  3.80it/s]
[16.22.52] [INFO] Validation stats: Policy Loss: 1.1517, Value Loss: 0.0626, Total Loss: 1.2143, Value Mean: 0.0152, Value Std: 0.8274
[16.22.53] [INFO] Trainer finished at iteration 32.
[16.22.53] [INFO] All processes started at iteration 33.
[16.22.54] [INFO] Model and optimizer loaded from iteration 33
[16.23.17] [INFO] Evaluation results at iteration 32:
[16.23.17] [INFO]     Policy accuracy @1: 35.25%
[16.23.17] [INFO]     Policy accuracy @5: 76.18%
[16.23.17] [INFO]     Policy accuracy @10: 89.21%
[16.23.17] [INFO]     Avg value loss: 0.9878470718860626
[16.25.30] [INFO] Results after playing two most recent models at iteration 32: Wins: 12, Losses: 18, Draws: 10
[16.27.12] [INFO] Results after playing the current vs the reference at iteration 32: Wins: 13, Losses: 20, Draws: 7
[16.27.38] [INFO] Results after playing vs random at iteration 32: Wins: 39, Losses: 0, Draws: 1
[16.29.53] [INFO] Loading memories for iteration 33 with window size 3 (30-33)
[16.30.02] [INFO] Loaded 2294330 samples from 18420 games
Training batches: 100%|█████████▉| 1117/1120 [01:13<00:00, 15.24it/s]
[16.31.16] [INFO] Last gradient norm: 0.734375
[16.31.16] [INFO] Training stats: Policy Loss: 1.2028, Value Loss: 0.0799, Total Loss: 1.2828, Value Mean: 0.0052, Value Std: 0.8153
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.44it/s]
[16.31.33] [INFO] Validation stats: Policy Loss: 1.1379, Value Loss: 0.1054, Total Loss: 1.2433, Value Mean: -0.0197, Value Std: 0.8323
[16.31.35] [INFO] Trainer finished at iteration 33.
[16.31.35] [INFO] All processes started at iteration 34.
[16.31.35] [INFO] Model and optimizer loaded from iteration 34
[16.31.58] [INFO] Evaluation results at iteration 33:
[16.31.58] [INFO]     Policy accuracy @1: 35.59%
[16.31.58] [INFO]     Policy accuracy @5: 76.07%
[16.31.58] [INFO]     Policy accuracy @10: 88.29%
[16.31.58] [INFO]     Avg value loss: 1.0830268363157909
[16.33.57] [INFO] Results after playing two most recent models at iteration 33: Wins: 24, Losses: 10, Draws: 6
[16.36.02] [INFO] Results after playing the current vs the reference at iteration 33: Wins: 12, Losses: 20, Draws: 8
[16.36.28] [INFO] Results after playing vs random at iteration 33: Wins: 39, Losses: 0, Draws: 1
[16.38.45] [INFO] Loading memories for iteration 34 with window size 3 (31-34)
[16.38.55] [INFO] Loaded 2309480 samples from 18560 games
Training batches: 100%|█████████▉| 1124/1127 [01:19<00:00, 14.17it/s]
[16.40.14] [INFO] Last gradient norm: 0.703125
[16.40.14] [INFO] Training stats: Policy Loss: 1.1984, Value Loss: 0.0816, Total Loss: 1.2800, Value Mean: 0.0048, Value Std: 0.8160
Validation batches: 100%|██████████| 55/55 [00:15<00:00,  3.44it/s]
[16.40.30] [INFO] Validation stats: Policy Loss: 1.1568, Value Loss: 0.0676, Total Loss: 1.2247, Value Mean: 0.0351, Value Std: 0.8276
[16.40.32] [INFO] Trainer finished at iteration 34.
[16.40.33] [INFO] All processes started at iteration 35.
[16.40.33] [INFO] Model and optimizer loaded from iteration 35
[16.40.56] [INFO] Evaluation results at iteration 34:
[16.40.56] [INFO]     Policy accuracy @1: 35.75%
[16.40.56] [INFO]     Policy accuracy @5: 76.63%
[16.40.56] [INFO]     Policy accuracy @10: 88.18%
[16.40.56] [INFO]     Avg value loss: 1.3839568297068279
[16.42.56] [INFO] Results after playing two most recent models at iteration 34: Wins: 17, Losses: 17, Draws: 6
[16.44.41] [INFO] Results after playing the current vs the reference at iteration 34: Wins: 11, Losses: 20, Draws: 9
[16.45.05] [INFO] Results after playing vs random at iteration 34: Wins: 40, Losses: 0, Draws: 0
[16.47.33] [INFO] Loading memories for iteration 35 with window size 3 (32-35)
[16.47.42] [INFO] Loaded 2294975 samples from 18420 games
Training batches: 100%|█████████▉| 1119/1120 [01:12<00:00, 15.40it/s]
[16.48.55] [INFO] Last gradient norm: 0.75390625
[16.48.55] [INFO] Training stats: Policy Loss: 1.2013, Value Loss: 0.0827, Total Loss: 1.2841, Value Mean: 0.0044, Value Std: 0.8170
Validation batches: 100%|██████████| 56/56 [00:14<00:00,  3.81it/s]
[16.49.10] [INFO] Validation stats: Policy Loss: 1.1638, Value Loss: 0.0604, Total Loss: 1.2241, Value Mean: 0.0261, Value Std: 0.8304
[16.49.12] [INFO] Trainer finished at iteration 35.
[16.49.12] [INFO] All processes started at iteration 36.
[16.49.12] [INFO] Model and optimizer loaded from iteration 36
[16.49.38] [INFO] Evaluation results at iteration 35:
[16.49.38] [INFO]     Policy accuracy @1: 35.17%
[16.49.38] [INFO]     Policy accuracy @5: 76.78%
[16.49.38] [INFO]     Policy accuracy @10: 88.97%
[16.49.38] [INFO]     Avg value loss: 1.3020210226376852
[16.51.31] [INFO] Results after playing two most recent models at iteration 35: Wins: 17, Losses: 17, Draws: 6
[16.53.18] [INFO] Results after playing the current vs the reference at iteration 35: Wins: 14, Losses: 20, Draws: 6
[16.53.44] [INFO] Results after playing vs random at iteration 35: Wins: 40, Losses: 0, Draws: 0
[16.56.32] [INFO] Loading memories for iteration 36 with window size 3 (33-36)
[16.56.42] [INFO] Loaded 2320925 samples from 18640 games
Training batches: 100%|█████████▉| 1130/1133 [01:15<00:00, 14.95it/s]
[16.57.57] [INFO] Last gradient norm: 0.71875
[16.57.57] [INFO] Training stats: Policy Loss: 1.1944, Value Loss: 0.0794, Total Loss: 1.2739, Value Mean: 0.0044, Value Std: 0.8170
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.49it/s]
[16.58.13] [INFO] Validation stats: Policy Loss: 1.1596, Value Loss: 0.0619, Total Loss: 1.2221, Value Mean: 0.0465, Value Std: 0.8068
[16.58.15] [INFO] Trainer finished at iteration 36.
[16.58.15] [INFO] All processes started at iteration 37.
[16.58.15] [INFO] Model and optimizer loaded from iteration 37
[16.58.41] [INFO] Evaluation results at iteration 36:
[16.58.41] [INFO]     Policy accuracy @1: 35.11%
[16.58.41] [INFO]     Policy accuracy @5: 75.57%
[16.58.41] [INFO]     Policy accuracy @10: 87.65%
[16.58.41] [INFO]     Avg value loss: 1.141348659992218
[17.00.40] [INFO] Results after playing two most recent models at iteration 36: Wins: 16, Losses: 21, Draws: 3
[17.02.29] [INFO] Results after playing the current vs the reference at iteration 36: Wins: 10, Losses: 20, Draws: 10
[17.02.54] [INFO] Results after playing vs random at iteration 36: Wins: 40, Losses: 0, Draws: 0
[17.05.25] [INFO] Loading memories for iteration 37 with window size 3 (34-37)
[17.05.35] [INFO] Loaded 2316010 samples from 18580 games
Training batches: 100%|█████████▉| 1128/1130 [01:11<00:00, 15.87it/s]
[17.06.46] [INFO] Last gradient norm: 0.82421875
[17.06.46] [INFO] Training stats: Policy Loss: 1.2032, Value Loss: 0.0840, Total Loss: 1.2871, Value Mean: 0.0047, Value Std: 0.8138
Validation batches: 100%|██████████| 56/56 [00:14<00:00,  3.74it/s]
[17.07.01] [INFO] Validation stats: Policy Loss: 1.1546, Value Loss: 0.0657, Total Loss: 1.2203, Value Mean: -0.0340, Value Std: 0.8216
[17.07.03] [INFO] Trainer finished at iteration 37.
[17.07.03] [INFO] All processes started at iteration 38.
[17.07.03] [INFO] Model and optimizer loaded from iteration 38
[17.07.26] [INFO] Evaluation results at iteration 37:
[17.07.26] [INFO]     Policy accuracy @1: 35.40%
[17.07.26] [INFO]     Policy accuracy @5: 75.94%
[17.07.26] [INFO]     Policy accuracy @10: 87.78%
[17.07.26] [INFO]     Avg value loss: 1.138677179813385
[17.09.22] [INFO] Results after playing two most recent models at iteration 37: Wins: 14, Losses: 16, Draws: 10
[17.11.07] [INFO] Results after playing the current vs the reference at iteration 37: Wins: 7, Losses: 29, Draws: 4
[17.11.31] [INFO] Results after playing vs random at iteration 37: Wins: 40, Losses: 0, Draws: 0
[17.14.13] [INFO] Loading memories for iteration 38 with window size 3 (35-38)
[17.14.22] [INFO] Loaded 2327520 samples from 18660 games
Training batches: 100%|█████████▉| 1134/1136 [01:15<00:00, 15.05it/s]
[17.15.38] [INFO] Last gradient norm: 0.7734375
[17.15.38] [INFO] Training stats: Policy Loss: 1.1912, Value Loss: 0.0806, Total Loss: 1.2718, Value Mean: 0.0051, Value Std: 0.8158
Validation batches: 100%|██████████| 57/57 [00:15<00:00,  3.65it/s]
[17.15.53] [INFO] Validation stats: Policy Loss: 1.1446, Value Loss: 0.0798, Total Loss: 1.2248, Value Mean: 0.0737, Value Std: 0.8368
[17.15.56] [INFO] Trainer finished at iteration 38.
[17.15.56] [INFO] All processes started at iteration 39.
[17.15.56] [INFO] Model and optimizer loaded from iteration 39
[17.16.19] [INFO] Evaluation results at iteration 38:
[17.16.19] [INFO]     Policy accuracy @1: 35.03%
[17.16.19] [INFO]     Policy accuracy @5: 74.83%
[17.16.19] [INFO]     Policy accuracy @10: 87.23%
[17.16.19] [INFO]     Avg value loss: 1.250489850838979
[17.18.06] [INFO] Results after playing two most recent models at iteration 38: Wins: 17, Losses: 17, Draws: 6
[17.19.44] [INFO] Results after playing the current vs the reference at iteration 38: Wins: 15, Losses: 22, Draws: 3
[17.20.12] [INFO] Results after playing vs random at iteration 38: Wins: 39, Losses: 0, Draws: 1
[17.22.55] [INFO] Loading memories for iteration 39 with window size 3 (36-39)
[17.23.04] [INFO] Loaded 2345050 samples from 18800 games
Training batches: 100%|█████████▉| 1143/1145 [01:16<00:00, 14.89it/s]
[17.24.21] [INFO] Last gradient norm: 0.83984375
[17.24.21] [INFO] Training stats: Policy Loss: 1.1927, Value Loss: 0.0804, Total Loss: 1.2731, Value Mean: 0.0058, Value Std: 0.8144
Validation batches: 100%|██████████| 58/58 [00:15<00:00,  3.76it/s]
[17.24.37] [INFO] Validation stats: Policy Loss: 1.1592, Value Loss: 0.0961, Total Loss: 1.2551, Value Mean: 0.0914, Value Std: 0.8308
[17.24.39] [INFO] Trainer finished at iteration 39.
[17.24.39] [INFO] All processes started at iteration 40.
[17.24.39] [INFO] Model and optimizer loaded from iteration 40
[17.25.02] [INFO] Evaluation results at iteration 39:
[17.25.02] [INFO]     Policy accuracy @1: 35.11%
[17.25.02] [INFO]     Policy accuracy @5: 74.56%
[17.25.02] [INFO]     Policy accuracy @10: 87.02%
[17.25.02] [INFO]     Avg value loss: 1.3411470532417298
[17.26.50] [INFO] Results after playing two most recent models at iteration 39: Wins: 16, Losses: 16, Draws: 8
[17.28.35] [INFO] Results after playing the current vs the reference at iteration 39: Wins: 4, Losses: 28, Draws: 8
[17.29.03] [INFO] Results after playing vs random at iteration 39: Wins: 39, Losses: 0, Draws: 1
[17.31.48] [INFO] Loading memories for iteration 40 with window size 3 (37-40)
[17.31.58] [INFO] Loaded 2341485 samples from 18780 games
Training batches: 100%|█████████▉| 1141/1143 [01:19<00:00, 14.28it/s]
[17.33.18] [INFO] Last gradient norm: 0.80078125
[17.33.18] [INFO] Training stats: Policy Loss: 1.1784, Value Loss: 0.0782, Total Loss: 1.2565, Value Mean: 0.0057, Value Std: 0.8187
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.47it/s]
[17.33.34] [INFO] Validation stats: Policy Loss: 1.1385, Value Loss: 0.0562, Total Loss: 1.1948, Value Mean: 0.0152, Value Std: 0.8477
[17.33.36] [INFO] Trainer finished at iteration 40.
[17.33.36] [INFO] All processes started at iteration 41.
[17.33.37] [INFO] Model and optimizer loaded from iteration 41
[17.34.01] [INFO] Evaluation results at iteration 40:
[17.34.01] [INFO]     Policy accuracy @1: 34.16%
[17.34.01] [INFO]     Policy accuracy @5: 74.83%
[17.34.01] [INFO]     Policy accuracy @10: 86.91%
[17.34.01] [INFO]     Avg value loss: 1.3390438874562582
[17.36.11] [INFO] Results after playing two most recent models at iteration 40: Wins: 17, Losses: 14, Draws: 9
[17.37.50] [INFO] Results after playing the current vs the reference at iteration 40: Wins: 10, Losses: 26, Draws: 4
[17.38.14] [INFO] Results after playing vs random at iteration 40: Wins: 40, Losses: 0, Draws: 0
[17.40.48] [INFO] Loading memories for iteration 41 with window size 3 (38-41)
[17.40.57] [INFO] Loaded 2329605 samples from 18700 games
Training batches: 100%|█████████▉| 1135/1137 [01:20<00:00, 14.08it/s]
[17.42.18] [INFO] Last gradient norm: 0.7421875
[17.42.18] [INFO] Training stats: Policy Loss: 1.1736, Value Loss: 0.0773, Total Loss: 1.2509, Value Mean: 0.0054, Value Std: 0.8206
Validation batches: 100%|██████████| 55/55 [00:16<00:00,  3.30it/s]
[17.42.35] [INFO] Validation stats: Policy Loss: 1.1290, Value Loss: 0.0611, Total Loss: 1.1902, Value Mean: 0.0340, Value Std: 0.8579
[17.42.37] [INFO] Trainer finished at iteration 41.
[17.42.37] [INFO] All processes started at iteration 42.
[17.42.37] [INFO] Model and optimizer loaded from iteration 42
[17.43.00] [INFO] Evaluation results at iteration 41:
[17.43.00] [INFO]     Policy accuracy @1: 35.17%
[17.43.00] [INFO]     Policy accuracy @5: 74.35%
[17.43.00] [INFO]     Policy accuracy @10: 87.26%
[17.43.00] [INFO]     Avg value loss: 1.1278700649738311
[17.44.42] [INFO] Results after playing two most recent models at iteration 41: Wins: 14, Losses: 21, Draws: 5
[17.46.30] [INFO] Results after playing the current vs the reference at iteration 41: Wins: 11, Losses: 25, Draws: 4
[17.46.55] [INFO] Results after playing vs random at iteration 41: Wins: 39, Losses: 0, Draws: 1
[17.49.36] [INFO] Loading memories for iteration 42 with window size 3 (39-42)
[17.49.46] [INFO] Loaded 2304790 samples from 18500 games
Training batches: 100%|█████████▉| 1123/1125 [01:22<00:00, 13.55it/s]
[17.51.09] [INFO] Last gradient norm: 0.69921875
[17.51.09] [INFO] Training stats: Policy Loss: 1.1698, Value Loss: 0.0783, Total Loss: 1.2481, Value Mean: 0.0047, Value Std: 0.8189
Validation batches: 100%|██████████| 54/54 [00:15<00:00,  3.50it/s]
[17.51.25] [INFO] Validation stats: Policy Loss: 1.1155, Value Loss: 0.1208, Total Loss: 1.2363, Value Mean: 0.1337, Value Std: 0.8213
[17.51.27] [INFO] Trainer finished at iteration 42.
[17.51.27] [INFO] All processes started at iteration 43.
[17.51.27] [INFO] Model and optimizer loaded from iteration 43
[17.51.51] [INFO] Evaluation results at iteration 42:
[17.51.51] [INFO]     Policy accuracy @1: 34.43%
[17.51.51] [INFO]     Policy accuracy @5: 74.88%
[17.51.51] [INFO]     Policy accuracy @10: 86.81%
[17.51.51] [INFO]     Avg value loss: 1.2876596252123516
[17.54.02] [INFO] Results after playing two most recent models at iteration 42: Wins: 12, Losses: 21, Draws: 7
[17.55.46] [INFO] Results after playing the current vs the reference at iteration 42: Wins: 9, Losses: 28, Draws: 3
[17.56.12] [INFO] Results after playing vs random at iteration 42: Wins: 39, Losses: 0, Draws: 1
[17.58.36] [INFO] Loading memories for iteration 43 with window size 3 (40-43)
[17.58.47] [INFO] Loaded 2295990 samples from 18420 games
Training batches: 100%|█████████▉| 1119/1121 [01:18<00:00, 14.23it/s]
[18.00.05] [INFO] Last gradient norm: 0.765625
[18.00.05] [INFO] Training stats: Policy Loss: 1.1670, Value Loss: 0.0796, Total Loss: 1.2466, Value Mean: 0.0043, Value Std: 0.8190
Validation batches: 100%|██████████| 57/57 [00:19<00:00,  2.97it/s]
[18.00.25] [INFO] Validation stats: Policy Loss: 1.1109, Value Loss: 0.0441, Total Loss: 1.1549, Value Mean: -0.0083, Value Std: 0.8621
[18.00.27] [INFO] Trainer finished at iteration 43.
[18.00.27] [INFO] All processes started at iteration 44.
[18.00.27] [INFO] Model and optimizer loaded from iteration 44
[18.00.53] [INFO] Evaluation results at iteration 43:
[18.00.53] [INFO]     Policy accuracy @1: 35.43%
[18.00.53] [INFO]     Policy accuracy @5: 75.01%
[18.00.53] [INFO]     Policy accuracy @10: 87.41%
[18.00.53] [INFO]     Avg value loss: 1.3316221356391906
[18.02.47] [INFO] Results after playing two most recent models at iteration 43: Wins: 15, Losses: 17, Draws: 8
[18.04.22] [INFO] Results after playing the current vs the reference at iteration 43: Wins: 11, Losses: 23, Draws: 6
[18.04.47] [INFO] Results after playing vs random at iteration 43: Wins: 39, Losses: 0, Draws: 1
[18.07.19] [INFO] Loading memories for iteration 44 with window size 3 (41-44)
[18.07.29] [INFO] Loaded 2295030 samples from 18420 games
Training batches: 100%|█████████▉| 1118/1120 [01:19<00:00, 14.09it/s]
[18.08.48] [INFO] Last gradient norm: 0.74609375
[18.08.48] [INFO] Training stats: Policy Loss: 1.1511, Value Loss: 0.0793, Total Loss: 1.2304, Value Mean: 0.0044, Value Std: 0.8183
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.46it/s]
[18.09.05] [INFO] Validation stats: Policy Loss: 1.0849, Value Loss: 0.0742, Total Loss: 1.1588, Value Mean: -0.0737, Value Std: 0.8154
[18.09.06] [INFO] Trainer finished at iteration 44.
[18.09.06] [INFO] All processes started at iteration 45.
[18.09.07] [INFO] Model and optimizer loaded from iteration 45
[18.09.33] [INFO] Evaluation results at iteration 44:
[18.09.33] [INFO]     Policy accuracy @1: 35.54%
[18.09.33] [INFO]     Policy accuracy @5: 74.59%
[18.09.33] [INFO]     Policy accuracy @10: 86.75%
[18.09.33] [INFO]     Avg value loss: 1.2947680234909058
[18.11.35] [INFO] Results after playing two most recent models at iteration 44: Wins: 22, Losses: 12, Draws: 6
[18.13.25] [INFO] Results after playing the current vs the reference at iteration 44: Wins: 13, Losses: 23, Draws: 4
[18.13.55] [INFO] Results after playing vs random at iteration 44: Wins: 38, Losses: 0, Draws: 2
[18.16.27] [INFO] Loading memories for iteration 45 with window size 3 (42-45)
[18.16.37] [INFO] Loaded 2319905 samples from 18620 games
Training batches: 100%|█████████▉| 1130/1132 [01:19<00:00, 14.25it/s]
[18.17.57] [INFO] Last gradient norm: 0.77734375
[18.17.57] [INFO] Training stats: Policy Loss: 1.1538, Value Loss: 0.0798, Total Loss: 1.2337, Value Mean: 0.0048, Value Std: 0.8163
Validation batches: 100%|██████████| 58/58 [00:16<00:00,  3.43it/s]
[18.18.13] [INFO] Validation stats: Policy Loss: 1.1065, Value Loss: 0.0553, Total Loss: 1.1619, Value Mean: -0.0244, Value Std: 0.8371
[18.18.15] [INFO] Trainer finished at iteration 45.
[18.18.15] [INFO] All processes started at iteration 46.
[18.18.15] [INFO] Model and optimizer loaded from iteration 46
[18.18.40] [INFO] Evaluation results at iteration 45:
[18.18.40] [INFO]     Policy accuracy @1: 34.96%
[18.18.40] [INFO]     Policy accuracy @5: 73.96%
[18.18.40] [INFO]     Policy accuracy @10: 86.33%
[18.18.40] [INFO]     Avg value loss: 1.1909233729044597
[18.20.25] [INFO] Results after playing two most recent models at iteration 45: Wins: 10, Losses: 22, Draws: 8
[18.22.13] [INFO] Results after playing the current vs the reference at iteration 45: Wins: 14, Losses: 18, Draws: 8
[18.22.39] [INFO] Results after playing vs random at iteration 45: Wins: 38, Losses: 0, Draws: 2
[18.25.35] [INFO] Loading memories for iteration 46 with window size 3 (43-46)
[18.25.46] [INFO] Loaded 2328310 samples from 18700 games
Training batches: 100%|█████████▉| 1133/1136 [01:17<00:00, 14.55it/s]
[18.27.04] [INFO] Last gradient norm: 0.74609375
[18.27.04] [INFO] Training stats: Policy Loss: 1.1590, Value Loss: 0.0809, Total Loss: 1.2399, Value Mean: 0.0050, Value Std: 0.8153
Validation batches: 100%|██████████| 55/55 [00:15<00:00,  3.61it/s]
[18.27.19] [INFO] Validation stats: Policy Loss: 1.1440, Value Loss: 0.0657, Total Loss: 1.2101, Value Mean: 0.0380, Value Std: 0.8096
[18.27.21] [INFO] Trainer finished at iteration 46.
[18.27.21] [INFO] All processes started at iteration 47.
[18.27.21] [INFO] Model and optimizer loaded from iteration 47
[18.27.46] [INFO] Evaluation results at iteration 46:
[18.27.46] [INFO]     Policy accuracy @1: 34.61%
[18.27.46] [INFO]     Policy accuracy @5: 74.14%
[18.27.46] [INFO]     Policy accuracy @10: 86.83%
[18.27.46] [INFO]     Avg value loss: 1.1342304527759552
[18.29.37] [INFO] Results after playing two most recent models at iteration 46: Wins: 20, Losses: 11, Draws: 9
[18.31.15] [INFO] Results after playing the current vs the reference at iteration 46: Wins: 11, Losses: 25, Draws: 4
[18.31.43] [INFO] Results after playing vs random at iteration 46: Wins: 39, Losses: 0, Draws: 1
[18.34.22] [INFO] Loading memories for iteration 47 with window size 3 (44-47)
[18.34.32] [INFO] Loaded 2309870 samples from 18560 games
Training batches: 100%|█████████▉| 1125/1127 [01:21<00:00, 13.79it/s]
[18.35.54] [INFO] Last gradient norm: 0.734375
[18.35.54] [INFO] Training stats: Policy Loss: 1.1678, Value Loss: 0.0788, Total Loss: 1.2465, Value Mean: 0.0050, Value Std: 0.8143
Validation batches: 100%|██████████| 55/55 [00:16<00:00,  3.27it/s]
[18.36.11] [INFO] Validation stats: Policy Loss: 1.1369, Value Loss: 0.0666, Total Loss: 1.2033, Value Mean: 0.0212, Value Std: 0.8164
[18.36.13] [INFO] Trainer finished at iteration 47.
[18.36.13] [INFO] All processes started at iteration 48.
[18.36.13] [INFO] Model and optimizer loaded from iteration 48
[18.36.37] [INFO] Evaluation results at iteration 47:
[18.36.37] [INFO]     Policy accuracy @1: 34.06%
[18.36.37] [INFO]     Policy accuracy @5: 75.36%
[18.36.37] [INFO]     Policy accuracy @10: 87.68%
[18.36.37] [INFO]     Avg value loss: 1.1553655763467152
[18.38.23] [INFO] Results after playing two most recent models at iteration 47: Wins: 19, Losses: 17, Draws: 4
[18.39.51] [INFO] Results after playing the current vs the reference at iteration 47: Wins: 10, Losses: 23, Draws: 7
[18.40.17] [INFO] Results after playing vs random at iteration 47: Wins: 40, Losses: 0, Draws: 0
[18.43.24] [INFO] Loading memories for iteration 48 with window size 3 (45-48)
[18.43.34] [INFO] Loaded 2323415 samples from 18660 games
Training batches: 100%|█████████▉| 1131/1134 [01:18<00:00, 14.44it/s]
[18.44.53] [INFO] Last gradient norm: 0.70703125
[18.44.53] [INFO] Training stats: Policy Loss: 1.1741, Value Loss: 0.0805, Total Loss: 1.2547, Value Mean: 0.0047, Value Std: 0.8132
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.42it/s]
[18.45.09] [INFO] Validation stats: Policy Loss: 1.1038, Value Loss: 0.0552, Total Loss: 1.1589, Value Mean: 0.0433, Value Std: 0.8415
[18.45.11] [INFO] Trainer finished at iteration 48.
[18.45.11] [INFO] All processes started at iteration 49.
[18.45.11] [INFO] Model and optimizer loaded from iteration 49
[18.45.36] [INFO] Evaluation results at iteration 48:
[18.45.36] [INFO]     Policy accuracy @1: 33.84%
[18.45.36] [INFO]     Policy accuracy @5: 74.22%
[18.45.36] [INFO]     Policy accuracy @10: 86.78%
[18.45.36] [INFO]     Avg value loss: 1.1196693817774455
[18.47.34] [INFO] Results after playing two most recent models at iteration 48: Wins: 14, Losses: 19, Draws: 7
[18.49.21] [INFO] Results after playing the current vs the reference at iteration 48: Wins: 4, Losses: 27, Draws: 9
[18.49.47] [INFO] Results after playing vs random at iteration 48: Wins: 40, Losses: 0, Draws: 0
[18.52.02] [INFO] Loading memories for iteration 49 with window size 3 (46-49)
[18.52.12] [INFO] Loaded 2301300 samples from 18480 games
Training batches: 100%|█████████▉| 1121/1123 [01:15<00:00, 14.85it/s]
[18.53.28] [INFO] Last gradient norm: 0.859375
[18.53.28] [INFO] Training stats: Policy Loss: 1.1664, Value Loss: 0.0806, Total Loss: 1.2471, Value Mean: 0.0050, Value Std: 0.8131
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.34it/s]
[18.53.45] [INFO] Validation stats: Policy Loss: 1.1041, Value Loss: 0.0695, Total Loss: 1.1735, Value Mean: 0.0453, Value Std: 0.8258
[18.53.46] [INFO] Trainer finished at iteration 49.
[18.53.46] [INFO] All processes started at iteration 50.
[18.53.47] [INFO] Model and optimizer loaded from iteration 50
[18.54.11] [INFO] Evaluation results at iteration 49:
[18.54.11] [INFO]     Policy accuracy @1: 34.24%
[18.54.11] [INFO]     Policy accuracy @5: 74.06%
[18.54.11] [INFO]     Policy accuracy @10: 86.96%
[18.54.11] [INFO]     Avg value loss: 1.2417286515235901
[18.56.06] [INFO] Results after playing two most recent models at iteration 49: Wins: 16, Losses: 17, Draws: 7
[18.57.46] [INFO] Results after playing the current vs the reference at iteration 49: Wins: 8, Losses: 26, Draws: 6
[18.58.10] [INFO] Results after playing vs random at iteration 49: Wins: 40, Losses: 0, Draws: 0
[19.00.57] [INFO] Loading memories for iteration 50 with window size 3 (47-50)
[19.01.06] [INFO] Loaded 2323720 samples from 18660 games
Training batches: 100%|█████████▉| 1132/1134 [01:19<00:00, 14.33it/s]
[19.02.26] [INFO] Last gradient norm: 0.796875
[19.02.26] [INFO] Training stats: Policy Loss: 1.1581, Value Loss: 0.0805, Total Loss: 1.2386, Value Mean: 0.0050, Value Std: 0.8127
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s]
[19.02.42] [INFO] Validation stats: Policy Loss: 1.1044, Value Loss: 0.1712, Total Loss: 1.2758, Value Mean: -0.0484, Value Std: 0.7953
[19.02.44] [INFO] Trainer finished at iteration 50.
[19.02.44] [INFO] All processes started at iteration 51.
[19.02.44] [INFO] Model and optimizer loaded from iteration 51
[19.03.09] [INFO] Evaluation results at iteration 50:
[19.03.09] [INFO]     Policy accuracy @1: 33.47%
[19.03.09] [INFO]     Policy accuracy @5: 74.62%
[19.03.09] [INFO]     Policy accuracy @10: 87.04%
[19.03.09] [INFO]     Avg value loss: 1.1561421891053518
[19.05.04] [INFO] Results after playing two most recent models at iteration 50: Wins: 21, Losses: 15, Draws: 4
[19.06.51] [INFO] Results after playing the current vs the reference at iteration 50: Wins: 11, Losses: 25, Draws: 4
[19.07.21] [INFO] Results after playing vs random at iteration 50: Wins: 40, Losses: 0, Draws: 0
[19.09.25] [INFO] Loading memories for iteration 51 with window size 3 (48-51)
[19.09.34] [INFO] Loaded 2309415 samples from 18540 games
Training batches: 100%|█████████▉| 1124/1127 [01:20<00:00, 13.99it/s]
[19.10.55] [INFO] Last gradient norm: 0.72265625
[19.10.55] [INFO] Training stats: Policy Loss: 1.1523, Value Loss: 0.0843, Total Loss: 1.2367, Value Mean: 0.0052, Value Std: 0.8116
Validation batches: 100%|██████████| 53/53 [00:16<00:00,  3.30it/s]
[19.11.11] [INFO] Validation stats: Policy Loss: 1.1163, Value Loss: 0.0702, Total Loss: 1.1862, Value Mean: -0.0265, Value Std: 0.8097
[19.11.13] [INFO] Trainer finished at iteration 51.
[19.11.13] [INFO] All processes started at iteration 52.
[19.11.13] [INFO] Model and optimizer loaded from iteration 52
[19.11.37] [INFO] Evaluation results at iteration 51:
[19.11.37] [INFO]     Policy accuracy @1: 33.74%
[19.11.37] [INFO]     Policy accuracy @5: 74.67%
[19.11.37] [INFO]     Policy accuracy @10: 87.02%
[19.11.37] [INFO]     Avg value loss: 0.9952717343966166
[19.13.36] [INFO] Results after playing two most recent models at iteration 51: Wins: 12, Losses: 13, Draws: 15
[19.15.10] [INFO] Results after playing the current vs the reference at iteration 51: Wins: 10, Losses: 26, Draws: 4
[19.15.34] [INFO] Results after playing vs random at iteration 51: Wins: 39, Losses: 0, Draws: 1
[19.18.24] [INFO] Loading memories for iteration 52 with window size 3 (49-52)
[19.18.33] [INFO] Loaded 2309660 samples from 18540 games
Training batches: 100%|█████████▉| 1124/1127 [01:20<00:00, 13.95it/s]
[19.19.53] [INFO] Last gradient norm: 0.72265625
[19.19.53] [INFO] Training stats: Policy Loss: 1.1517, Value Loss: 0.0798, Total Loss: 1.2313, Value Mean: 0.0056, Value Std: 0.8121
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.44it/s]
[19.20.10] [INFO] Validation stats: Policy Loss: 1.1229, Value Loss: 0.0770, Total Loss: 1.1998, Value Mean: 0.0680, Value Std: 0.8111
[19.20.12] [INFO] Trainer finished at iteration 52.
[19.20.12] [INFO] All processes started at iteration 53.
[19.20.12] [INFO] Model and optimizer loaded from iteration 53
[19.20.36] [INFO] Evaluation results at iteration 52:
[19.20.36] [INFO]     Policy accuracy @1: 33.69%
[19.20.36] [INFO]     Policy accuracy @5: 73.90%
[19.20.36] [INFO]     Policy accuracy @10: 85.85%
[19.20.36] [INFO]     Avg value loss: 1.0807419002056122
[19.22.32] [INFO] Results after playing two most recent models at iteration 52: Wins: 19, Losses: 14, Draws: 7
[19.24.17] [INFO] Results after playing the current vs the reference at iteration 52: Wins: 9, Losses: 23, Draws: 8
[19.24.43] [INFO] Results after playing vs random at iteration 52: Wins: 40, Losses: 0, Draws: 0
[19.27.03] [INFO] Loading memories for iteration 53 with window size 3 (50-53)
[19.27.13] [INFO] Loaded 2318515 samples from 18600 games
Training batches: 100%|█████████▉| 1129/1132 [01:18<00:00, 14.37it/s]
[19.28.32] [INFO] Last gradient norm: 0.74609375
[19.28.32] [INFO] Training stats: Policy Loss: 1.1632, Value Loss: 0.0792, Total Loss: 1.2424, Value Mean: 0.0053, Value Std: 0.8119
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.45it/s]
[19.28.48] [INFO] Validation stats: Policy Loss: 1.1251, Value Loss: 0.0563, Total Loss: 1.1818, Value Mean: 0.0431, Value Std: 0.8142
[19.28.50] [INFO] Trainer finished at iteration 53.
[19.28.50] [INFO] All processes started at iteration 54.
[19.28.50] [INFO] Model and optimizer loaded from iteration 54
[19.29.14] [INFO] Evaluation results at iteration 53:
[19.29.14] [INFO]     Policy accuracy @1: 33.50%
[19.29.14] [INFO]     Policy accuracy @5: 73.90%
[19.29.14] [INFO]     Policy accuracy @10: 86.73%
[19.29.14] [INFO]     Avg value loss: 0.9677967011928559
[19.31.15] [INFO] Results after playing two most recent models at iteration 53: Wins: 19, Losses: 14, Draws: 7
[19.33.02] [INFO] Results after playing the current vs the reference at iteration 53: Wins: 17, Losses: 17, Draws: 6
[19.33.30] [INFO] Results after playing vs random at iteration 53: Wins: 39, Losses: 0, Draws: 1
[19.35.52] [INFO] Loading memories for iteration 54 with window size 3 (51-54)
[19.36.01] [INFO] Loaded 2303720 samples from 18480 games
Training batches: 100%|█████████▉| 1121/1124 [01:18<00:00, 14.36it/s]
[19.37.19] [INFO] Last gradient norm: 0.74609375
[19.37.19] [INFO] Training stats: Policy Loss: 1.1583, Value Loss: 0.0793, Total Loss: 1.2376, Value Mean: 0.0052, Value Std: 0.8110
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.35it/s]
[19.37.36] [INFO] Validation stats: Policy Loss: 1.0985, Value Loss: 0.0655, Total Loss: 1.1641, Value Mean: 0.0454, Value Std: 0.8355
[19.37.38] [INFO] Trainer finished at iteration 54.
[19.37.38] [INFO] All processes started at iteration 55.
[19.37.38] [INFO] Model and optimizer loaded from iteration 55
[19.38.02] [INFO] Evaluation results at iteration 54:
[19.38.02] [INFO]     Policy accuracy @1: 34.72%
[19.38.02] [INFO]     Policy accuracy @5: 73.32%
[19.38.02] [INFO]     Policy accuracy @10: 86.28%
[19.38.02] [INFO]     Avg value loss: 1.0375537912050883
[19.39.59] [INFO] Results after playing two most recent models at iteration 54: Wins: 12, Losses: 19, Draws: 9
[19.41.41] [INFO] Results after playing the current vs the reference at iteration 54: Wins: 10, Losses: 26, Draws: 4
[19.42.07] [INFO] Results after playing vs random at iteration 54: Wins: 39, Losses: 0, Draws: 1
[19.44.59] [INFO] Loading memories for iteration 55 with window size 3 (52-55)
[19.45.09] [INFO] Loaded 2324415 samples from 18660 games
Training batches: 100%|█████████▉| 1131/1134 [01:19<00:00, 14.19it/s]
[19.46.29] [INFO] Last gradient norm: 0.73046875
[19.46.29] [INFO] Training stats: Policy Loss: 1.1503, Value Loss: 0.0780, Total Loss: 1.2283, Value Mean: 0.0048, Value Std: 0.8081
Validation batches: 100%|██████████| 55/55 [00:15<00:00,  3.48it/s]
[19.46.45] [INFO] Validation stats: Policy Loss: 1.0935, Value Loss: 0.0593, Total Loss: 1.1527, Value Mean: 0.0000, Value Std: 0.8241
[19.46.47] [INFO] Trainer finished at iteration 55.
[19.46.47] [INFO] All processes started at iteration 56.
[19.46.47] [INFO] Model and optimizer loaded from iteration 56
[19.47.11] [INFO] Evaluation results at iteration 55:
[19.47.11] [INFO]     Policy accuracy @1: 34.21%
[19.47.11] [INFO]     Policy accuracy @5: 73.88%
[19.47.11] [INFO]     Policy accuracy @10: 86.62%
[19.47.11] [INFO]     Avg value loss: 1.0368490596612294
[19.49.23] [INFO] Results after playing two most recent models at iteration 55: Wins: 14, Losses: 17, Draws: 9
[19.51.04] [INFO] Results after playing the current vs the reference at iteration 55: Wins: 6, Losses: 30, Draws: 4
[19.51.32] [INFO] Results after playing vs random at iteration 55: Wins: 39, Losses: 0, Draws: 1
[19.53.58] [INFO] Loading memories for iteration 56 with window size 3 (53-56)
[19.54.08] [INFO] Loaded 2300695 samples from 18480 games
Training batches: 100%|█████████▉| 1120/1123 [01:15<00:00, 14.79it/s]
[19.55.24] [INFO] Last gradient norm: 0.71484375
[19.55.24] [INFO] Training stats: Policy Loss: 1.1532, Value Loss: 0.0793, Total Loss: 1.2325, Value Mean: 0.0050, Value Std: 0.8091
Validation batches: 100%|██████████| 55/55 [00:15<00:00,  3.54it/s]
[19.55.40] [INFO] Validation stats: Policy Loss: 1.1229, Value Loss: 0.0627, Total Loss: 1.1857, Value Mean: 0.0066, Value Std: 0.8550
[19.55.41] [INFO] Trainer finished at iteration 56.
[19.55.41] [INFO] All processes started at iteration 57.
[19.55.42] [INFO] Model and optimizer loaded from iteration 57
[19.56.05] [INFO] Evaluation results at iteration 56:
[19.56.05] [INFO]     Policy accuracy @1: 34.29%
[19.56.05] [INFO]     Policy accuracy @5: 73.56%
[19.56.05] [INFO]     Policy accuracy @10: 85.96%
[19.56.05] [INFO]     Avg value loss: 1.0478001415729523
[19.58.01] [INFO] Results after playing two most recent models at iteration 56: Wins: 13, Losses: 21, Draws: 6
[19.59.41] [INFO] Results after playing the current vs the reference at iteration 56: Wins: 9, Losses: 29, Draws: 2
[20.00.08] [INFO] Results after playing vs random at iteration 56: Wins: 40, Losses: 0, Draws: 0
[20.03.02] [INFO] Loading memories for iteration 57 with window size 3 (54-57)
[20.03.12] [INFO] Loaded 2308000 samples from 18540 games
Training batches: 100%|█████████▉| 1124/1126 [01:19<00:00, 14.17it/s]
[20.04.32] [INFO] Last gradient norm: 0.80859375
[20.04.32] [INFO] Training stats: Policy Loss: 1.1533, Value Loss: 0.0821, Total Loss: 1.2355, Value Mean: 0.0055, Value Std: 0.8055
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.44it/s]
[20.04.48] [INFO] Validation stats: Policy Loss: 1.1198, Value Loss: 0.0838, Total Loss: 1.2038, Value Mean: 0.0048, Value Std: 0.8110
[20.04.50] [INFO] Trainer finished at iteration 57.
[20.04.50] [INFO] All processes started at iteration 58.
[20.04.50] [INFO] Model and optimizer loaded from iteration 58
[20.05.13] [INFO] Evaluation results at iteration 57:
[20.05.13] [INFO]     Policy accuracy @1: 33.58%
[20.05.13] [INFO]     Policy accuracy @5: 74.03%
[20.05.13] [INFO]     Policy accuracy @10: 86.17%
[20.05.13] [INFO]     Avg value loss: 1.1316293517748515
[20.06.59] [INFO] Results after playing two most recent models at iteration 57: Wins: 17, Losses: 12, Draws: 11
[20.08.41] [INFO] Results after playing the current vs the reference at iteration 57: Wins: 10, Losses: 26, Draws: 4
[20.09.06] [INFO] Results after playing vs random at iteration 57: Wins: 39, Losses: 0, Draws: 1
[20.11.40] [INFO] Loading memories for iteration 58 with window size 3 (55-58)
[20.11.49] [INFO] Loaded 2294680 samples from 18440 games
Training batches: 100%|█████████▉| 1118/1120 [01:15<00:00, 14.78it/s]
[20.13.05] [INFO] Last gradient norm: 0.7578125
[20.13.05] [INFO] Training stats: Policy Loss: 1.1638, Value Loss: 0.0803, Total Loss: 1.2441, Value Mean: 0.0053, Value Std: 0.8090
Validation batches: 100%|██████████| 55/55 [00:15<00:00,  3.49it/s]
[20.13.21] [INFO] Validation stats: Policy Loss: 1.1317, Value Loss: 0.0872, Total Loss: 1.2192, Value Mean: -0.0821, Value Std: 0.8154
[20.13.23] [INFO] Trainer finished at iteration 58.
[20.13.23] [INFO] All processes started at iteration 59.
[20.13.23] [INFO] Model and optimizer loaded from iteration 59
[20.13.48] [INFO] Evaluation results at iteration 58:
[20.13.48] [INFO]     Policy accuracy @1: 33.58%
[20.13.48] [INFO]     Policy accuracy @5: 73.53%
[20.13.48] [INFO]     Policy accuracy @10: 86.28%
[20.13.48] [INFO]     Avg value loss: 0.987442837158839
[20.15.40] [INFO] Results after playing two most recent models at iteration 58: Wins: 14, Losses: 17, Draws: 9
[20.16.55] [INFO] Results after playing the current vs the reference at iteration 58: Wins: 7, Losses: 32, Draws: 1
[20.17.22] [INFO] Results after playing vs random at iteration 58: Wins: 39, Losses: 0, Draws: 1
[20.20.43] [INFO] Loading memories for iteration 59 with window size 3 (56-59)
[20.20.53] [INFO] Loaded 2299930 samples from 18480 games
Training batches: 100%|█████████▉| 1120/1123 [01:18<00:00, 14.22it/s]
[20.22.12] [INFO] Last gradient norm: 0.73828125
[20.22.12] [INFO] Training stats: Policy Loss: 1.1639, Value Loss: 0.0800, Total Loss: 1.2439, Value Mean: 0.0057, Value Std: 0.8103
Validation batches: 100%|██████████| 56/56 [00:15<00:00,  3.56it/s]
[20.22.28] [INFO] Validation stats: Policy Loss: 1.1339, Value Loss: 0.1006, Total Loss: 1.2345, Value Mean: -0.0359, Value Std: 0.8204
[20.22.30] [INFO] Trainer finished at iteration 59.
[20.22.30] [INFO] All processes started at iteration 60.
[20.22.31] [INFO] Model and optimizer loaded from iteration 60
[20.22.54] [INFO] Evaluation results at iteration 59:
[20.22.54] [INFO]     Policy accuracy @1: 34.51%
[20.22.54] [INFO]     Policy accuracy @5: 73.08%
[20.22.54] [INFO]     Policy accuracy @10: 85.85%
[20.22.54] [INFO]     Avg value loss: 1.1779605984687804
[20.24.40] [INFO] Results after playing two most recent models at iteration 59: Wins: 19, Losses: 12, Draws: 9
[20.26.29] [INFO] Results after playing the current vs the reference at iteration 59: Wins: 11, Losses: 27, Draws: 2
[20.26.56] [INFO] Results after playing vs random at iteration 59: Wins: 36, Losses: 0, Draws: 4
[20.29.42] [INFO] Loading memories for iteration 60 with window size 3 (57-60)
[20.29.51] [INFO] Loaded 2309130 samples from 18560 games
Training batches: 100%|█████████▉| 1124/1127 [01:17<00:00, 14.48it/s]
[20.31.10] [INFO] Last gradient norm: 0.7265625
[20.31.10] [INFO] Training stats: Policy Loss: 1.1735, Value Loss: 0.0839, Total Loss: 1.2573, Value Mean: 0.0055, Value Std: 0.8061
Validation batches: 100%|██████████| 56/56 [00:15<00:00,  3.57it/s]
[20.31.25] [INFO] Validation stats: Policy Loss: 1.1327, Value Loss: 0.0551, Total Loss: 1.1874, Value Mean: -0.0116, Value Std: 0.8198
[20.31.27] [INFO] Trainer finished at iteration 60.
[20.31.27] [INFO] All processes started at iteration 61.
[20.31.27] [INFO] Model and optimizer loaded from iteration 61
[20.31.53] [INFO] Evaluation results at iteration 60:
[20.31.53] [INFO]     Policy accuracy @1: 34.08%
[20.31.53] [INFO]     Policy accuracy @5: 73.59%
[20.31.53] [INFO]     Policy accuracy @10: 86.44%
[20.31.53] [INFO]     Avg value loss: 0.9352491597334543
[20.33.53] [INFO] Results after playing two most recent models at iteration 60: Wins: 18, Losses: 14, Draws: 8
[20.35.42] [INFO] Results after playing the current vs the reference at iteration 60: Wins: 12, Losses: 23, Draws: 5
[20.36.09] [INFO] Results after playing vs random at iteration 60: Wins: 40, Losses: 0, Draws: 0
[20.38.58] [INFO] Loading memories for iteration 61 with window size 3 (58-61)
[20.39.08] [INFO] Loaded 2315225 samples from 18600 games
Training batches: 100%|█████████▉| 1127/1130 [01:17<00:00, 14.57it/s]
[20.40.26] [INFO] Last gradient norm: 0.78125
[20.40.26] [INFO] Training stats: Policy Loss: 1.1713, Value Loss: 0.0786, Total Loss: 1.2499, Value Mean: 0.0054, Value Std: 0.8084
Validation batches: 100%|██████████| 57/57 [00:14<00:00,  3.87it/s]
[20.40.40] [INFO] Validation stats: Policy Loss: 1.1276, Value Loss: 0.0624, Total Loss: 1.1901, Value Mean: -0.0746, Value Std: 0.8329
[20.40.42] [INFO] Trainer finished at iteration 61.
[20.40.42] [INFO] All processes started at iteration 62.
[20.40.42] [INFO] Model and optimizer loaded from iteration 62
[20.41.06] [INFO] Evaluation results at iteration 61:
[20.41.06] [INFO]     Policy accuracy @1: 34.03%
[20.41.06] [INFO]     Policy accuracy @5: 73.80%
[20.41.06] [INFO]     Policy accuracy @10: 86.73%
[20.41.06] [INFO]     Avg value loss: 1.029503490527471
[20.42.55] [INFO] Results after playing two most recent models at iteration 61: Wins: 21, Losses: 16, Draws: 3
[20.44.57] [INFO] Results after playing the current vs the reference at iteration 61: Wins: 10, Losses: 25, Draws: 5
[20.45.25] [INFO] Results after playing vs random at iteration 61: Wins: 40, Losses: 0, Draws: 0
[20.47.53] [INFO] Loading memories for iteration 62 with window size 3 (59-62)
[20.48.02] [INFO] Loaded 2330075 samples from 18720 games
Training batches: 100%|█████████▉| 1134/1137 [01:15<00:00, 14.94it/s]
[20.49.19] [INFO] Last gradient norm: 0.77734375
[20.49.19] [INFO] Training stats: Policy Loss: 1.1730, Value Loss: 0.0785, Total Loss: 1.2515, Value Mean: 0.0044, Value Std: 0.8080
Validation batches: 100%|██████████| 56/56 [00:16<00:00,  3.38it/s]
[20.49.35] [INFO] Validation stats: Policy Loss: 1.1422, Value Loss: 0.0547, Total Loss: 1.1971, Value Mean: 0.0552, Value Std: 0.8254
[20.49.37] [INFO] Trainer finished at iteration 62.
[20.49.37] [INFO] All processes started at iteration 63.
[20.49.38] [INFO] Model and optimizer loaded from iteration 63
[20.50.03] [INFO] Evaluation results at iteration 62:
[20.50.03] [INFO]     Policy accuracy @1: 34.35%
[20.50.03] [INFO]     Policy accuracy @5: 73.69%
[20.50.03] [INFO]     Policy accuracy @10: 86.36%
[20.50.03] [INFO]     Avg value loss: 1.1307217975457509
[20.52.00] [INFO] Results after playing two most recent models at iteration 62: Wins: 22, Losses: 13, Draws: 5
[20.53.53] [INFO] Results after playing the current vs the reference at iteration 62: Wins: 10, Losses: 20, Draws: 10
[20.54.20] [INFO] Results after playing vs random at iteration 62: Wins: 40, Losses: 0, Draws: 0
[20.57.20] [INFO] Loading memories for iteration 63 with window size 3 (60-63)
[20.57.29] [INFO] Loaded 2339485 samples from 18800 games
Training batches: 100%|█████████▉| 1140/1142 [01:18<00:00, 14.61it/s]
[20.58.48] [INFO] Last gradient norm: 0.69140625
[20.58.48] [INFO] Training stats: Policy Loss: 1.1832, Value Loss: 0.0772, Total Loss: 1.2606, Value Mean: 0.0044, Value Std: 0.8145
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.49it/s]
[20.59.04] [INFO] Validation stats: Policy Loss: 1.1525, Value Loss: 0.0508, Total Loss: 1.2034, Value Mean: -0.0360, Value Std: 0.8580
[20.59.06] [INFO] Trainer finished at iteration 63.
[20.59.06] [INFO] All processes started at iteration 64.
[20.59.06] [INFO] Model and optimizer loaded from iteration 64
[20.59.30] [INFO] Evaluation results at iteration 63:
[20.59.30] [INFO]     Policy accuracy @1: 33.66%
[20.59.30] [INFO]     Policy accuracy @5: 73.72%
[20.59.30] [INFO]     Policy accuracy @10: 86.28%
[20.59.30] [INFO]     Avg value loss: 1.0410947501659393
[21.01.25] [INFO] Results after playing two most recent models at iteration 63: Wins: 22, Losses: 11, Draws: 7
[21.03.12] [INFO] Results after playing the current vs the reference at iteration 63: Wins: 8, Losses: 27, Draws: 5
[21.03.43] [INFO] Results after playing vs random at iteration 63: Wins: 38, Losses: 0, Draws: 2
[21.06.18] [INFO] Loading memories for iteration 64 with window size 3 (61-64)
[21.06.28] [INFO] Loaded 2322320 samples from 18660 games
Training batches: 100%|█████████▉| 1132/1133 [01:16<00:00, 14.84it/s]
[21.07.44] [INFO] Last gradient norm: 0.7109375
[21.07.44] [INFO] Training stats: Policy Loss: 1.1873, Value Loss: 0.0754, Total Loss: 1.2627, Value Mean: 0.0043, Value Std: 0.8166
Validation batches: 100%|██████████| 54/54 [00:16<00:00,  3.19it/s]
[21.08.01] [INFO] Validation stats: Policy Loss: 1.1444, Value Loss: 0.0543, Total Loss: 1.1988, Value Mean: 0.0237, Value Std: 0.8262
[21.08.03] [INFO] Trainer finished at iteration 64.
[21.08.03] [INFO] All processes started at iteration 65.
[21.08.03] [INFO] Model and optimizer loaded from iteration 65
[21.08.27] [INFO] Evaluation results at iteration 64:
[21.08.27] [INFO]     Policy accuracy @1: 34.06%
[21.08.27] [INFO]     Policy accuracy @5: 72.98%
[21.08.27] [INFO]     Policy accuracy @10: 86.20%
[21.08.27] [INFO]     Avg value loss: 1.060919322570165
[21.10.34] [INFO] Results after playing two most recent models at iteration 64: Wins: 21, Losses: 10, Draws: 9
[21.12.26] [INFO] Results after playing the current vs the reference at iteration 64: Wins: 8, Losses: 27, Draws: 5
[21.12.54] [INFO] Results after playing vs random at iteration 64: Wins: 38, Losses: 0, Draws: 2
[21.15.44] [INFO] Loading memories for iteration 65 with window size 3 (62-65)
[21.15.54] [INFO] Loaded 2348875 samples from 18860 games
Training batches: 100%|█████████▉| 1144/1146 [01:18<00:00, 14.54it/s]
[21.17.13] [INFO] Last gradient norm: 0.76171875
[21.17.13] [INFO] Training stats: Policy Loss: 1.1960, Value Loss: 0.0755, Total Loss: 1.2716, Value Mean: 0.0038, Value Std: 0.8099
Validation batches: 100%|██████████| 60/60 [00:15<00:00,  3.85it/s]
[21.17.29] [INFO] Validation stats: Policy Loss: 1.1352, Value Loss: 0.0580, Total Loss: 1.1934, Value Mean: -0.0577, Value Std: 0.8163
[21.17.31] [INFO] Trainer finished at iteration 65.
[21.17.31] [INFO] All processes started at iteration 66.
[21.17.31] [INFO] Model and optimizer loaded from iteration 66
[21.17.58] [INFO] Evaluation results at iteration 65:
[21.17.58] [INFO]     Policy accuracy @1: 33.58%
[21.17.58] [INFO]     Policy accuracy @5: 73.90%
[21.17.58] [INFO]     Policy accuracy @10: 86.99%
[21.17.58] [INFO]     Avg value loss: 0.965385514497757
[21.20.05] [INFO] Results after playing two most recent models at iteration 65: Wins: 14, Losses: 20, Draws: 6
[21.22.17] [INFO] Results after playing the current vs the reference at iteration 65: Wins: 5, Losses: 27, Draws: 8
[21.22.47] [INFO] Results after playing vs random at iteration 65: Wins: 39, Losses: 0, Draws: 1
[21.24.52] [INFO] Loading memories for iteration 66 with window size 3 (63-66)
[21.25.03] [INFO] Loaded 2358010 samples from 18940 games
Training batches: 100%|█████████▉| 1149/1151 [01:24<00:00, 13.60it/s]
[21.26.27] [INFO] Last gradient norm: 0.7421875
[21.26.27] [INFO] Training stats: Policy Loss: 1.1973, Value Loss: 0.0780, Total Loss: 1.2753, Value Mean: 0.0038, Value Std: 0.8074
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.39it/s]
[21.26.44] [INFO] Validation stats: Policy Loss: 1.1471, Value Loss: 0.0986, Total Loss: 1.2456, Value Mean: -0.1024, Value Std: 0.8095
[21.26.46] [INFO] Trainer finished at iteration 66.
[21.26.46] [INFO] All processes started at iteration 67.
[21.26.46] [INFO] Model and optimizer loaded from iteration 67
[21.27.11] [INFO] Evaluation results at iteration 66:
[21.27.11] [INFO]     Policy accuracy @1: 34.40%
[21.27.11] [INFO]     Policy accuracy @5: 72.66%
[21.27.11] [INFO]     Policy accuracy @10: 86.33%
[21.27.11] [INFO]     Avg value loss: 1.0874011973539988
[21.29.15] [INFO] Results after playing two most recent models at iteration 66: Wins: 17, Losses: 17, Draws: 6
[21.31.03] [INFO] Results after playing the current vs the reference at iteration 66: Wins: 11, Losses: 25, Draws: 4
[21.31.30] [INFO] Results after playing vs random at iteration 66: Wins: 40, Losses: 0, Draws: 0
[21.34.17] [INFO] Loading memories for iteration 67 with window size 3 (64-67)
[21.34.27] [INFO] Loaded 2382280 samples from 19120 games
Training batches: 100%|█████████▉| 1161/1163 [01:17<00:00, 15.03it/s]
[21.35.45] [INFO] Last gradient norm: 0.73828125
[21.35.45] [INFO] Training stats: Policy Loss: 1.2078, Value Loss: 0.0791, Total Loss: 1.2870, Value Mean: 0.0038, Value Std: 0.8029
Validation batches: 100%|██████████| 59/59 [00:17<00:00,  3.43it/s]
[21.36.02] [INFO] Validation stats: Policy Loss: 1.1717, Value Loss: 0.0550, Total Loss: 1.2263, Value Mean: -0.0557, Value Std: 0.7907
[21.36.04] [INFO] Trainer finished at iteration 67.
[21.36.04] [INFO] All processes started at iteration 68.
[21.36.04] [INFO] Model and optimizer loaded from iteration 68
[21.36.27] [INFO] Evaluation results at iteration 67:
[21.36.27] [INFO]     Policy accuracy @1: 34.37%
[21.36.27] [INFO]     Policy accuracy @5: 73.59%
[21.36.27] [INFO]     Policy accuracy @10: 86.81%
[21.36.27] [INFO]     Avg value loss: 0.8979520122210185
[21.38.31] [INFO] Results after playing two most recent models at iteration 67: Wins: 26, Losses: 9, Draws: 5
[21.40.13] [INFO] Results after playing the current vs the reference at iteration 67: Wins: 13, Losses: 20, Draws: 7
[21.40.40] [INFO] Results after playing vs random at iteration 67: Wins: 39, Losses: 0, Draws: 1
[21.43.26] [INFO] Loading memories for iteration 68 with window size 3 (65-68)
[21.43.37] [INFO] Loaded 2410030 samples from 19340 games
Training batches: 100%|█████████▉| 1175/1176 [01:20<00:00, 14.65it/s]
[21.44.57] [INFO] Last gradient norm: 0.703125
[21.44.57] [INFO] Training stats: Policy Loss: 1.2052, Value Loss: 0.0779, Total Loss: 1.2832, Value Mean: 0.0041, Value Std: 0.8038
Validation batches: 100%|██████████| 57/57 [00:15<00:00,  3.80it/s]
[21.45.12] [INFO] Validation stats: Policy Loss: 1.1586, Value Loss: 0.0680, Total Loss: 1.2271, Value Mean: -0.0575, Value Std: 0.8278
[21.45.14] [INFO] Trainer finished at iteration 68.
[21.45.14] [INFO] All processes started at iteration 69.
[21.45.14] [INFO] Model and optimizer loaded from iteration 69
[21.45.38] [INFO] Evaluation results at iteration 68:
[21.45.38] [INFO]     Policy accuracy @1: 33.82%
[21.45.38] [INFO]     Policy accuracy @5: 74.14%
[21.45.38] [INFO]     Policy accuracy @10: 86.89%
[21.45.38] [INFO]     Avg value loss: 0.9922180453936259
[21.47.36] [INFO] Results after playing two most recent models at iteration 68: Wins: 13, Losses: 19, Draws: 8
[21.49.31] [INFO] Results after playing the current vs the reference at iteration 68: Wins: 7, Losses: 25, Draws: 8
[21.49.59] [INFO] Results after playing vs random at iteration 68: Wins: 38, Losses: 0, Draws: 2
[21.52.45] [INFO] Loading memories for iteration 69 with window size 3 (66-69)
[21.52.56] [INFO] Loaded 2404520 samples from 19300 games
Training batches: 100%|█████████▉| 1171/1174 [01:24<00:00, 13.84it/s]
[21.54.21] [INFO] Last gradient norm: 0.8046875
[21.54.21] [INFO] Training stats: Policy Loss: 1.2061, Value Loss: 0.0764, Total Loss: 1.2825, Value Mean: 0.0042, Value Std: 0.8029
Validation batches: 100%|██████████| 59/59 [00:15<00:00,  3.70it/s]
[21.54.37] [INFO] Validation stats: Policy Loss: 1.1597, Value Loss: 0.0859, Total Loss: 1.2458, Value Mean: -0.0036, Value Std: 0.8149
[21.54.39] [INFO] Trainer finished at iteration 69.
[21.54.39] [INFO] All processes started at iteration 70.
[21.54.39] [INFO] Model and optimizer loaded from iteration 70
[21.55.03] [INFO] Evaluation results at iteration 69:
[21.55.03] [INFO]     Policy accuracy @1: 34.29%
[21.55.03] [INFO]     Policy accuracy @5: 73.82%
[21.55.03] [INFO]     Policy accuracy @10: 86.65%
[21.55.03] [INFO]     Avg value loss: 1.102474194765091
[21.57.05] [INFO] Results after playing two most recent models at iteration 69: Wins: 11, Losses: 24, Draws: 5
[21.59.03] [INFO] Results after playing the current vs the reference at iteration 69: Wins: 7, Losses: 26, Draws: 7
[21.59.30] [INFO] Results after playing vs random at iteration 69: Wins: 38, Losses: 0, Draws: 2
[22.02.10] [INFO] Loading memories for iteration 70 with window size 3 (67-70)
[22.02.21] [INFO] Loaded 2417515 samples from 19400 games
Training batches: 100%|█████████▉| 1178/1180 [01:16<00:00, 15.32it/s]
[22.03.38] [INFO] Last gradient norm: 0.765625
[22.03.38] [INFO] Training stats: Policy Loss: 1.2117, Value Loss: 0.0777, Total Loss: 1.2894, Value Mean: 0.0039, Value Std: 0.8000
Validation batches: 100%|██████████| 58/58 [00:15<00:00,  3.64it/s]
[22.03.54] [INFO] Validation stats: Policy Loss: 1.1692, Value Loss: 0.0846, Total Loss: 1.2538, Value Mean: 0.0670, Value Std: 0.7846
[22.03.56] [INFO] Trainer finished at iteration 70.
[22.03.56] [INFO] All processes started at iteration 71.
[22.03.56] [INFO] Model and optimizer loaded from iteration 71
[22.04.24] [INFO] Evaluation results at iteration 70:
[22.04.24] [INFO]     Policy accuracy @1: 33.24%
[22.04.24] [INFO]     Policy accuracy @5: 73.08%
[22.04.24] [INFO]     Policy accuracy @10: 86.44%
[22.04.24] [INFO]     Avg value loss: 1.06887047290802
[22.06.25] [INFO] Results after playing two most recent models at iteration 70: Wins: 18, Losses: 17, Draws: 5
[22.08.02] [INFO] Results after playing the current vs the reference at iteration 70: Wins: 8, Losses: 29, Draws: 3
[22.08.26] [INFO] Results after playing vs random at iteration 70: Wins: 40, Losses: 0, Draws: 0
[22.11.27] [INFO] Loading memories for iteration 71 with window size 3 (68-71)
[22.11.36] [INFO] Loaded 2401230 samples from 19260 games
Training batches: 100%|█████████▉| 1169/1172 [01:19<00:00, 14.72it/s]
[22.12.56] [INFO] Last gradient norm: 0.76171875
[22.12.56] [INFO] Training stats: Policy Loss: 1.1998, Value Loss: 0.0782, Total Loss: 1.2780, Value Mean: 0.0043, Value Std: 0.8030
Validation batches: 100%|██████████| 57/57 [00:16<00:00,  3.39it/s]
[22.13.13] [INFO] Validation stats: Policy Loss: 1.1624, Value Loss: 0.0898, Total Loss: 1.2522, Value Mean: 0.0576, Value Std: 0.8331
[22.13.15] [INFO] Trainer finished at iteration 71.
[22.13.15] [INFO] All processes started at iteration 72.
[22.13.16] [INFO] Model and optimizer loaded from iteration 72
[22.13.47] [INFO] Evaluation results at iteration 71:
[22.13.47] [INFO]     Policy accuracy @1: 34.16%
[22.13.47] [INFO]     Policy accuracy @5: 72.24%
[22.13.47] [INFO]     Policy accuracy @10: 85.93%
[22.13.47] [INFO]     Avg value loss: 1.0734027942021689
[22.15.52] [INFO] Results after playing two most recent models at iteration 71: Wins: 18, Losses: 14, Draws: 8
[22.17.27] [INFO] Results after playing the current vs the reference at iteration 71: Wins: 11, Losses: 24, Draws: 5
[22.17.52] [INFO] Results after playing vs random at iteration 71: Wins: 40, Losses: 0, Draws: 0
[22.20.47] [INFO] Loading memories for iteration 72 with window size 3 (69-72)
[22.20.58] [INFO] Loaded 2408990 samples from 19340 games
Training batches: 100%|█████████▉| 1173/1176 [01:20<00:00, 14.56it/s]
[22.22.18] [INFO] Last gradient norm: 0.7109375
[22.22.18] [INFO] Training stats: Policy Loss: 1.1889, Value Loss: 0.0799, Total Loss: 1.2687, Value Mean: 0.0048, Value Std: 0.8033
Validation batches: 100%|██████████| 58/58 [00:18<00:00,  3.12it/s]
[22.22.37] [INFO] Validation stats: Policy Loss: 1.1251, Value Loss: 0.0567, Total Loss: 1.1822, Value Mean: 0.0191, Value Std: 0.8260
[22.22.39] [INFO] Trainer finished at iteration 72.
[22.22.39] [INFO] All processes started at iteration 73.
[22.22.39] [INFO] Model and optimizer loaded from iteration 73
[22.23.04] [INFO] Evaluation results at iteration 72:
[22.23.04] [INFO]     Policy accuracy @1: 33.34%
[22.23.04] [INFO]     Policy accuracy @5: 73.37%
[22.23.04] [INFO]     Policy accuracy @10: 86.57%
[22.23.04] [INFO]     Avg value loss: 1.1041302859783173
[22.24.58] [INFO] Results after playing two most recent models at iteration 72: Wins: 13, Losses: 22, Draws: 5
[22.26.53] [INFO] Results after playing the current vs the reference at iteration 72: Wins: 7, Losses: 23, Draws: 10
[22.27.21] [INFO] Results after playing vs random at iteration 72: Wins: 39, Losses: 0, Draws: 1
[22.30.09] [INFO] Loading memories for iteration 73 with window size 3 (70-73)
[22.30.20] [INFO] Loaded 2367535 samples from 19020 games
Training batches: 100%|█████████▉| 1154/1156 [01:20<00:00, 14.31it/s]
[22.31.41] [INFO] Last gradient norm: 0.72265625
[22.31.41] [INFO] Training stats: Policy Loss: 1.1829, Value Loss: 0.0800, Total Loss: 1.2630, Value Mean: 0.0050, Value Std: 0.8050
Validation batches: 100%|██████████| 55/55 [00:16<00:00,  3.29it/s]
[22.31.58] [INFO] Validation stats: Policy Loss: 1.1368, Value Loss: 0.0649, Total Loss: 1.2018, Value Mean: -0.0206, Value Std: 0.8036
[22.32.00] [INFO] Trainer finished at iteration 73.
[22.32.00] [INFO] All processes started at iteration 74.
[22.32.00] [INFO] Model and optimizer loaded from iteration 74
[22.32.24] [INFO] Evaluation results at iteration 73:
[22.32.24] [INFO]     Policy accuracy @1: 34.03%
[22.32.24] [INFO]     Policy accuracy @5: 73.51%
[22.32.24] [INFO]     Policy accuracy @10: 85.88%
[22.32.24] [INFO]     Avg value loss: 1.1348852833112082
[22.34.26] [INFO] Results after playing two most recent models at iteration 73: Wins: 13, Losses: 16, Draws: 11
[22.36.07] [INFO] Results after playing the current vs the reference at iteration 73: Wins: 9, Losses: 27, Draws: 4
[22.36.31] [INFO] Results after playing vs random at iteration 73: Wins: 40, Losses: 0, Draws: 0
[22.39.33] [INFO] Loading memories for iteration 74 with window size 3 (71-74)
[22.39.43] [INFO] Loaded 2345820 samples from 18840 games
Training batches: 100%|█████████▉| 1143/1145 [01:20<00:00, 14.12it/s]
[22.41.04] [INFO] Last gradient norm: 0.71484375
[22.41.04] [INFO] Training stats: Policy Loss: 1.1807, Value Loss: 0.0812, Total Loss: 1.2618, Value Mean: 0.0043, Value Std: 0.8094
Validation batches: 100%|██████████| 56/56 [00:15<00:00,  3.54it/s]
[22.41.20] [INFO] Validation stats: Policy Loss: 1.1444, Value Loss: 0.0599, Total Loss: 1.2044, Value Mean: 0.0231, Value Std: 0.8145
[22.41.22] [INFO] Trainer finished at iteration 74.
[22.41.22] [INFO] All processes started at iteration 75.
[22.41.22] [INFO] Model and optimizer loaded from iteration 75
[22.41.46] [INFO] Evaluation results at iteration 74:
[22.41.46] [INFO]     Policy accuracy @1: 33.66%
[22.41.46] [INFO]     Policy accuracy @5: 73.37%
[22.41.46] [INFO]     Policy accuracy @10: 86.81%
[22.41.46] [INFO]     Avg value loss: 1.0651513834794362
[22.43.56] [INFO] Results after playing two most recent models at iteration 74: Wins: 17, Losses: 12, Draws: 11
[22.45.30] [INFO] Results after playing the current vs the reference at iteration 74: Wins: 5, Losses: 29, Draws: 6
[22.45.57] [INFO] Results after playing vs random at iteration 74: Wins: 39, Losses: 0, Draws: 1
[22.48.52] [INFO] Loading memories for iteration 75 with window size 3 (72-75)
[22.49.03] [INFO] Loaded 2333055 samples from 18720 games
Training batches: 100%|█████████▉| 1138/1139 [01:20<00:00, 14.15it/s]
[22.50.23] [INFO] Last gradient norm: 0.703125
[22.50.23] [INFO] Training stats: Policy Loss: 1.1913, Value Loss: 0.0789, Total Loss: 1.2702, Value Mean: 0.0040, Value Std: 0.8120
Validation batches: 100%|██████████| 56/56 [00:17<00:00,  3.23it/s]
[22.50.40] [INFO] Validation stats: Policy Loss: 1.1659, Value Loss: 0.0970, Total Loss: 1.2624, Value Mean: 0.1235, Value Std: 0.7958
[22.50.42] [INFO] Trainer finished at iteration 75.
[22.50.42] [INFO] All processes started at iteration 76.
[22.50.43] [INFO] Model and optimizer loaded from iteration 76
[22.51.07] [INFO] Evaluation results at iteration 75:
[22.51.07] [INFO]     Policy accuracy @1: 34.40%
[22.51.07] [INFO]     Policy accuracy @5: 73.14%
[22.51.07] [INFO]     Policy accuracy @10: 86.67%
[22.51.07] [INFO]     Avg value loss: 1.0525961379210154
[22.53.03] [INFO] Results after playing two most recent models at iteration 75: Wins: 17, Losses: 21, Draws: 2
[22.54.53] [INFO] Results after playing the current vs the reference at iteration 75: Wins: 11, Losses: 24, Draws: 5
[22.55.18] [INFO] Results after playing vs random at iteration 75: Wins: 40, Losses: 0, Draws: 0
[22.57.53] [INFO] Loading memories for iteration 76 with window size 3 (73-76)
[22.58.03] [INFO] Loaded 2341350 samples from 18780 games
Training batches: 100%|█████████▉| 1142/1143 [01:21<00:00, 14.04it/s]
[22.59.25] [INFO] Last gradient norm: 0.7890625
[22.59.25] [INFO] Training stats: Policy Loss: 1.1890, Value Loss: 0.0794, Total Loss: 1.2683, Value Mean: 0.0038, Value Std: 0.8080
Validation batches: 100%|██████████| 59/59 [00:15<00:00,  3.79it/s]
[22.59.40] [INFO] Validation stats: Policy Loss: 1.1160, Value Loss: 0.0474, Total Loss: 1.1634, Value Mean: 0.0293, Value Std: 0.8161
[22.59.42] [INFO] Trainer finished at iteration 76.
[22.59.42] [INFO] All processes started at iteration 77.
[22.59.43] [INFO] Model and optimizer loaded from iteration 77
[23.00.08] [INFO] Evaluation results at iteration 76:
[23.00.08] [INFO]     Policy accuracy @1: 33.66%
[23.00.08] [INFO]     Policy accuracy @5: 72.37%
[23.00.08] [INFO]     Policy accuracy @10: 85.99%
[23.00.08] [INFO]     Avg value loss: 1.077113684018453
[23.02.21] [INFO] Results after playing two most recent models at iteration 76: Wins: 12, Losses: 20, Draws: 8
[23.04.21] [INFO] Results after playing the current vs the reference at iteration 76: Wins: 10, Losses: 22, Draws: 8
[23.04.51] [INFO] Results after playing vs random at iteration 76: Wins: 38, Losses: 0, Draws: 2
[23.07.15] [INFO] Loading memories for iteration 77 with window size 3 (74-77)
[23.07.25] [INFO] Loaded 2378985 samples from 19080 games
Training batches: 100%|█████████▉| 1159/1161 [01:17<00:00, 15.01it/s]
[23.08.42] [INFO] Last gradient norm: 0.77734375
[23.08.42] [INFO] Training stats: Policy Loss: 1.1746, Value Loss: 0.0764, Total Loss: 1.2509, Value Mean: 0.0042, Value Std: 0.8087
Validation batches: 100%|██████████| 59/59 [00:16<00:00,  3.54it/s]
[23.08.59] [INFO] Validation stats: Policy Loss: 1.1053, Value Loss: 0.0647, Total Loss: 1.1692, Value Mean: -0.0526, Value Std: 0.8282
[23.09.01] [INFO] Trainer finished at iteration 77.
[23.09.01] [INFO] All processes started at iteration 78.
[23.09.01] [INFO] Model and optimizer loaded from iteration 78
[23.09.24] [INFO] Evaluation results at iteration 77:
[23.09.24] [INFO]     Policy accuracy @1: 33.40%
[23.09.24] [INFO]     Policy accuracy @5: 73.11%
[23.09.24] [INFO]     Policy accuracy @10: 85.96%
[23.09.24] [INFO]     Avg value loss: 1.019670957326889
[23.11.33] [INFO] Results after playing two most recent models at iteration 77: Wins: 17, Losses: 17, Draws: 6
[23.13.16] [INFO] Results after playing the current vs the reference at iteration 77: Wins: 9, Losses: 24, Draws: 7
[23.13.43] [INFO] Results after playing vs random at iteration 77: Wins: 39, Losses: 0, Draws: 1
[23.16.42] [INFO] Loading memories for iteration 78 with window size 3 (75-78)
[23.16.52] [INFO] Loaded 2397860 samples from 19240 games
Training batches: 100%|█████████▉| 1168/1170 [01:21<00:00, 14.32it/s]
[23.18.14] [INFO] Last gradient norm: 0.76171875
[23.18.14] [INFO] Training stats: Policy Loss: 1.1664, Value Loss: 0.0762, Total Loss: 1.2426, Value Mean: 0.0043, Value Std: 0.8065
Validation batches: 100%|██████████| 58/58 [00:16<00:00,  3.56it/s]
[23.18.31] [INFO] Validation stats: Policy Loss: 1.1328, Value Loss: 0.0637, Total Loss: 1.1968, Value Mean: 0.0409, Value Std: 0.8281
[23.18.32] [INFO] Trainer finished at iteration 78.
[23.18.32] [INFO] All processes started at iteration 79.
[23.18.33] [INFO] Model and optimizer loaded from iteration 79
[23.18.56] [INFO] Evaluation results at iteration 78:
[23.18.56] [INFO]     Policy accuracy @1: 32.60%
[23.18.56] [INFO]     Policy accuracy @5: 72.50%
[23.18.56] [INFO]     Policy accuracy @10: 85.70%
[23.18.56] [INFO]     Avg value loss: 1.1714144468307495
[23.20.59] [INFO] Results after playing two most recent models at iteration 78: Wins: 13, Losses: 19, Draws: 8
[23.23.03] [INFO] Results after playing the current vs the reference at iteration 78: Wins: 12, Losses: 20, Draws: 8
slurmstepd: error: *** JOB 3101507 ON hkn0902 CANCELLED AT 2025-04-24T23:23:08 DUE TO TIME LIMIT ***
[23.23.33] [INFO] Results after playing vs random at iteration 78: Wins: 40, Losses: 0, Draws: 0

============================= JOB FEEDBACK =============================

Job ID: 3101507
Cluster: hk
User/Group: fu5896/hk-project-starter-p0023563
Account: hk-project-starter-p0023563
State: TIMEOUT (exit code 0)
Partition: accelerated-h100
Nodes: 1
Cores per node: 122
Nodelist: hkn0902
CPU Utilized: 00:00:03
CPU Efficiency: 0.00% of 61-00:44:44 core-walltime
Job Wall-clock time: 12:00:22
Starttime: Thu Apr 24 11:22:46 2025
Endtime: Thu Apr 24 23:23:08 2025
Memory Utilized: 282.00 GB
Memory Efficiency: 57.75% of 488.28 GB
Energy Consumed: 2393665875 Joule / 664907.1875 Watthours
Average node power draw: 55380.7291425663 Watt
(Chess) [fu5896@hkn1993 py]$
